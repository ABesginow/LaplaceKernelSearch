{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "78e5200b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import stan\n",
    "import torch\n",
    "import gpytorch\n",
    "import math\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "ed7f3809",
   "metadata": {},
   "outputs": [],
   "source": [
    "asyncio.run(asyncio.sleep(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "167614c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training data is 100 points in [0,1] inclusive regularly spaced\n",
    "train_x = torch.linspace(0, 1, 5)\n",
    "# True function is sin(2*pi*x) with Gaussian noise\n",
    "train_y = torch.sin(train_x * (2 * math.pi)) + torch.randn(train_x.size()) * math.sqrt(0.04)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "a516432e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n        matrix[N, N] K;\\n        array[1] real left;\\n        array[1] real right;\\n        for (i in 1:N){\\n            for (j in 1:N){\\n            left[1] = x[j];\\n            right[1] = x[i];\\n                K[j, i] = softplus(theta[1]) + (softplus(theta[2]) * gp_periodic_cov(left, right, 1.0, sqrt(softplus(theta[3])), softplus(theta[4])))[1][1];\\n            }\\n        }\\n'"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "STAN_data = {'N': 100, 'D': 4, 'x': [-1.7062203884124756, -1.6717512607574463, -1.6372822523117065, -1.6028130054473877, -1.568343997001648, -1.5338748693466187, -1.499405860900879, -1.46493661403656, -1.4304676055908203, -1.395998477935791, -1.3615294694900513, -1.327060341835022, -1.2925913333892822, -1.258122205734253, -1.2236530780792236, -1.1891839504241943, -1.154714822769165, -1.1202456951141357, -1.085776686668396, -1.0513075590133667, -1.0168384313583374, -0.9823693633079529, -0.9479001760482788, -0.9134311676025391, -0.878961980342865, -0.8444929122924805, -0.8100237846374512, -0.7755547165870667, -0.7410856485366821, -0.7066165208816528, -0.6721473932266235, -0.637678325176239, -0.6032092571258545, -0.5687401294708252, -0.5342710614204407, -0.49980196356773376, -0.46533286571502686, -0.43086379766464233, -0.39639464020729065, -0.36192557215690613, -0.32745644450187683, -0.2929873466491699, -0.258518248796463, -0.2240491509437561, -0.1895800530910492, -0.15511097013950348, -0.12064185738563538, -0.08617276698350906, -0.051703665405511856, -0.017234569415450096, 0.017234528437256813, 0.05170363560318947, 0.08617272228002548, 0.1206418126821518, 0.1551109254360199, 0.189580038189888, 0.2240491807460785, 0.2585182785987854, 0.2929874062538147, 0.3274564743041992, 0.36192557215690613, 0.39639464020729065, 0.43086379766464233, 0.46533286571502686, 0.49980196356773376, 0.5342710614204407, 0.5687401294708252, 0.6032092571258545, 0.637678325176239, 0.6721474528312683, 0.7066166400909424, 0.7410856485366821, 0.7755547165870667, 0.8100237846374512, 0.8444929122924805, 0.878961980342865, 0.9134311079978943, 0.9479001760482788, 0.9823693633079529, 1.0168383121490479, 1.0513075590133667, 1.085776686668396, 1.1202456951141357, 1.154714822769165, 1.1891839504241943, 1.223652958869934, 1.2581220865249634, 1.2925913333892822, 1.327060341835022, 1.3615293502807617, 1.395998477935791, 1.4304676055908203, 1.46493661403656, 1.499405860900879, 1.5338748693466187, 1.568343997001648, 1.6028130054473877, 1.6372822523117065, 1.6717512607574463, 1.7062203884124756], 'y': [-1.947783630384947e-06, 0.8384983539581299, 1.350435495376587, 1.3364328145980835, 0.8019461631774902, -0.04487171769142151, -0.8742072582244873, -1.3630785942077637, -1.321084976196289, -0.764581024646759, 0.08969195932149887, 0.9090359807014465, 1.3743486404418945, 1.3044071197509766, 0.7264529466629028, -0.13442744314670563, -0.9429534673690796, -1.3842355012893677, -1.2864148616790771, -0.6875864267349243, 0.17902740836143494, 0.9759174585342407, 1.392728328704834, 1.2671271562576294, 0.6480297446250916, -0.22344724833965302, -1.007901668548584, -1.399818778038025, -1.2465636730194092, -0.6078219413757324, 0.26764193177223206, 1.038870096206665, 1.4054996967315674, 1.224745512008667, 0.56700199842453, -0.3115646541118622, -1.0687905550003052, -1.4097654819488525, -1.2016937732696533, -0.525610625743866, 0.3551761209964752, 1.0976362228393555, 1.412611722946167, 1.1774319410324097, 0.483690470457077, -0.39842915534973145, -1.1253769397735596, -1.4140355587005615, -1.1519849300384521, -0.4412827789783478, 0.4412810802459717, 1.1519839763641357, 1.4140355587005615, 1.1253776550292969, 0.39843034744262695, -0.48368895053863525, -1.1774319410324097, -1.412611722946167, -1.0976362228393555, -0.3551762104034424, 0.5256105065345764, 1.2016937732696533, 1.4097654819488525, 1.0687905550003052, 0.311564564704895, -0.5670020580291748, -1.224745512008667, -1.4054996967315674, -1.0388691425323486, -0.26764070987701416, 0.6078243255615234, 1.2465636730194092, 1.399818778038025, 1.007901668548584, 0.22344717383384705, -0.6480298638343811, -1.2671259641647339, -1.392728328704834, -0.9759174585342407, -0.1790301650762558, 0.6875863671302795, 1.2864148616790771, 1.3842355012893677, 0.9429534673690796, 0.13442736864089966, -0.7264506220817566, -1.3044061660766602, -1.3743486404418945, -0.9090380668640137, -0.08969474583864212, 0.764581024646759, 1.321084976196289, 1.3630785942077637, 0.8742072582244873, 0.04487164318561554, -0.8019461631774902, -1.3364328145980835, -1.350435495376587, -0.8384983539581299, 1.8736051288215094e-06], \n",
    "             't_mu': [-1.7920000553131104, 0.33799999952316284, 0.2840000092983246, -1.4630000591278076], \n",
    "             't_sigma': [[3.2660000324249268, 0.0, 0.0, 0.0], [0.0, 2.635999917984009, 0.0, 0.0], [0.0, 0.0, 0.9020000100135803, 0.0], [0.0, 0.0, 0.0, 1.6330000162124634]]}\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "STAN_code = \"\"\"\n",
    "    functions {\n",
    "        array[] real softplus(array[] real v){\n",
    "            array[num_elements(v)] real r;\n",
    "            for (d in 1:num_elements(v)){\n",
    "                r[d] = log(1.0 + exp(v[d]));\n",
    "            }\n",
    "            return r;\n",
    "        }\n",
    "        real softplus(real v){\n",
    "            return log(1.0 + exp(v));\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    data {\n",
    "        int N;\n",
    "        int D;\n",
    "        array[N] real x;\n",
    "        vector[N] y;\n",
    "        vector[D] t_mu;\n",
    "        matrix[D, D] t_sigma;\n",
    "    }\n",
    "\n",
    "     \n",
    "    parameters {\n",
    "        vector<lower=-3.0>[D] theta;\n",
    "    }\n",
    "    \n",
    "    transformed parameters{\n",
    "        cov_matrix[N] K;\n",
    "        K = identity_matrix(dims(x)[1])*softplus(theta[1]) + gp_periodic_cov(x, 1.0, sqrt(softplus(theta[2])), softplus(theta[3]));\n",
    "    }\n",
    "    \n",
    "    model {\n",
    "        \n",
    "        vector[N] mu;\n",
    "        theta ~ multi_normal(t_mu, t_sigma);\n",
    "        mu = zeros_vector(N);\n",
    "        y ~ multi_normal(mu, K);\n",
    "    }\n",
    "\n",
    "    \n",
    "    \n",
    "    generated quantities {\n",
    "        matrix[N, N] PER = gp_periodic_cov(x, 1.0, sqrt(softplus(theta[2])), softplus(theta[3]));\n",
    "        matrix[N, N] LIN = softplus(theta[4]) * gp_dot_prod_cov(x, 0.0);\n",
    "        matrix[N, N] noise =  identity_matrix(dims(x)[1])*softplus(theta[1]);\n",
    "        matrix[N, N] TOT2 = noise + PER .* LIN;\n",
    "        matrix[N, N] TOT = identity_matrix(dims(x)[1])*softplus(theta[1]) + ((gp_periodic_cov(x, 1.0, sqrt(softplus(theta[2])), softplus(theta[3])) .* softplus(theta[4]) * gp_dot_prod_cov(x, 0.0)));\n",
    "    }\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "        matrix[N, N] K;\n",
    "        array[1] real left;\n",
    "        array[1] real right;\n",
    "        for (i in 1:N){\n",
    "            for (j in 1:N){\n",
    "            left[1] = x[j];\n",
    "            right[1] = x[i];\n",
    "                K[j, i] = softplus(theta[1]) + (softplus(theta[2]) * gp_periodic_cov(left, right, 1.0, sqrt(softplus(theta[3])), softplus(theta[4])))[1][1];\n",
    "            }\n",
    "        }\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "#K = identity_matrix(dims(x)[1])*softplus(theta[1]) + ((softplus(theta[2]) * gp_periodic_cov(x, 1.0, sqrt(softplus(theta[3])), softplus(theta[4]))) * (softplus(theta[5]) * gp_exp_quad_cov(x, 1.0, softplus(theta[6]))));"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1b3e71d4",
   "metadata": {},
   "source": [
    "STAN_code = \"\"\"\n",
    "    functions {\n",
    "        array[] real softplus(array[] real v){\n",
    "            array[num_elements(v)] real r;\n",
    "            for (d in 1:num_elements(v)){\n",
    "                r[d] = log(1.0 + exp(v[d]));\n",
    "            }\n",
    "            return r;\n",
    "        }\n",
    "        real softplus(real v){\n",
    "            return log(1.0 + exp(v));\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    data {\n",
    "        int N;\n",
    "        int D;\n",
    "        int P;\n",
    "        array[N] real x;\n",
    "        vector[N] y;\n",
    "        vector[P] t_mu;\n",
    "        matrix[P, P] t_sigma;\n",
    "        vector[D] theta;\n",
    "    }\n",
    "    parameters {\n",
    "        vector[P] pl;\n",
    "    }\n",
    "\n",
    "    model {\n",
    "        matrix[N, N] K;\n",
    "        vector[N] mu;\n",
    "        pl ~ multi_normal(t_mu, t_sigma);\n",
    "        K = gp_exp_quad_cov(x, 1.0, softplus(pl[1]));\n",
    "        mu = zeros_vector(N);\n",
    "        y ~ multi_normal(mu, K);\n",
    "    }\n",
    "    \n",
    "    generated quantities {\n",
    "        matrix[N, N] K;\n",
    "        array[1] real left;\n",
    "        array[1] real right;\n",
    "        for (i in 1:N){\n",
    "            for (j in 1:N){\n",
    "            left[1] = x[j];\n",
    "            right[1] = x[i];\n",
    "                K[j, i] = softplus(theta[1]) + (softplus(theta[2]) .* gp_periodic_cov(left, right, 1.0, sqrt(softplus(theta[3])), softplus(theta[4])))[1][1];\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "raw",
   "id": "7ba023ea",
   "metadata": {},
   "source": [
    "if type(train_x) == tuple:\n",
    "    x = train_x[0].tolist()\n",
    "    # Assuming I have [[x1], [x2], [x3], ...]\n",
    "    if not np.ndim(x) == 1:\n",
    "        x = [t[0] for t in x]\n",
    "else:\n",
    "    x = train_x.tolist()\n",
    "    \n",
    "if type(train_y) == tuple:\n",
    "    y = train_y[0].tolist()\n",
    "    # Assuming I have [[x1], [x2], [x3], ...]\n",
    "    if not np.ndim(y) == 1:\n",
    "        x = [t[0] for t in y]\n",
    "else:\n",
    "    y = train_y.tolist()\n",
    "    \n",
    "    \n",
    "theta = [0.0 for i in range(6)]\n",
    "\n",
    "\n",
    "STAN_data = {\n",
    "    \"N\" : 5,\n",
    "    \"D\" : len(theta),\n",
    "    \"P\" : 1,\n",
    "    \"x\" : train_x.tolist(),\n",
    "    \"y\" : train_y.tolist(),\n",
    "    \"t_mu\" : [1.0],\n",
    "    \"t_sigma\" : [[1.0]],\n",
    "    \"theta\" : theta\n",
    "    \n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "371dbe7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building..."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "In file included from /home/besginow/anaconda3/envs/sage/lib/python3.10/site-packages/httpstan/include/stan/math/prim/fun.hpp:124,\n",
      "                 from /home/besginow/anaconda3/envs/sage/lib/python3.10/site-packages/httpstan/include/stan/math/rev/fun/multiply.hpp:7,\n",
      "                 from /home/besginow/anaconda3/envs/sage/lib/python3.10/site-packages/httpstan/include/stan/math/rev/fun/elt_multiply.hpp:9,\n",
      "                 from /home/besginow/anaconda3/envs/sage/lib/python3.10/site-packages/httpstan/include/stan/math/rev/fun.hpp:55,\n",
      "                 from /home/besginow/anaconda3/envs/sage/lib/python3.10/site-packages/httpstan/include/stan/math/rev.hpp:10,\n",
      "                 from /home/besginow/anaconda3/envs/sage/lib/python3.10/site-packages/httpstan/include/stan/math.hpp:19,\n",
      "                 from /home/besginow/anaconda3/envs/sage/lib/python3.10/site-packages/httpstan/include/stan/model/model_header.hpp:4,\n",
      "                 from /home/besginow/.cache/httpstan/4.9.1/models/xgebu5y7/model_xgebu5y7.cpp:2:\n",
      "/home/besginow/anaconda3/envs/sage/lib/python3.10/site-packages/httpstan/include/stan/math/prim/fun/grad_2F1.hpp: In instantiation of 'TupleT stan::math::internal::grad_2F1_impl(const T1&, const T2&, const T3&, const T_z&, double, int) [with bool calc_a1 = true; bool calc_a2 = true; bool calc_b1 = true; bool calc_z = true; T1 = double; T2 = double; T3 = double; T_z = double; ScalarT = double; TupleT = std::tuple<double, double, double, double>]':\n",
      "/home/besginow/anaconda3/envs/sage/lib/python3.10/site-packages/httpstan/include/stan/math/prim/fun/grad_2F1.hpp:307:57:   required from 'auto stan::math::grad_2F1(const T1&, const T2&, const T3&, const T_z&, double, int) [with bool ReturnSameT = true; T1 = double; T2 = double; T3 = double; T_z = double; stan::require_t<std::integral_constant<bool, __v> >* <anonymous> = 0]'\n",
      "/home/besginow/anaconda3/envs/sage/lib/python3.10/site-packages/httpstan/include/stan/math/prim/fun/grad_inc_beta.hpp:37:46:   required from here\n",
      "/home/besginow/anaconda3/envs/sage/lib/python3.10/site-packages/httpstan/include/stan/math/prim/fun/grad_2F1.hpp:192:12: warning: unused variable 'pre_mult' [-Wunused-variable]\n",
      "  192 |       auto pre_mult = a2 * pow(1 - z, -1 - a2);\n",
      "      |            ^~~~~~~~\n",
      "/home/besginow/anaconda3/envs/sage/lib/python3.10/site-packages/httpstan/include/stan/math/prim/fun/grad_2F1.hpp: In instantiation of 'TupleT stan::math::internal::grad_2F1_impl(const T1&, const T2&, const T3&, const T_z&, double, int) [with bool calc_a1 = true; bool calc_a2 = true; bool calc_b1 = true; bool calc_z = true; T1 = stan::math::var_value<double>; T2 = stan::math::var_value<double>; T3 = stan::math::var_value<double>; T_z = stan::math::var_value<double>; ScalarT = stan::math::var_value<double>; TupleT = std::tuple<stan::math::var_value<double, void>, stan::math::var_value<double, void>, stan::math::var_value<double, void>, stan::math::var_value<double, void> >]':\n",
      "/home/besginow/anaconda3/envs/sage/lib/python3.10/site-packages/httpstan/include/stan/math/prim/fun/grad_2F1.hpp:307:57:   required from 'auto stan::math::grad_2F1(const T1&, const T2&, const T3&, const T_z&, double, int) [with bool ReturnSameT = true; T1 = stan::math::var_value<double>; T2 = stan::math::var_value<double>; T3 = stan::math::var_value<double>; T_z = stan::math::var_value<double>; stan::require_t<std::integral_constant<bool, __v> >* <anonymous> = 0]'\n",
      "/home/besginow/anaconda3/envs/sage/lib/python3.10/site-packages/httpstan/include/stan/math/rev/fun/grad_inc_beta.hpp:49:51:   required from here\n",
      "/home/besginow/anaconda3/envs/sage/lib/python3.10/site-packages/httpstan/include/stan/math/prim/fun/grad_2F1.hpp:192:12: warning: variable 'pre_mult' set but not used [-Wunused-but-set-variable]\n",
      "/home/besginow/.cache/httpstan/4.9.1/models/xgebu5y7/model_xgebu5y7.cpp: In instantiation of 'void model_xgebu5y7_namespace::model_xgebu5y7::transform_inits_impl(VecVar&, VecI&, VecVar&, std::ostream*) const [with VecVar = std::vector<double, std::allocator<double> >; VecI = std::vector<int>; stan::require_vector_t<T_y>* <anonymous> = 0; stan::require_vector_like_vt<std::is_integral, VecI>* <anonymous> = 0; std::ostream = std::basic_ostream<char>]':\n",
      "/home/besginow/.cache/httpstan/4.9.1/models/xgebu5y7/model_xgebu5y7.cpp:799:69:   required from here\n",
      "/home/besginow/.cache/httpstan/4.9.1/models/xgebu5y7/model_xgebu5y7.cpp:541:11: warning: variable 'pos__' set but not used [-Wunused-but-set-variable]\n",
      "  541 |       int pos__ = std::numeric_limits<int>::min();\n",
      "      |           ^~~~~\n",
      "In file included from /home/besginow/anaconda3/envs/sage/lib/python3.10/site-packages/httpstan/include/stan/math/prim/fun.hpp:124,\n",
      "                 from /home/besginow/anaconda3/envs/sage/lib/python3.10/site-packages/httpstan/include/stan/math/rev/fun/multiply.hpp:7,\n",
      "                 from /home/besginow/anaconda3/envs/sage/lib/python3.10/site-packages/httpstan/include/stan/math/rev/fun/elt_multiply.hpp:9,\n",
      "                 from /home/besginow/anaconda3/envs/sage/lib/python3.10/site-packages/httpstan/include/stan/math/rev/fun.hpp:55,\n",
      "                 from /home/besginow/anaconda3/envs/sage/lib/python3.10/site-packages/httpstan/include/stan/math/rev.hpp:10,\n",
      "                 from /home/besginow/anaconda3/envs/sage/lib/python3.10/site-packages/httpstan/include/stan/math.hpp:19,\n",
      "                 from /home/besginow/anaconda3/envs/sage/lib/python3.10/site-packages/httpstan/include/stan/model/model_header.hpp:4,\n",
      "                 from /home/besginow/.cache/httpstan/4.9.1/models/xgebu5y7/model_xgebu5y7.cpp:2:\n",
      "/home/besginow/anaconda3/envs/sage/lib/python3.10/site-packages/httpstan/include/stan/math/prim/fun/grad_2F1.hpp: In instantiation of 'TupleT stan::math::internal::grad_2F1_impl_ab(const T1&, const T2&, const T3&, const T_z&, double, int) [with bool calc_a1 = true; bool calc_a2 = true; bool calc_b1 = true; T1 = double; T2 = double; T3 = double; T_z = double; ScalarT = double; TupleT = std::tuple<double, double, double>]':\n",
      "/home/besginow/anaconda3/envs/sage/lib/python3.10/site-packages/httpstan/include/stan/math/prim/fun/grad_2F1.hpp:205:78:   required from 'TupleT stan::math::internal::grad_2F1_impl(const T1&, const T2&, const T3&, const T_z&, double, int) [with bool calc_a1 = true; bool calc_a2 = true; bool calc_b1 = true; bool calc_z = true; T1 = double; T2 = double; T3 = double; T_z = double; ScalarT = double; TupleT = std::tuple<double, double, double, double>]'\n",
      "/home/besginow/anaconda3/envs/sage/lib/python3.10/site-packages/httpstan/include/stan/math/prim/fun/grad_2F1.hpp:307:57:   required from 'auto stan::math::grad_2F1(const T1&, const T2&, const T3&, const T_z&, double, int) [with bool ReturnSameT = true; T1 = double; T2 = double; T3 = double; T_z = double; stan::require_t<std::integral_constant<bool, __v> >* <anonymous> = 0]'\n",
      "/home/besginow/anaconda3/envs/sage/lib/python3.10/site-packages/httpstan/include/stan/math/prim/fun/grad_inc_beta.hpp:37:46:   required from here\n",
      "/home/besginow/anaconda3/envs/sage/lib/python3.10/site-packages/httpstan/include/stan/math/prim/fun/grad_2F1.hpp:68:10: warning: unused variable 'log_precision' [-Wunused-variable]\n",
      "   68 |   double log_precision = log(precision);\n",
      "      |          ^~~~~~~~~~~~~\n",
      "/home/besginow/anaconda3/envs/sage/lib/python3.10/site-packages/httpstan/include/stan/math/prim/fun/grad_2F1.hpp: In instantiation of 'TupleT stan::math::internal::grad_2F1_impl_ab(const T1&, const T2&, const T3&, const T_z&, double, int) [with bool calc_a1 = true; bool calc_a2 = true; bool calc_b1 = true; T1 = stan::math::var_value<double>; T2 = stan::math::var_value<double>; T3 = stan::math::var_value<double>; T_z = stan::math::var_value<double>; ScalarT = stan::math::var_value<double>; TupleT = std::tuple<stan::math::var_value<double, void>, stan::math::var_value<double, void>, stan::math::var_value<double, void> >]':\n",
      "/home/besginow/anaconda3/envs/sage/lib/python3.10/site-packages/httpstan/include/stan/math/prim/fun/grad_2F1.hpp:205:78:   required from 'TupleT stan::math::internal::grad_2F1_impl(const T1&, const T2&, const T3&, const T_z&, double, int) [with bool calc_a1 = true; bool calc_a2 = true; bool calc_b1 = true; bool calc_z = true; T1 = stan::math::var_value<double>; T2 = stan::math::var_value<double>; T3 = stan::math::var_value<double>; T_z = stan::math::var_value<double>; ScalarT = stan::math::var_value<double>; TupleT = std::tuple<stan::math::var_value<double, void>, stan::math::var_value<double, void>, stan::math::var_value<double, void>, stan::math::var_value<double, void> >]'\n",
      "/home/besginow/anaconda3/envs/sage/lib/python3.10/site-packages/httpstan/include/stan/math/prim/fun/grad_2F1.hpp:307:57:   required from 'auto stan::math::grad_2F1(const T1&, const T2&, const T3&, const T_z&, double, int) [with bool ReturnSameT = true; T1 = stan::math::var_value<double>; T2 = stan::math::var_value<double>; T3 = stan::math::var_value<double>; T_z = stan::math::var_value<double>; stan::require_t<std::integral_constant<bool, __v> >* <anonymous> = 0]'\n",
      "/home/besginow/anaconda3/envs/sage/lib/python3.10/site-packages/httpstan/include/stan/math/rev/fun/grad_inc_beta.hpp:49:51:   required from here\n",
      "/home/besginow/anaconda3/envs/sage/lib/python3.10/site-packages/httpstan/include/stan/math/prim/fun/grad_2F1.hpp:68:10: warning: unused variable 'log_precision' [-Wunused-variable]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Building: 16.4s, done.Messages from stanc:\n",
      "Warning in '/tmp/httpstan_x3n20es1/model_xgebu5y7.stan', line 5, column 12: A\n",
      "    control flow statement inside function softplus depends on argument v. At\n",
      "    '/tmp/httpstan_x3n20es1/model_xgebu5y7.stan', line 31, column 99 to\n",
      "    column 107, the value of v depends on parameter(s): theta.\n",
      "Warning in '/tmp/httpstan_x3n20es1/model_xgebu5y7.stan', line 5, column 12: A\n",
      "    control flow statement inside function softplus depends on argument v. At\n",
      "    '/tmp/httpstan_x3n20es1/model_xgebu5y7.stan', line 31, column 49 to\n",
      "    column 57, the value of v depends on parameter(s): theta.\n",
      "Warning in '/tmp/httpstan_x3n20es1/model_xgebu5y7.stan', line 5, column 12: A\n",
      "    control flow statement inside function softplus depends on argument v. At\n",
      "    '/tmp/httpstan_x3n20es1/model_xgebu5y7.stan', line 31, column 120 to\n",
      "    column 128, the value of v depends on parameter(s): theta.\n",
      "Warning: The parameter theta has no priors. This means either no prior is\n",
      "    provided, or the prior(s) depend on data variables. In the later case,\n",
      "    this may be a false positive.\n"
     ]
    }
   ],
   "source": [
    "post = stan.build(STAN_code, data=STAN_data, random_seed=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "da6997f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling:   0%\n",
      "Sampling:   0% (1/1001)\n",
      "Sampling:  10% (100/1001)\n",
      "Sampling:  20% (200/1001)\n",
      "Sampling:  30% (300/1001)\n",
      "Sampling:  40% (400/1001)\n",
      "Sampling:  50% (500/1001)\n",
      "Sampling:  60% (600/1001)\n",
      "Sampling:  70% (700/1001)\n",
      "Sampling:  80% (800/1001)\n",
      "Sampling:  90% (900/1001)\n",
      "Sampling: 100% (1001/1001)\n",
      "Sampling: 100% (1001/1001), done.\n",
      "Messages received during sampling:\n",
      "  Gradient evaluation took 0.000597 seconds\n",
      "  1000 transitions using 10 leapfrog steps per transition would take 5.97 seconds.\n",
      "  Adjust your expectations accordingly!\n",
      "  Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n",
      "  Exception: model_xgebu5y7_namespace::log_prob: K is not symmetric. K[1,2] = -nan, but K[2,1] = -nan (in '/tmp/httpstan_ie8vrmrz/model_xgebu5y7.stan', line 30, column 8 to column 24)\n",
      "  If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n",
      "  but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n",
      "  Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n",
      "  Exception: model_xgebu5y7_namespace::log_prob: K is not symmetric. K[1,2] = -nan, but K[2,1] = -nan (in '/tmp/httpstan_ie8vrmrz/model_xgebu5y7.stan', line 30, column 8 to column 24)\n",
      "  If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n",
      "  but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n",
      "  Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n",
      "  Exception: model_xgebu5y7_namespace::log_prob: K is not symmetric. K[1,2] = -nan, but K[2,1] = -nan (in '/tmp/httpstan_ie8vrmrz/model_xgebu5y7.stan', line 30, column 8 to column 24)\n",
      "  If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n",
      "  but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n",
      "  Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n",
      "  Exception: model_xgebu5y7_namespace::log_prob: K is not symmetric. K[1,2] = -nan, but K[2,1] = -nan (in '/tmp/httpstan_ie8vrmrz/model_xgebu5y7.stan', line 30, column 8 to column 24)\n",
      "  If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n",
      "  but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n",
      "  Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n",
      "  Exception: model_xgebu5y7_namespace::log_prob: K is not symmetric. K[1,2] = -nan, but K[2,1] = -nan (in '/tmp/httpstan_ie8vrmrz/model_xgebu5y7.stan', line 30, column 8 to column 24)\n",
      "  If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n",
      "  but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n",
      "  Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n",
      "  Exception: model_xgebu5y7_namespace::log_prob: K is not symmetric. K[1,2] = -nan, but K[2,1] = -nan (in '/tmp/httpstan_ie8vrmrz/model_xgebu5y7.stan', line 30, column 8 to column 24)\n",
      "  If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n",
      "  but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n",
      "  Exception: model_xgebu5y7_namespace::write_array: PER is not positive definite. (in '/tmp/httpstan_ie8vrmrz/model_xgebu5y7.stan', line 45, column 8 to column 98)\n"
     ]
    }
   ],
   "source": [
    "fit = post.sample(num_chains=1, num_samples=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "c40dad06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "draws\n",
      "0   NaN\n",
      "dtype: float64\n",
      "draws\n",
      "0   NaN\n",
      "dtype: float64\n",
      "draws\n",
      "0   NaN\n",
      "Name: noise.93.100, dtype: float64\n",
      "draws\n",
      "0   NaN\n",
      "Name: noise.100.93, dtype: float64\n",
      "draws\n",
      "0   NaN\n",
      "Name: LIN.93.100, dtype: float64\n",
      "draws\n",
      "0   NaN\n",
      "Name: LIN.100.93, dtype: float64\n",
      "draws\n",
      "0   NaN\n",
      "Name: TOT2.93.100, dtype: float64\n",
      "draws\n",
      "0   NaN\n",
      "Name: TOT2.100.93, dtype: float64\n",
      "draws\n",
      "0   NaN\n",
      "Name: TOT.93.100, dtype: float64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "draws\n",
       "0   NaN\n",
       "Name: TOT.100.93, dtype: float64"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = fit.to_frame()\n",
    "print(a[\"noise.93.100\"] + a[\"LIN.93.100\"] * a[\"PER.93.100\"])\n",
    "print(a[\"noise.100.93\"] + a[\"LIN.100.93\"] * a[\"PER.100.93\"])\n",
    "print(a[\"noise.93.100\"])\n",
    "print(a[\"noise.100.93\"])\n",
    "print(a[\"LIN.93.100\"])\n",
    "print(a[\"LIN.100.93\"])\n",
    "#print(a[\"PER.93.100\"])\n",
    "#print(a[\"PER.100.93\"])\n",
    "print(a[\"TOT2.93.100\"])\n",
    "print(a[\"TOT2.100.93\"])\n",
    "print(a[\"TOT.93.100\"])\n",
    "a[\"TOT.100.93\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "24d24f3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PER is failure 10000 times\n",
      "LIN is failure 10000 times\n",
      "noise is failure 10000 times\n",
      "TOT is failure 10000 times\n"
     ]
    }
   ],
   "source": [
    "\n",
    "frame = fit.to_frame()\n",
    "#print(frame[list(fit.constrained_param_names)[1:5]])\n",
    "#print(frame[list(fit.constrained_param_names)[101:105]])\n",
    "for key in [\"PER\", \"LIN\", \"noise\", \"TOT\"]:\n",
    "    count = 0\n",
    "    you_failure = False \n",
    "    for i in range(100):\n",
    "        for j in range(100):\n",
    "            if not frame[f\"{key}.{i+1}.{j+1}\"][0] == frame[f\"{key}.{j+1}.{i+1}\"][0]:\n",
    "                count +=1\n",
    "                you_failure = True\n",
    "                #print(f\"{frame[f\"{key}.{i+1}.{j+1}\"][0]}\")\n",
    "                #print(f\"{frame[f\"{key}.{j+1}.{i+1}\"][0]}\")\n",
    "    if not you_failure:\n",
    "        print(f\"{key} is Dr!\")\n",
    "    else:\n",
    "        print(f\"{key} is failure {count} times\")\n",
    "\n",
    "\n",
    "    #frame[list(fit.constrained_param_names)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "c6c45c96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nAlternative for LIN:\\n\\nsoftplus(theta[i]) * gp_dot_prod_cov(array[] real x, real sigma)\\nwith sigma = 0\\n'"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "replacement_dictionary = {\n",
    "    \"c\" : \"softplus(theta[i])\",\n",
    "    \"SE\": \"gp_exp_quad_cov(x, 1.0, softplus(theta[i]))\",\n",
    "    \"PER\": \"gp_periodic_cov(x, 1.0, pow(softplus(theta[i]), 2), softplus(theta[i]))\",\n",
    "    \"LIN\": \"softplus(theta[i]) * (x' * x)\"\n",
    "}\n",
    "\"\"\"\n",
    "Alternative for LIN:\n",
    "\n",
    "softplus(theta[i]) * gp_dot_prod_cov(array[] real x, real sigma)\n",
    "with sigma = 0\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "75138439",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will use the simplest form of GP model, exact inference\n",
    "class ExactGPModel(gpytorch.models.ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood):\n",
    "        super(ExactGPModel, self).__init__(train_x, train_y, likelihood)\n",
    "        self.mean_module = gpytorch.means.ZeroMean()\n",
    "        #self.covar_module = gpytorch.kernels.RBFKernel()  # Passed w. param = 0\n",
    "        #self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel()) # Passed w. param=0, 1\n",
    "        #self.covar_module = gpytorch.kernels.PeriodicKernel() # Passed, now that sqrt(param) is used. w params = 0, 1\n",
    "        self.covar_module = gpytorch.kernels.LinearKernel() # Passed w. param = 0, 1\n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.PeriodicKernel()) * gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel()) \n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "\n",
    "# initialize likelihood and model\n",
    "likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "model = ExactGPModel(train_x, train_y, likelihood)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "0a313655",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('likelihood.noise_covar.raw_noise',\n",
       "  Parameter containing:\n",
       "  tensor([0.], requires_grad=True)),\n",
       " ('covar_module.kernels.0.raw_outputscale',\n",
       "  Parameter containing:\n",
       "  tensor(0., requires_grad=True)),\n",
       " ('covar_module.kernels.0.base_kernel.raw_lengthscale',\n",
       "  Parameter containing:\n",
       "  tensor([[0.]], requires_grad=True)),\n",
       " ('covar_module.kernels.0.base_kernel.raw_period_length',\n",
       "  Parameter containing:\n",
       "  tensor([[0.]], requires_grad=True)),\n",
       " ('covar_module.kernels.1.raw_outputscale',\n",
       "  Parameter containing:\n",
       "  tensor(0., requires_grad=True)),\n",
       " ('covar_module.kernels.1.base_kernel.raw_lengthscale',\n",
       "  Parameter containing:\n",
       "  tensor([[0.]], requires_grad=True))]"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(model.named_parameters())\n",
    "#list(model.parameters())[1].data = torch.tensor([[1.0]])\n",
    "#list(model.parameters())[2].data = torch.tensor([[1.0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "c41c8f0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.1737, 0.0422, 0.0676, 0.2218, 0.0104],\n",
       "        [0.0422, 1.1737, 0.0422, 0.0676, 0.2218],\n",
       "        [0.0676, 0.0422, 1.1737, 0.0422, 0.0676],\n",
       "        [0.2218, 0.0676, 0.0422, 1.1737, 0.0422],\n",
       "        [0.0104, 0.2218, 0.0676, 0.0422, 1.1737]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "covariance = torch.eye(len(train_x)) * likelihood.noise + model.covar_module(train_x).evaluate()\n",
    "covariance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "08469209",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.linalg_eig(\n",
       "eigenvalues=tensor([1.5130+0.j, 1.3166+0.j, 1.1436+0.j, 0.9424+0.j, 0.9528+0.j],\n",
       "       grad_fn=<LinalgEigBackward0>),\n",
       "eigenvectors=tensor([[-4.5856e-01+0.j, -5.3788e-01+0.j,  6.5003e-02+0.j,  5.3432e-01+0.j,\n",
       "         -4.5902e-01+0.j],\n",
       "        [-4.9305e-01+0.j,  4.5901e-01+0.j,  2.3086e-01+0.j, -4.5123e-01+0.j,\n",
       "         -5.3787e-01+0.j],\n",
       "        [-3.0537e-01+0.j,  1.6510e-06+0.j, -9.4072e-01+0.j, -1.4762e-01+0.j,\n",
       "          3.1461e-06+0.j],\n",
       "        [-4.9305e-01+0.j, -4.5901e-01+0.j,  2.3086e-01+0.j, -4.5122e-01+0.j,\n",
       "          5.3788e-01+0.j],\n",
       "        [-4.5856e-01+0.j,  5.3787e-01+0.j,  6.5006e-02+0.j,  5.3432e-01+0.j,\n",
       "          4.5900e-01+0.j]], grad_fn=<LinalgEigBackward0>))"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.linalg.eig(covariance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "1135fe43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0024)"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.nn.functional.softplus(torch.tensor(-3.0))**2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
