{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(1, \"..\")\n",
    "import pprint\n",
    "import gpytorch\n",
    "import torch\n",
    "import numpy as np\n",
    "import metrics\n",
    "import copy\n",
    "import configparser\n",
    "from experiment_functions import Experiment\n",
    "from GaussianProcess import ExactGPModel\n",
    "from globalParams import options, hyperparameter_limits\n",
    "import gpytorch\n",
    "from helpFunctions import get_string_representation_of_kernel as gsr\n",
    "from helpFunctions import clean_kernel_expression\n",
    "from helpFunctions import get_kernels_in_kernel_expression\n",
    "from helpFunctions import amount_of_base_kernels\n",
    "from itertools import product\n",
    "import json\n",
    "from kernelSearch import *\n",
    "from matplotlib import pyplot as plt\n",
    "from metrics import *\n",
    "from multiprocessing import Pool\n",
    "import numpy as np\n",
    "import os\n",
    "import pdb\n",
    "import pickle\n",
    "import random\n",
    "import tikzplotlib\n",
    "import time\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "# To run STAN in a Jupyter notebook\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def log_prior(model, theta_mu=None, sigma=None):\n",
    "    # params -\n",
    "    # TODO de-spaghettize this once the priors are coded properly\n",
    "    prior_dict = {'SE': {'raw_lengthscale' : {\"mean\": -0.21221139138922668 , \"std\":1.8895426067756804}},\n",
    "                  'MAT52': {'raw_lengthscale' :{\"mean\": 0.7993038925994188, \"std\":2.145122566357853 } },\n",
    "                  'MAT32': {'raw_lengthscale' :{\"mean\": 1.5711054238673443, \"std\":2.4453761235991216 } },\n",
    "                  'RQ': {'raw_lengthscale' :{\"mean\": -0.049841950913676276, \"std\":1.9426354614713097 },\n",
    "                          'raw_alpha' :{\"mean\": 1.882148553921053, \"std\":3.096431944989054 } },\n",
    "                  'PER':{'raw_lengthscale':{\"mean\": 0.7778461197268618, \"std\":2.288946656544974 },\n",
    "                          'raw_period_length':{\"mean\": 0.6485334993738499, \"std\":0.9930632050553377 } },\n",
    "                  'LIN':{'raw_variance' :{\"mean\": -0.8017903983055685, \"std\":0.9966569921354465 } },\n",
    "                  'c':{'raw_outputscale':{\"mean\": -1.6253091096349706, \"std\":2.2570021716661923 } },\n",
    "                  'noise': {'raw_noise':{\"mean\": -3.51640656386717, \"std\":3.5831320474767407 }}}\n",
    "    #prior_dict = {\"SE\": {\"raw_lengthscale\": {\"mean\": 0.891, \"std\": 2.195}},\n",
    "    #              \"MAT\": {\"raw_lengthscale\": {\"mean\": 1.631, \"std\": 2.554}},\n",
    "    #              \"PER\": {\"raw_lengthscale\": {\"mean\": 0.338, \"std\": 2.636},\n",
    "    #                      \"raw_period_length\": {\"mean\": 0.284, \"std\": 0.902}},\n",
    "    #              \"LIN\": {\"raw_variance\": {\"mean\": -1.463, \"std\": 1.633}},\n",
    "    #              \"c\": {\"raw_outputscale\": {\"mean\": -2.163, \"std\": 2.448}},\n",
    "    #              \"noise\": {\"raw_noise\": {\"mean\": -1.792, \"std\": 3.266}}}\n",
    "\n",
    "    variances_list = list()\n",
    "    debug_param_name_list = list()\n",
    "    theta_mu = list()\n",
    "    params = list()\n",
    "    covar_string = gsr(model.covar_module)\n",
    "    covar_string = covar_string.replace(\"(\", \"\")\n",
    "    covar_string = covar_string.replace(\")\", \"\")\n",
    "    covar_string = covar_string.replace(\" \", \"\")\n",
    "    covar_string = covar_string.replace(\"PER\", \"PER+PER\")\n",
    "    covar_string_list = [s.split(\"*\") for s in covar_string.split(\"+\")]\n",
    "    covar_string_list.insert(0, [\"LIKELIHOOD\"])\n",
    "    covar_string_list = list(chain.from_iterable(covar_string_list))\n",
    "    both_PER_params = False\n",
    "    for (param_name, param), cov_str in zip(model.named_parameters(), covar_string_list):\n",
    "        params.append(param.item())\n",
    "        debug_param_name_list.append(param_name)\n",
    "        # First param is (always?) noise and is always with the likelihood\n",
    "        if \"likelihood\" in param_name:\n",
    "            theta_mu.append(prior_dict[\"noise\"][\"raw_noise\"][\"mean\"])\n",
    "            variances_list.append(prior_dict[\"noise\"][\"raw_noise\"][\"std\"])\n",
    "            continue\n",
    "        else:\n",
    "            if (cov_str == \"PER\" or cov_str == \"RQ\") and not both_PER_params:\n",
    "                theta_mu.append(prior_dict[cov_str][param_name.split(\".\")[-1]][\"mean\"])\n",
    "                variances_list.append(prior_dict[cov_str][param_name.split(\".\")[-1]][\"std\"])\n",
    "                both_PER_params = True\n",
    "            elif (cov_str == \"PER\" or cov_str == \"RQ\") and both_PER_params:\n",
    "                theta_mu.append(prior_dict[cov_str][param_name.split(\".\")[-1]][\"mean\"])\n",
    "                variances_list.append(prior_dict[cov_str][param_name.split(\".\")[-1]][\"std\"])\n",
    "                both_PER_params = False\n",
    "            else:\n",
    "                try:\n",
    "                    theta_mu.append(prior_dict[cov_str][param_name.split(\".\")[-1]][\"mean\"])\n",
    "                    variances_list.append(prior_dict[cov_str][param_name.split(\".\")[-1]][\"std\"])\n",
    "                except Exception as E:\n",
    "                    import pdb\n",
    "                    pdb.set_trace()\n",
    "                    prev_cov = cov_str\n",
    "    theta_mu = torch.tensor(theta_mu)\n",
    "    theta_mu = theta_mu.unsqueeze(0).t()\n",
    "    sigma = torch.diag(torch.Tensor(variances_list))\n",
    "    sigma = sigma@sigma\n",
    "    prior = torch.distributions.MultivariateNormal(theta_mu.t(), sigma)\n",
    "\n",
    "    # for convention reasons I'm diving by the number of datapoints\n",
    "    return prior.log_prob(torch.Tensor(params)).item() / len(*model.train_inputs)\n",
    "\n",
    "def optimize_hyperparameters(model, likelihood, train_iterations, X, Y, with_BFGS=False, MAP=False, prior=None, **kwargs):\n",
    "    \"\"\"\n",
    "    find optimal hyperparameters either by BO or by starting from random initial values multiple times, using an optimizer every time\n",
    "    and then returning the best result\n",
    "    \"\"\"\n",
    "    ## Setup\n",
    "    # Log the parameters found during training\n",
    "    log_param_path = kwargs.get(\"log_param_path\", False)\n",
    "    log_likelihood = kwargs.get(\"log_likelihood\", False)\n",
    "    random_restarts = kwargs.get(\"random_restarts\", options[\"training\"][\"restarts\"]+1)\n",
    "    best_loss = 1e400\n",
    "    optimal_parameters = dict()\n",
    "    limits = hyperparameter_limits\n",
    "    if log_param_path:\n",
    "        param_log_dict = {param_name[0] : list() for param_name in model.named_parameters()}\n",
    "    if log_likelihood:\n",
    "        likelihood_log = list()\n",
    "    # start runs\n",
    "    for iteration in range(random_restarts):\n",
    "    #for iteration in range(2):\n",
    "        # optimize and determine loss\n",
    "        # Perform a training for AIC and Laplace\n",
    "        model.train()\n",
    "        likelihood.train()\n",
    "\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=0.1)\n",
    "\n",
    "        mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "\n",
    "        for i in range(train_iterations):\n",
    "            # Zero gradients from previous iteration\n",
    "            optimizer.zero_grad()\n",
    "            # Output from model\n",
    "            output = model(X)\n",
    "            # Calc loss and backprop gradients\n",
    "            loss = -mll(output, Y)\n",
    "            if MAP:\n",
    "                log_p = log_prior(model)\n",
    "                loss -= log_p\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if log_param_path:\n",
    "                for param_name in model.named_parameters():\n",
    "                    param_log_dict[param_name[0]].append(param_name[1][0].item())\n",
    "            if log_likelihood:\n",
    "                likelihood_log.append(loss.item())\n",
    "\n",
    "        if with_BFGS:\n",
    "            # Additional BFGS optimization to better ensure optimal parameters\n",
    "            # LBFGS_optimizer = torch.optim.LBFGS(model.parameters(), max_iter=50, line_search_fn='strong_wolfe')\n",
    "            LBFGS_optimizer = torch.optim.LBFGS(\n",
    "                model.parameters(), max_iter=50,\n",
    "                line_search_fn='strong_wolfe')\n",
    "            # define closure\n",
    "\n",
    "            def closure():\n",
    "                LBFGS_optimizer.zero_grad()\n",
    "                output = model(X)\n",
    "                loss = -mll(output, Y)\n",
    "                if MAP:\n",
    "                    log_p = log_prior(model)\n",
    "                    loss -= log_p\n",
    "                LBFGS_optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                if log_param_path:\n",
    "                    for param_name in model.named_parameters():\n",
    "                        param_log_dict[param_name[0]].append(param_name[1][0].item())\n",
    "                if log_likelihood:\n",
    "                    likelihood_log.append(loss.item())\n",
    "                return loss\n",
    "            LBFGS_optimizer.step(closure)\n",
    "\n",
    "        # Zero gradients from previous iteration\n",
    "        optimizer.zero_grad()\n",
    "        # Output from model\n",
    "        output = model(X)\n",
    "        # Calc loss and backprop gradients\n",
    "        loss = -mll(output, Y)\n",
    "        if MAP:\n",
    "            log_p = log_prior(model)\n",
    "            loss -= log_p\n",
    "\n",
    "#        model.train_model(with_BFGS=with_BFGS)\n",
    "        current_loss = loss\n",
    "        # check if the current run is better than previous runs\n",
    "        if current_loss < best_loss:\n",
    "            # if it is the best, save all used parameters\n",
    "            best_loss = current_loss\n",
    "            for param_name, param in model.named_parameters():\n",
    "                optimal_parameters[param_name] = copy.deepcopy(param)\n",
    "\n",
    "        # set new random inital values\n",
    "        model.likelihood.noise_covar.noise = torch.rand(1) * (limits[\"Noise\"][1] - limits[\"Noise\"][0]) + limits[\"Noise\"][0]\n",
    "        #self.mean_module.constant = torch.rand(1) * (limits[\"Mean\"][1] - limits[\"Mean\"][0]) + limits[\"Mean\"][0]\n",
    "        for kernel in get_kernels_in_kernel_expression(model.covar_module):\n",
    "            hypers = limits[kernel._get_name()]\n",
    "            for hyperparameter in hypers:\n",
    "                new_value = torch.rand(1) * (hypers[hyperparameter][1] - hypers[hyperparameter][0]) + hypers[hyperparameter][0]\n",
    "                setattr(kernel, hyperparameter, new_value)\n",
    "\n",
    "        # print output if enabled\n",
    "        if options[\"training\"][\"print_optimizing_output\"]:\n",
    "            print(f\"HYPERPARAMETER OPTIMIZATION: Random Restart {iteration}: loss: {current_loss}, optimal loss: {best_loss}\")\n",
    "\n",
    "    # finally, set the hyperparameters those in the optimal run\n",
    "    model.initialize(**optimal_parameters)\n",
    "    output = model(X)\n",
    "    mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "    loss = -mll(output, Y)\n",
    "    if MAP:\n",
    "        log_p = log_prior(model)\n",
    "        loss -= log_p\n",
    "    if not loss == best_loss:\n",
    "        import pdb\n",
    "        pdb.set_trace()\n",
    "        print(loss)\n",
    "        print(best_loss)\n",
    "    if log_param_path:\n",
    "        logables = {\"training_log\": param_log_dict, \"likelihood_log\": log_likelihood}\n",
    "        return loss, logables\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-diddling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "END = 1\n",
    "COUNT = 5 \n",
    "train_x = torch.linspace(0, END, COUNT)\n",
    "train_y = torch.linspace(0, END, COUNT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simplest GP possible. SE with constant sigma_f \n",
    "class ExactGPModel(gpytorch.models.ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood):\n",
    "        super(ExactGPModel, self).__init__(train_x, train_y, likelihood)\n",
    "        self.mean_module = gpytorch.means.ZeroMean()\n",
    "        self.covar_module = gpytorch.kernels.RBFKernel()\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "\n",
    "# initialize likelihood and model\n",
    "likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "model = ExactGPModel(train_x, train_y, likelihood)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the MAP for 100 Iterations of ADAM and then 50 more of L-BFGS\n",
    "loss, training_log = optimize_hyperparameters(model, likelihood, 100, train_x, train_y, True, MAP=True, log_param_path=True, random_restarts=1, log_likelihood=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fd0503527d0>]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(training_log[\"training_log\"]['likelihood.noise_covar.raw_noise'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(-1.2098, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " {'neg MLL': tensor(-1.7903, grad_fn=<NegBackward0>),\n",
       "  'punish term': tensor(-3.0001, dtype=torch.float64),\n",
       "  'punish without replacement': tensor(inf, dtype=torch.float64),\n",
       "  'laplace without replacement': tensor(inf, dtype=torch.float64, grad_fn=<SubBackward0>),\n",
       "  'num_replaced': tensor(3),\n",
       "  'parameter list': ['likelihood.noise_covar.raw_noise',\n",
       "   'covar_module.raw_outputscale',\n",
       "   'covar_module.base_kernel.raw_lengthscale'],\n",
       "  'parameter values': tensor([[-5.9927],\n",
       "          [-5.9931],\n",
       "          [ 0.0000]]),\n",
       "  'corrected Hessian': tensor([[ 4.6427e+01, -6.5769e-16,  0.0000e+00],\n",
       "          [ 1.1284e-15,  4.6427e+01,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  4.6427e+01]], dtype=torch.float64),\n",
       "  'diag(constructed eigvals)': tensor([46.4268, 46.4268, 46.4268], dtype=torch.float64),\n",
       "  'original symmetrized Hessian': tensor([[ 0.0639, -0.0540,  0.0000],\n",
       "          [-0.0540,  0.0559,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000]], dtype=torch.float64),\n",
       "  'prior mean': tensor([[-3.5164],\n",
       "          [-1.6253],\n",
       "          [-0.2122]], dtype=torch.float64),\n",
       "  'diag(prior var)': tensor([12.8388,  5.0941,  3.5704], dtype=torch.float64),\n",
       "  'likelihood approximation': tensor(-1.2098, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       "  'Derivative time': 0.001775503158569336,\n",
       "  'Approximation time': 0.0004525184631347656,\n",
       "  'Correction time': 0.00020241737365722656,\n",
       "  'Prior generation time': 0.0001373291015625,\n",
       "  'Total time': 0.002559185028076172})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = model(train_x)\n",
    "loss = -mll(output, train_x)\n",
    "metrics.calculate_laplace(model, -loss, with_prior=True, param_punish_term=-1.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/besginow/anaconda3/envs/sage/lib/python3.10/site-packages/gpytorch/models/exact_gp.py:274: GPInputWarning: The input matches the stored training data. Did you forget to call model.train()?\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Get into evaluation (predictive posterior) mode\n",
    "model.eval()\n",
    "likelihood.eval()\n",
    "\n",
    "# Test points are regularly spaced along [0,1]\n",
    "# Make predictions by feeding model through likelihood\n",
    "with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "    output = model(train_x)\n",
    "    observed_pred = likelihood(output)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0034, 0.0009],\n",
       "        [0.0009, 0.0034]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "observed_pred.covariance_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0658, 0.0658])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "observed_pred.loc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Building: found in cache, done.Messages from stanc:\n",
      "Warning in '/tmp/httpstan_njirf1m1/model_mo3nc6ll.stan', line 5, column 12: A\n",
      "    control flow statement inside function softplus depends on argument v. At\n",
      "    '/tmp/httpstan_njirf1m1/model_mo3nc6ll.stan', line 32, column 74 to\n",
      "    column 82, the value of v depends on parameter(s): theta.\n",
      "Warning in '/tmp/httpstan_njirf1m1/model_mo3nc6ll.stan', line 5, column 12: A\n",
      "    control flow statement inside function softplus depends on argument v. At\n",
      "    '/tmp/httpstan_njirf1m1/model_mo3nc6ll.stan', line 32, column 51 to\n",
      "    column 59, the value of v depends on parameter(s): theta.\n",
      "Warning in '/tmp/httpstan_njirf1m1/model_mo3nc6ll.stan', line 5, column 12: A\n",
      "    control flow statement inside function softplus depends on argument v. At\n",
      "    '/tmp/httpstan_njirf1m1/model_mo3nc6ll.stan', line 32, column 120 to\n",
      "    column 128, the value of v depends on parameter(s): theta.\n",
      "Warning: The parameter theta has no priors. This means either no prior is\n",
      "    provided, or the prior(s) depend on data variables. In the later case,\n",
      "    this may be a false positive.\n",
      "Sampling:   0%\n",
      "Sampling: 100% (2000/2000)\n",
      "Sampling: 100% (2000/2000), done.\n",
      "Messages received during sampling:\n",
      "  Gradient evaluation took 2.2e-05 seconds\n",
      "  1000 transitions using 10 leapfrog steps per transition would take 0.22 seconds.\n",
      "  Adjust your expectations accordingly!\n",
      "  Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n",
      "  Exception: multi_normal_lpdf: Covariance matrix is not symmetric. Covariance matrix[1,2] = inf, but Covariance matrix[2,1] = inf (in '/tmp/httpstan_nkkkdkya/model_mo3nc6ll.stan', line 34, column 8 to column 32)\n",
      "  If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n",
      "  but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n",
      "  Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n",
      "  Exception: multi_normal_lpdf: Covariance matrix is not symmetric. Covariance matrix[1,2] = -nan, but Covariance matrix[2,1] = -nan (in '/tmp/httpstan_nkkkdkya/model_mo3nc6ll.stan', line 34, column 8 to column 32)\n",
      "  If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n",
      "  but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n",
      "  Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n",
      "  Exception: multi_normal_lpdf: Covariance matrix is not symmetric. Covariance matrix[1,2] = inf, but Covariance matrix[2,1] = inf (in '/tmp/httpstan_nkkkdkya/model_mo3nc6ll.stan', line 34, column 8 to column 32)\n",
      "  If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n",
      "  but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n",
      "  Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n",
      "  Exception: multi_normal_lpdf: Covariance matrix is not symmetric. Covariance matrix[1,2] = inf, but Covariance matrix[2,1] = inf (in '/tmp/httpstan_nkkkdkya/model_mo3nc6ll.stan', line 34, column 8 to column 32)\n",
      "  If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n",
      "  but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n",
      "  Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n",
      "  Exception: multi_normal_lpdf: Covariance matrix is not symmetric. Covariance matrix[1,2] = -nan, but Covariance matrix[2,1] = -nan (in '/tmp/httpstan_nkkkdkya/model_mo3nc6ll.stan', line 34, column 8 to column 32)\n",
      "  If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n",
      "  but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n",
      "  Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n",
      "  Exception: multi_normal_lpdf: Covariance matrix is not symmetric. Covariance matrix[1,2] = -nan, but Covariance matrix[2,1] = -nan (in '/tmp/httpstan_nkkkdkya/model_mo3nc6ll.stan', line 34, column 8 to column 32)\n",
      "  If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n",
      "  but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n",
      "/home/besginow/anaconda3/envs/sage/lib/python3.10/site-packages/gpytorch/models/exact_gp.py:274: GPInputWarning: The input matches the stored training data. Did you forget to call model.train()?\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-1185.0641)\n",
      "{'Kernel code': '\\n    functions {\\n        array[] real softplus(array[] real v){\\n            array[num_elements(v)] real r;\\n            for (d in 1:num_elements(v)){\\n                r[d] = log(1.0 + exp(v[d]));\\n            }\\n            return r;\\n        }\\n        real softplus(real v){\\n            return log(1.0 + exp(v));\\n        }\\n    }\\n    \\n    data {\\n        int N;\\n        int D;\\n        array[N] real x;\\n        vector[N] y;\\n        vector[D] t_mu;\\n        matrix[D, D] t_sigma;\\n    }\\n    \\n    parameters {\\n        vector<lower=-3.0>[D] theta;\\n    }\\n    \\n    model {\\n        matrix[N, N] K;\\n        vector[N] mu;\\n        theta ~ multi_normal(t_mu, t_sigma);\\n        K = (identity_matrix(dims(x)[1]).*softplus(theta[1])) + (softplus(theta[2]) .* gp_exp_quad_cov(x, 1.0, softplus(theta[3])));\\n        mu = zeros_vector(N);\\n        y ~ multi_normal(mu, K);\\n    }\\n    ', 'seed': 336947, 'Likelihood time': 1.7930924892425537, 'Model compile time': 0.11720156669616699, 'Sampling time': 0.81217360496521, 'Total time': 4.99091911315918, 'Bad entries': 0, 'Parameter statistics': {'theta.1': {'mu': -1.698602763614497, 'var': 1.6008702873414664}, 'theta.2': {'mu': -1.2287886063814633, 'var': 1.7155257820189904}, 'theta.3': {'mu': -0.0023142617869117217, 'var': 2.555815162338632}}, 'Parameter prior': {'mu': tensor([[-3.5164],\n",
      "        [-1.6253],\n",
      "        [-0.2122]]), 'var': tensor([[12.8388,  0.0000,  0.0000],\n",
      "        [ 0.0000,  5.0941,  0.0000],\n",
      "        [ 0.0000,  0.0000,  3.5704]])}, 'likelihood approximation': tensor(-1185.0641)}\n"
     ]
    }
   ],
   "source": [
    "from metrics import calculate_mc_STAN\n",
    "# Perform MCMC\n",
    "MCMC_approx, MC_log = calculate_mc_STAN(\n",
    "    model, likelihood, 1000)\n",
    "MC_logs = dict()\n",
    "print(MCMC_approx)\n",
    "print(MC_log)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sage",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
