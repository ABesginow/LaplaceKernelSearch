{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(1, \"..\")\n",
    "import pprint\n",
    "import gpytorch\n",
    "import torch\n",
    "import numpy as np\n",
    "import metrics\n",
    "import copy\n",
    "import configparser\n",
    "from experiment_functions import Experiment\n",
    "from GaussianProcess import ExactGPModel\n",
    "from globalParams import options, hyperparameter_limits\n",
    "import gpytorch\n",
    "from helpFunctions import get_string_representation_of_kernel as gsr\n",
    "from helpFunctions import clean_kernel_expression\n",
    "from helpFunctions import get_kernels_in_kernel_expression\n",
    "from helpFunctions import amount_of_base_kernels\n",
    "from itertools import product\n",
    "import json\n",
    "from kernelSearch import *\n",
    "from matplotlib import pyplot as plt\n",
    "from metrics import *\n",
    "from multiprocessing import Pool\n",
    "import numpy as np\n",
    "import os\n",
    "import pdb\n",
    "import pickle\n",
    "import random\n",
    "import tikzplotlib\n",
    "import time\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "# To run STAN in a Jupyter notebook\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def log_prior(model, theta_mu=None, sigma=None):\n",
    "    # params -\n",
    "    # TODO de-spaghettize this once the priors are coded properly\n",
    "    prior_dict = {'SE': {'raw_lengthscale' : {\"mean\": -0.21221139138922668 , \"std\":1.8895426067756804}},\n",
    "                  'MAT52': {'raw_lengthscale' :{\"mean\": 0.7993038925994188, \"std\":2.145122566357853 } },\n",
    "                  'MAT32': {'raw_lengthscale' :{\"mean\": 1.5711054238673443, \"std\":2.4453761235991216 } },\n",
    "                  'RQ': {'raw_lengthscale' :{\"mean\": -0.049841950913676276, \"std\":1.9426354614713097 },\n",
    "                          'raw_alpha' :{\"mean\": 1.882148553921053, \"std\":3.096431944989054 } },\n",
    "                  'PER':{'raw_lengthscale':{\"mean\": 0.7778461197268618, \"std\":2.288946656544974 },\n",
    "                          'raw_period_length':{\"mean\": 0.6485334993738499, \"std\":0.9930632050553377 } },\n",
    "                  'LIN':{'raw_variance' :{\"mean\": -0.8017903983055685, \"std\":0.9966569921354465 } },\n",
    "                  'c':{'raw_outputscale':{\"mean\": -1.6253091096349706, \"std\":2.2570021716661923 } },\n",
    "                  'noise': {'raw_noise':{\"mean\": -3.51640656386717, \"std\":3.5831320474767407 }}}\n",
    "    #prior_dict = {\"SE\": {\"raw_lengthscale\": {\"mean\": 0.891, \"std\": 2.195}},\n",
    "    #              \"MAT\": {\"raw_lengthscale\": {\"mean\": 1.631, \"std\": 2.554}},\n",
    "    #              \"PER\": {\"raw_lengthscale\": {\"mean\": 0.338, \"std\": 2.636},\n",
    "    #                      \"raw_period_length\": {\"mean\": 0.284, \"std\": 0.902}},\n",
    "    #              \"LIN\": {\"raw_variance\": {\"mean\": -1.463, \"std\": 1.633}},\n",
    "    #              \"c\": {\"raw_outputscale\": {\"mean\": -2.163, \"std\": 2.448}},\n",
    "    #              \"noise\": {\"raw_noise\": {\"mean\": -1.792, \"std\": 3.266}}}\n",
    "\n",
    "    variances_list = list()\n",
    "    debug_param_name_list = list()\n",
    "    theta_mu = list()\n",
    "    params = list()\n",
    "    covar_string = gsr(model.covar_module)\n",
    "    covar_string = covar_string.replace(\"(\", \"\")\n",
    "    covar_string = covar_string.replace(\")\", \"\")\n",
    "    covar_string = covar_string.replace(\" \", \"\")\n",
    "    covar_string = covar_string.replace(\"PER\", \"PER+PER\")\n",
    "    covar_string_list = [s.split(\"*\") for s in covar_string.split(\"+\")]\n",
    "    covar_string_list.insert(0, [\"LIKELIHOOD\"])\n",
    "    covar_string_list = list(chain.from_iterable(covar_string_list))\n",
    "    both_PER_params = False\n",
    "    for (param_name, param), cov_str in zip(model.named_parameters(), covar_string_list):\n",
    "        params.append(param.item())\n",
    "        debug_param_name_list.append(param_name)\n",
    "        # First param is (always?) noise and is always with the likelihood\n",
    "        if \"likelihood\" in param_name:\n",
    "            theta_mu.append(prior_dict[\"noise\"][\"raw_noise\"][\"mean\"])\n",
    "            variances_list.append(prior_dict[\"noise\"][\"raw_noise\"][\"std\"])\n",
    "            continue\n",
    "        else:\n",
    "            if (cov_str == \"PER\" or cov_str == \"RQ\") and not both_PER_params:\n",
    "                theta_mu.append(prior_dict[cov_str][param_name.split(\".\")[-1]][\"mean\"])\n",
    "                variances_list.append(prior_dict[cov_str][param_name.split(\".\")[-1]][\"std\"])\n",
    "                both_PER_params = True\n",
    "            elif (cov_str == \"PER\" or cov_str == \"RQ\") and both_PER_params:\n",
    "                theta_mu.append(prior_dict[cov_str][param_name.split(\".\")[-1]][\"mean\"])\n",
    "                variances_list.append(prior_dict[cov_str][param_name.split(\".\")[-1]][\"std\"])\n",
    "                both_PER_params = False\n",
    "            else:\n",
    "                try:\n",
    "                    theta_mu.append(prior_dict[cov_str][param_name.split(\".\")[-1]][\"mean\"])\n",
    "                    variances_list.append(prior_dict[cov_str][param_name.split(\".\")[-1]][\"std\"])\n",
    "                except Exception as E:\n",
    "                    import pdb\n",
    "                    pdb.set_trace()\n",
    "                    prev_cov = cov_str\n",
    "    theta_mu = torch.tensor(theta_mu)\n",
    "    theta_mu = theta_mu.unsqueeze(0).t()\n",
    "    sigma = torch.diag(torch.Tensor(variances_list))\n",
    "    sigma = sigma@sigma\n",
    "    prior = torch.distributions.MultivariateNormal(theta_mu.t(), sigma)\n",
    "\n",
    "    # for convention reasons I'm diving by the number of datapoints\n",
    "    return prior.log_prob(torch.Tensor(params)).item() / len(*model.train_inputs)\n",
    "\n",
    "def optimize_hyperparameters(model, likelihood, train_iterations, X, Y, with_BFGS=False, MAP=False, prior=None, **kwargs):\n",
    "    \"\"\"\n",
    "    find optimal hyperparameters either by BO or by starting from random initial values multiple times, using an optimizer every time\n",
    "    and then returning the best result\n",
    "    \"\"\"\n",
    "    ## Setup\n",
    "    # Log the parameters found during training\n",
    "    log_param_path = kwargs.get(\"log_param_path\", False)\n",
    "    log_likelihood = kwargs.get(\"log_likelihood\", False)\n",
    "    random_restarts = kwargs.get(\"random_restarts\", options[\"training\"][\"restarts\"]+1)\n",
    "    best_loss = 1e400\n",
    "    optimal_parameters = dict()\n",
    "    limits = hyperparameter_limits\n",
    "    if log_param_path:\n",
    "        param_log_dict = {param_name[0] : list() for param_name in model.named_parameters()}\n",
    "    if log_likelihood:\n",
    "        likelihood_log = list()\n",
    "    # start runs\n",
    "    for iteration in range(random_restarts):\n",
    "    #for iteration in range(2):\n",
    "        # optimize and determine loss\n",
    "        # Perform a training for AIC and Laplace\n",
    "        model.train()\n",
    "        likelihood.train()\n",
    "\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=0.1)\n",
    "\n",
    "        mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "\n",
    "        for i in range(train_iterations):\n",
    "            # Zero gradients from previous iteration\n",
    "            optimizer.zero_grad()\n",
    "            # Output from model\n",
    "            output = model(X)\n",
    "            # Calc loss and backprop gradients\n",
    "            loss = -mll(output, Y)\n",
    "            if MAP:\n",
    "                log_p = log_prior(model)\n",
    "                loss -= log_p\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if log_param_path:\n",
    "                for param_name in model.named_parameters():\n",
    "                    param_log_dict[param_name[0]].append(param_name[1][0].item())\n",
    "            if log_likelihood:\n",
    "                likelihood_log.append(loss.item())\n",
    "\n",
    "        if with_BFGS:\n",
    "            # Additional BFGS optimization to better ensure optimal parameters\n",
    "            # LBFGS_optimizer = torch.optim.LBFGS(model.parameters(), max_iter=50, line_search_fn='strong_wolfe')\n",
    "            LBFGS_optimizer = torch.optim.LBFGS(\n",
    "                model.parameters(), max_iter=50,\n",
    "                line_search_fn='strong_wolfe')\n",
    "            # define closure\n",
    "\n",
    "            def closure():\n",
    "                LBFGS_optimizer.zero_grad()\n",
    "                output = model(X)\n",
    "                loss = -mll(output, Y)\n",
    "                if MAP:\n",
    "                    log_p = log_prior(model)\n",
    "                    loss -= log_p\n",
    "                LBFGS_optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                if log_param_path:\n",
    "                    for param_name in model.named_parameters():\n",
    "                        param_log_dict[param_name[0]].append(param_name[1][0].item())\n",
    "                if log_likelihood:\n",
    "                    likelihood_log.append(loss.item())\n",
    "                return loss\n",
    "            LBFGS_optimizer.step(closure)\n",
    "\n",
    "        # Zero gradients from previous iteration\n",
    "        optimizer.zero_grad()\n",
    "        # Output from model\n",
    "        output = model(X)\n",
    "        # Calc loss and backprop gradients\n",
    "        loss = -mll(output, Y)\n",
    "        if MAP:\n",
    "            log_p = log_prior(model)\n",
    "            loss -= log_p\n",
    "\n",
    "#        model.train_model(with_BFGS=with_BFGS)\n",
    "        current_loss = loss\n",
    "        # check if the current run is better than previous runs\n",
    "        if current_loss < best_loss:\n",
    "            # if it is the best, save all used parameters\n",
    "            best_loss = current_loss\n",
    "            for param_name, param in model.named_parameters():\n",
    "                optimal_parameters[param_name] = copy.deepcopy(param)\n",
    "\n",
    "        # set new random inital values\n",
    "        model.likelihood.noise_covar.noise = torch.rand(1) * (limits[\"Noise\"][1] - limits[\"Noise\"][0]) + limits[\"Noise\"][0]\n",
    "        #self.mean_module.constant = torch.rand(1) * (limits[\"Mean\"][1] - limits[\"Mean\"][0]) + limits[\"Mean\"][0]\n",
    "        for kernel in get_kernels_in_kernel_expression(model.covar_module):\n",
    "            hypers = limits[kernel._get_name()]\n",
    "            for hyperparameter in hypers:\n",
    "                new_value = torch.rand(1) * (hypers[hyperparameter][1] - hypers[hyperparameter][0]) + hypers[hyperparameter][0]\n",
    "                setattr(kernel, hyperparameter, new_value)\n",
    "\n",
    "        # print output if enabled\n",
    "        if options[\"training\"][\"print_optimizing_output\"]:\n",
    "            print(f\"HYPERPARAMETER OPTIMIZATION: Random Restart {iteration}: loss: {current_loss}, optimal loss: {best_loss}\")\n",
    "\n",
    "    # finally, set the hyperparameters those in the optimal run\n",
    "    model.initialize(**optimal_parameters)\n",
    "    output = model(X)\n",
    "    mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "    loss = -mll(output, Y)\n",
    "    if MAP:\n",
    "        log_p = log_prior(model)\n",
    "        loss -= log_p\n",
    "    if not loss == best_loss:\n",
    "        import pdb\n",
    "        pdb.set_trace()\n",
    "        print(loss)\n",
    "        print(best_loss)\n",
    "    if log_param_path:\n",
    "        logables = {\"training_log\": param_log_dict, \"likelihood_log\": likelihood_log}\n",
    "        return loss, logables\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-diddling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "END = 1\n",
    "COUNT = 5 \n",
    "train_x = torch.linspace(0, END, COUNT)\n",
    "train_y = torch.linspace(0, END, COUNT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simplest GP possible. SE with constant sigma_f \n",
    "class ExactGPModel(gpytorch.models.ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood):\n",
    "        super(ExactGPModel, self).__init__(train_x, train_y, likelihood)\n",
    "        self.mean_module = gpytorch.means.ZeroMean()\n",
    "        self.covar_module = gpytorch.kernels.RBFKernel()\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "\n",
    "# initialize likelihood and model\n",
    "likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "model = ExactGPModel(train_x, train_y, likelihood)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the MAP for 100 Iterations of ADAM and then 50 more of L-BFGS\n",
    "loss, training_log = optimize_hyperparameters(model, likelihood, 200, train_x, train_y, True, MAP=True, log_param_path=True, random_restarts=1, log_likelihood=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f2ae2193970>]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#plt.plot(training_log[\"training_log\"]['likelihood.noise_covar.raw_noise'])\n",
    "plt.plot(training_log[\"likelihood_log\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(-2.2947, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " {'neg MLL': tensor(0.2947, grad_fn=<NegBackward0>),\n",
       "  'punish term': tensor(-2.0001, dtype=torch.float64),\n",
       "  'punish without replacement': tensor(5.2930, dtype=torch.float64),\n",
       "  'laplace without replacement': tensor(4.9983, dtype=torch.float64, grad_fn=<SubBackward0>),\n",
       "  'num_replaced': tensor(2),\n",
       "  'parameter list': ['likelihood.noise_covar.raw_noise',\n",
       "   'covar_module.raw_lengthscale'],\n",
       "  'parameter values': tensor([[-11.4403],\n",
       "          [  1.8133]]),\n",
       "  'corrected Hessian': tensor([[46.4268,  0.0000],\n",
       "          [ 0.0000, 46.4268]], dtype=torch.float64),\n",
       "  'diag(constructed eigvals)': tensor([46.4268, 46.4268], dtype=torch.float64),\n",
       "  'original symmetrized Hessian': tensor([[0.0139, 0.0082],\n",
       "          [0.0082, 0.0769]], dtype=torch.float64),\n",
       "  'prior mean': tensor([[-3.5164],\n",
       "          [-0.2122]], dtype=torch.float64),\n",
       "  'diag(prior var)': tensor([12.8388,  3.5704], dtype=torch.float64),\n",
       "  'likelihood approximation': tensor(-2.2947, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       "  'Derivative time': 0.0011882781982421875,\n",
       "  'Approximation time': 0.00041604042053222656,\n",
       "  'Correction time': 0.00019407272338867188,\n",
       "  'Prior generation time': 0.00010275840759277344,\n",
       "  'Total time': 0.0019032955169677734})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.calculate_laplace(model, -loss, with_prior=True, param_punish_term=-1.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/besginow/anaconda3/envs/sage/lib/python3.10/site-packages/gpytorch/models/exact_gp.py:274: GPInputWarning: The input matches the stored training data. Did you forget to call model.train()?\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Get into evaluation (predictive posterior) mode\n",
    "model.eval()\n",
    "likelihood.eval()\n",
    "\n",
    "# Test points are regularly spaced along [0,1]\n",
    "# Make predictions by feeding model through likelihood\n",
    "with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "    output = model(train_x)\n",
    "    observed_pred = likelihood(output)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2.0725e-04,  2.7955e-05, -7.9274e-06, -1.3113e-05,  6.9737e-06],\n",
       "        [ 2.7955e-05,  1.5534e-04,  3.6895e-05,  1.4186e-05, -1.3590e-05],\n",
       "        [-7.9274e-06,  3.6895e-05,  1.6315e-04,  3.6955e-05, -7.9870e-06],\n",
       "        [-1.3173e-05,  1.4186e-05,  3.6955e-05,  1.5534e-04,  2.8014e-05],\n",
       "        [ 6.9737e-06, -1.3590e-05, -7.9870e-06,  2.8014e-05,  2.0761e-04]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "observed_pred.covariance_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0021, 0.2460, 0.5004, 0.7544, 0.9969])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "observed_pred.loc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Building: found in cache, done.Messages from stanc:\n",
      "Warning in '/tmp/httpstan_gfm4it5p/model_3fzp33pw.stan', line 5, column 12: A\n",
      "    control flow statement inside function softplus depends on argument v. At\n",
      "    '/tmp/httpstan_gfm4it5p/model_3fzp33pw.stan', line 32, column 97 to\n",
      "    column 105, the value of v depends on parameter(s): theta.\n",
      "Warning in '/tmp/httpstan_gfm4it5p/model_3fzp33pw.stan', line 5, column 12: A\n",
      "    control flow statement inside function softplus depends on argument v. At\n",
      "    '/tmp/httpstan_gfm4it5p/model_3fzp33pw.stan', line 32, column 51 to\n",
      "    column 59, the value of v depends on parameter(s): theta.\n",
      "Warning: The parameter theta has no priors. This means either no prior is\n",
      "    provided, or the prior(s) depend on data variables. In the later case,\n",
      "    this may be a false positive.\n",
      "Sampling:   0%"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Sampling: 100% (2000/2000)\n",
      "Sampling: 100% (2000/2000), done.\n",
      "Messages received during sampling:\n",
      "  Gradient evaluation took 3.2e-05 seconds\n",
      "  1000 transitions using 10 leapfrog steps per transition would take 0.32 seconds.\n",
      "  Adjust your expectations accordingly!\n",
      "  Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n",
      "  Exception: multi_normal_lpdf: Covariance matrix is not symmetric. Covariance matrix[1,2] = -nan, but Covariance matrix[2,1] = -nan (in '/tmp/httpstan_bo63oqgh/model_3fzp33pw.stan', line 34, column 8 to column 32)\n",
      "  If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n",
      "  but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n",
      "  Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n",
      "  Exception: multi_normal_lpdf: Covariance matrix is not symmetric. Covariance matrix[1,2] = -nan, but Covariance matrix[2,1] = -nan (in '/tmp/httpstan_bo63oqgh/model_3fzp33pw.stan', line 34, column 8 to column 32)\n",
      "  If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n",
      "  but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n",
      "  Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n",
      "  Exception: multi_normal_lpdf: Covariance matrix is not symmetric. Covariance matrix[1,2] = -nan, but Covariance matrix[2,1] = -nan (in '/tmp/httpstan_bo63oqgh/model_3fzp33pw.stan', line 34, column 8 to column 32)\n",
      "  If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n",
      "  but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n",
      "/home/besginow/anaconda3/envs/sage/lib/python3.10/site-packages/gpytorch/models/exact_gp.py:274: GPInputWarning: The input matches the stored training data. Did you forget to call model.train()?\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-149.1557)\n",
      "{'Kernel code': '\\n    functions {\\n        array[] real softplus(array[] real v){\\n            array[num_elements(v)] real r;\\n            for (d in 1:num_elements(v)){\\n                r[d] = log(1.0 + exp(v[d]));\\n            }\\n            return r;\\n        }\\n        real softplus(real v){\\n            return log(1.0 + exp(v));\\n        }\\n    }\\n    \\n    data {\\n        int N;\\n        int D;\\n        array[N] real x;\\n        vector[N] y;\\n        vector[D] t_mu;\\n        matrix[D, D] t_sigma;\\n    }\\n    \\n    parameters {\\n        vector<lower=-3.0>[D] theta;\\n    }\\n    \\n    model {\\n        matrix[N, N] K;\\n        vector[N] mu;\\n        theta ~ multi_normal(t_mu, t_sigma);\\n        K = (identity_matrix(dims(x)[1]).*softplus(theta[1])) + gp_exp_quad_cov(x, 1.0, softplus(theta[2]));\\n        mu = zeros_vector(N);\\n        y ~ multi_normal(mu, K);\\n    }\\n    ', 'seed': 132244, 'Likelihood time': 1.482344150543213, 'Model compile time': 0.009731054306030273, 'Sampling time': 0.19740772247314453, 'Total time': 1.8648896217346191, 'Bad entries': 0, 'Parameter statistics': {'theta.1': {'mu': -2.1990858440937147, 'var': 0.6120814932028593}, 'theta.2': {'mu': 0.7892152765001696, 'var': 1.3715762905440243}}, 'Parameter prior': {'mu': tensor([[-3.5164],\n",
      "        [-0.2122]]), 'var': tensor([[12.8388,  0.0000],\n",
      "        [ 0.0000,  3.5704]])}, 'likelihood approximation': tensor(-149.1557), 'manual lp list': [tensor(-13.7378), tensor(1.4749), tensor(-20.3636), tensor(-20.6609), tensor(-3.2549), tensor(0.0957), tensor(-1.7632), tensor(-179.5125), tensor(-4.7914), tensor(-54.2491), tensor(-0.3379), tensor(-1413.8639), tensor(-2.2081), tensor(-6972.2456), tensor(-4087.0869), tensor(-4257.9937), tensor(-7875.3081), tensor(-9306.7178), tensor(-429.6600), tensor(-75.7630), tensor(1.9035), tensor(2.0842), tensor(-335.4910), tensor(-0.3241), tensor(-0.3598), tensor(-18.3102), tensor(-27.6899), tensor(-6.5005), tensor(-19.4788), tensor(-21.7503), tensor(-2.3046), tensor(-9.0742), tensor(-1.4299), tensor(-8.9468), tensor(-8.3605), tensor(1.8531), tensor(1.9893), tensor(-41.0753), tensor(-3.5580), tensor(-4.2454), tensor(-9.3395), tensor(-2.6932), tensor(-8.1253), tensor(-2.0262), tensor(-16.4475), tensor(0.2466), tensor(-11.4638), tensor(-9.2675), tensor(-20.8503), tensor(-16.6957), tensor(-21.3637), tensor(-308.6306), tensor(-27.2889), tensor(-4.8814), tensor(-4.8814), tensor(-4.3620), tensor(2.2010), tensor(-6.3564), tensor(-5.3502), tensor(0.9599), tensor(-906.9547), tensor(-2513.4263), tensor(-25.7267), tensor(-6.4180), tensor(-12.3645), tensor(1.9599), tensor(-22.1203), tensor(-6.2417), tensor(-10.2797), tensor(-13.3984), tensor(-3.6612), tensor(1.5594), tensor(-19.9462), tensor(-362.7525), tensor(-81.7289), tensor(-79.5172), tensor(-3.0809), tensor(-0.2970), tensor(-13.5909), tensor(-4.0394), tensor(-238.6834), tensor(1.9629), tensor(-28.1958), tensor(-160.1129), tensor(-22.6642), tensor(-30.7940), tensor(1.9648), tensor(2.1478), tensor(-45.4500), tensor(-0.2851), tensor(-9.9665), tensor(-31.1634), tensor(-0.2151), tensor(-1.2020), tensor(1.1802), tensor(1.3125), tensor(0.3308), tensor(0.3832), tensor(1.3940), tensor(1.6173), tensor(-0.9319), tensor(0.1864), tensor(-4.1091), tensor(-6.2920), tensor(-4.2751), tensor(1.3923), tensor(0.8797), tensor(-58.2589), tensor(-0.3772), tensor(-11.3708), tensor(0.5739), tensor(-34.8118), tensor(-567.8384), tensor(0.7923), tensor(-4.2232), tensor(-21.7422), tensor(-162.5802), tensor(-4.8910), tensor(-13.6636), tensor(-20.8837), tensor(1.3331), tensor(2.1298), tensor(-78.5002), tensor(0.8868), tensor(-195.6503), tensor(-4.5481), tensor(0.2177), tensor(1.7375), tensor(-12.2443), tensor(-7.3366), tensor(1.8698), tensor(-7.4239), tensor(-1.2191), tensor(-0.3833), tensor(0.9951), tensor(0.7628), tensor(-0.9101), tensor(1.2932), tensor(-7.7343), tensor(-11.5738), tensor(-5.2113), tensor(-12.0148), tensor(1.2324), tensor(-2570.9990), tensor(-3.6968), tensor(-9.8702), tensor(-7.3341), tensor(-127.8474), tensor(-1.0904), tensor(-1900.9539), tensor(-22.3567), tensor(-1435.7045), tensor(-130.3376), tensor(-5.7892), tensor(-0.6013), tensor(-85.3438), tensor(-50.9303), tensor(-20.8458), tensor(-1.2140), tensor(-25.3655), tensor(-20.8323), tensor(-6.3767), tensor(1.6508), tensor(1.0811), tensor(0.4765), tensor(-8.1009), tensor(-192.5814), tensor(-2.8053), tensor(-0.9318), tensor(1.1865), tensor(-7.3574), tensor(-71.6590), tensor(-155.2235), tensor(-5.7337), tensor(1.3884), tensor(-17.3398), tensor(-3.8074), tensor(-7.9077), tensor(0.1279), tensor(-8.5270), tensor(0.0104), tensor(-2.3921), tensor(-6.6370), tensor(-36.2066), tensor(-6.5930), tensor(-5.1913), tensor(-0.0305), tensor(0.4510), tensor(-11.1292), tensor(0.7258), tensor(-900.5405), tensor(1.1725), tensor(-1.7384), tensor(-12.5202), tensor(-3.2670), tensor(0.0839), tensor(-623.4883), tensor(1.3456), tensor(-9.4492), tensor(-1.3443), tensor(-18.9529), tensor(1.2777), tensor(0.7410), tensor(0.3833), tensor(-11.8486), tensor(1.4641), tensor(-60.3673), tensor(-14.9222), tensor(-38.5678), tensor(1.5218), tensor(-402.7864), tensor(-16.9982), tensor(-403.1171), tensor(1.6091), tensor(0.2547), tensor(-597.9656), tensor(-0.1895), tensor(-9.7918), tensor(-1.7599), tensor(-63.2146), tensor(-0.4672), tensor(-70.2610), tensor(-4.7055), tensor(1.5195), tensor(-91.1430), tensor(-2.8152), tensor(1.0318), tensor(0.7691), tensor(0.5402), tensor(-18.3337), tensor(-21.9943), tensor(-12.8808), tensor(1.1956), tensor(-0.1539), tensor(-6.3168), tensor(-2.3534), tensor(1.8839), tensor(-1.9417), tensor(-52.0200), tensor(-21.6915), tensor(-4.6726), tensor(-1.1466), tensor(-1.4351), tensor(-13.1271), tensor(1.0810), tensor(-18.3581), tensor(0.7198), tensor(-2.2967), tensor(1.3221), tensor(1.1468), tensor(1.2230), tensor(0.4145), tensor(1.3692), tensor(1.3692), tensor(-1146.9718), tensor(-4149.7788), tensor(-0.9238), tensor(-37.7161), tensor(-18.6894), tensor(0.2353), tensor(-92.4153), tensor(-143.3800), tensor(-105.1314), tensor(-534.2419), tensor(-14.7561), tensor(-6.5307), tensor(-219.1817), tensor(1.2247), tensor(0.5011), tensor(1.4430), tensor(-12.6558), tensor(-1.4137), tensor(-4.8998), tensor(-6.4931), tensor(1.1206), tensor(-3.7736), tensor(-2.4128), tensor(-15.8060), tensor(-10.8799), tensor(-4.2076), tensor(1.5839), tensor(1.5839), tensor(-8.6189), tensor(-8.8518), tensor(-12.4926), tensor(0.3669), tensor(-18.2275), tensor(-2.6335), tensor(-1.5232), tensor(-3.9827), tensor(-20.1176), tensor(-13.7042), tensor(-27.5622), tensor(-3.8024), tensor(-1.8936), tensor(-11.8494), tensor(-6.9312), tensor(0.4381), tensor(-26.9956), tensor(-0.3492), tensor(-39.1083), tensor(1.8274), tensor(-0.3364), tensor(-8.5567), tensor(-23.1100), tensor(-7.9581), tensor(-10.3321), tensor(-10.3214), tensor(1.0941), tensor(-183.2980), tensor(1.4306), tensor(-106.7775), tensor(-86.5416), tensor(-0.8242), tensor(-2.7133), tensor(-2.9019), tensor(0.5865), tensor(-3.2966), tensor(-3.1700), tensor(-1.9551), tensor(-6.0780), tensor(1.0855), tensor(0.9884), tensor(0.9100), tensor(0.9846), tensor(-0.3305), tensor(-1.6912), tensor(-21.9538), tensor(-1.1741), tensor(-13.5494), tensor(-3655.7095), tensor(-1.0964), tensor(-5.2298), tensor(-433.4973), tensor(-0.4393), tensor(-0.1859), tensor(-22.3003), tensor(1.8651), tensor(1.1433), tensor(-270.9548), tensor(-14.7429), tensor(-11.2073), tensor(-10.4375), tensor(1.1744), tensor(-1.2194), tensor(0.0173), tensor(-29.8759), tensor(-11.1801), tensor(-25.2318), tensor(-45.2367), tensor(-6.3205), tensor(-0.5888), tensor(-0.3514), tensor(-47.2800), tensor(-3.4884), tensor(-17.5925), tensor(-3.6993), tensor(-3.7074), tensor(-1.7378), tensor(-190.8058), tensor(-2.7505), tensor(-3.0907), tensor(-1322.9639), tensor(-115.8441), tensor(-39.4691), tensor(-26.5845), tensor(-180.1593), tensor(-272.4997), tensor(-187.1670), tensor(-25.3613), tensor(-0.2515), tensor(1.4181), tensor(-38.7284), tensor(-6.4660), tensor(-58.4941), tensor(-94.9535), tensor(-3.4624), tensor(-4.6697), tensor(-2.9388), tensor(-2.1506), tensor(0.1257), tensor(-30.2154), tensor(-2.4585), tensor(-13.0456), tensor(-13.6502), tensor(-6.8326), tensor(-2.4075), tensor(-5.5124), tensor(-5.7073), tensor(-11.1181), tensor(-1.0928), tensor(-0.7231), tensor(-4310.5327), tensor(-0.7579), tensor(-6.2769), tensor(0.6396), tensor(-6.2828), tensor(-3.2290), tensor(1.3385), tensor(-141.7036), tensor(-25.3322), tensor(-16.7270), tensor(-16.7270), tensor(-4.0974), tensor(-4.1582), tensor(1.7532), tensor(-13.1898), tensor(1.3634), tensor(-3.0592), tensor(0.3581), tensor(-0.6690), tensor(-0.1838), tensor(1.8327), tensor(-58.1621), tensor(-1.8980), tensor(0.6068), tensor(-2.2902), tensor(-3.6887), tensor(-1.7305), tensor(-2.8522), tensor(-11.8882), tensor(-39.9372), tensor(-0.4040), tensor(-1.7638), tensor(-12.7841), tensor(-1.9051), tensor(-5.7986), tensor(-3.3405), tensor(-43.9058), tensor(2.2273), tensor(-4.8916), tensor(-1.3438), tensor(-0.4281), tensor(-3.3024), tensor(-1173.6152), tensor(-72.8476), tensor(-75.4110), tensor(-47.2752), tensor(2.0224), tensor(-1.2328), tensor(-7.5771), tensor(-2.2573), tensor(-7.6847), tensor(-45.1760), tensor(-9.9583), tensor(-0.0692), tensor(-6.0550), tensor(0.2744), tensor(-19.7549), tensor(-61.6805), tensor(-5.3983), tensor(-6.6729), tensor(-7.1863), tensor(-11.7542), tensor(-4.0522), tensor(-2.0624), tensor(-310.4427), tensor(-4.3011), tensor(-4.6947), tensor(-4.6080), tensor(-2.7594), tensor(-212.6929), tensor(-37.9499), tensor(2.1730), tensor(-5.1006), tensor(-13.5184), tensor(-0.5148), tensor(0.5582), tensor(0.0370), tensor(-24.0027), tensor(-34.1528), tensor(-26.5500), tensor(0.5942), tensor(-1.8064), tensor(-140.4730), tensor(-39.8976), tensor(-143.6836), tensor(-4.7527), tensor(-7.1518), tensor(0.4714), tensor(0.3595), tensor(-1.9741), tensor(-3.5323), tensor(-21.5929), tensor(0.3124), tensor(0.3124), tensor(1.6001), tensor(-12.2971), tensor(-14.4075), tensor(-12.8860), tensor(-3406.9653), tensor(-1119.6031), tensor(-5.5115), tensor(-11.9633), tensor(-6.6389), tensor(-1.5977), tensor(-8.0446), tensor(-11.8729), tensor(-11.8729), tensor(-1.4829), tensor(-12.9321), tensor(1.2000), tensor(-4.4408), tensor(1.0084), tensor(-17.6022), tensor(1.0963), tensor(-16.3405), tensor(-0.3691), tensor(-15.7458), tensor(-0.4967), tensor(-0.5448), tensor(-4.1227), tensor(-7.0237), tensor(-5.1249), tensor(-6.3249), tensor(0.5972), tensor(1.9001), tensor(0.1989), tensor(-13.4920), tensor(0.5555), tensor(-27.7550), tensor(-32.9354), tensor(-91.7820), tensor(-33.8045), tensor(-6.2795), tensor(-0.2172), tensor(1.5189), tensor(-7.7436), tensor(0.4800), tensor(-16.8400), tensor(-5.6655), tensor(-7.7563), tensor(0.8205), tensor(-8.0315), tensor(-7.6142), tensor(-7.6142), tensor(-11.3655), tensor(-24.7598), tensor(-6.1269), tensor(-4.6574), tensor(-5.2970), tensor(-11.6363), tensor(1.5528), tensor(2.1177), tensor(-279.5981), tensor(-8.7341), tensor(-9.7615), tensor(-2.5840), tensor(-6.5891), tensor(0.7326), tensor(0.9135), tensor(-4086.7278), tensor(-8.8927), tensor(-8.4932), tensor(-1.8069), tensor(-376.9641), tensor(0.3952), tensor(-14.0681), tensor(-2.7682), tensor(-17.3094), tensor(-47.3630), tensor(-3.5151), tensor(-2.7071), tensor(-24.6425), tensor(-1800.2921), tensor(-9.2841), tensor(-3.0590), tensor(-3.0590), tensor(-210.6832), tensor(-20.5004), tensor(-29.2589), tensor(-1.5286), tensor(-2.7650), tensor(-2.7161), tensor(-1.1036), tensor(-4.7285), tensor(-1.4180), tensor(-8.8801), tensor(-2.3092), tensor(1.2914), tensor(-14.4566), tensor(-0.1067), tensor(-6.8780), tensor(-2.2920), tensor(-6.5703), tensor(-42.1784), tensor(-4363.0259), tensor(-4.2866), tensor(-2.2574), tensor(-5.0857), tensor(-10.7367), tensor(-0.4129), tensor(-11.7311), tensor(-5.0152), tensor(-3.7764), tensor(-225.2955), tensor(-0.2230), tensor(-219.7711), tensor(0.8818), tensor(-6.0954), tensor(-2.2562), tensor(-12.7539), tensor(-1.2124), tensor(-11.7298), tensor(-1.7890), tensor(-3.5032), tensor(-22.8615), tensor(-14.4994), tensor(0.3138), tensor(-52.3747), tensor(-2.0379), tensor(-7.6314), tensor(-9.3889), tensor(1.1070), tensor(-5.7635), tensor(1.0293), tensor(-10.7215), tensor(0.1984), tensor(-7.5317), tensor(0.8708), tensor(-11.6379), tensor(-2.1475), tensor(-1.8520), tensor(-0.4793), tensor(-0.4502), tensor(-2.0429), tensor(-10.0962), tensor(-234.2848), tensor(-2.8532), tensor(-2.5352), tensor(-2.2540), tensor(-2.3520), tensor(-3.1678), tensor(-3.5147), tensor(-17.1721), tensor(-6.4332), tensor(-49.5415), tensor(-54.4230), tensor(-468.1459), tensor(-14.8414), tensor(0.5773), tensor(-0.1104), tensor(-3.4712), tensor(-2.7167), tensor(-3.9506), tensor(-18.1361), tensor(0.5588), tensor(-15.1711), tensor(2.2874), tensor(2.1766), tensor(-13.2892), tensor(1.3956), tensor(-4.4023), tensor(-2.0024), tensor(-10.1303), tensor(1.5683), tensor(-41.3686), tensor(-1514.1520), tensor(-1832.8451), tensor(-4.7397), tensor(2.2424), tensor(0.4671), tensor(-13.2457), tensor(-17.0668), tensor(-0.6583), tensor(-537.1920), tensor(-0.2576), tensor(-1.3439), tensor(-9.4699), tensor(-5.6005), tensor(-13.4211), tensor(-4.4591), tensor(-3.0871), tensor(-11.4080), tensor(-13.0580), tensor(-0.2364), tensor(-12.2817), tensor(0.5474), tensor(-2.4875), tensor(-6.8673), tensor(-2.7368), tensor(0.5630), tensor(-17.3025), tensor(-3.4730), tensor(-3.9434), tensor(-6.0761), tensor(-11.5704), tensor(-6.1430), tensor(-1.1989), tensor(-1.2926), tensor(-14.9977), tensor(-7.8312), tensor(-9.9351), tensor(-4.6870), tensor(-6.4532), tensor(-382.6156), tensor(-19.0226), tensor(-0.0870), tensor(-9.1884), tensor(0.3630), tensor(-0.5518), tensor(-180.5900), tensor(-35.5668), tensor(-0.4424), tensor(-2.4749), tensor(-0.0758), tensor(-4.6671), tensor(-16.2255), tensor(-1.4379), tensor(-0.0845), tensor(-6649.5010), tensor(-516.7011), tensor(0.5017), tensor(-2.0118), tensor(1.4612), tensor(1.5633), tensor(0.8026), tensor(0.0416), tensor(-53.8165), tensor(-5.9156), tensor(-118.9473), tensor(-7.8030), tensor(-1.1853), tensor(-3.1633), tensor(-2.6880), tensor(-2.5282), tensor(-143.7375), tensor(-13.0536), tensor(0.6208), tensor(-8.1287), tensor(-0.8017), tensor(-9.1457), tensor(-1.4745), tensor(1.2751), tensor(1.2086), tensor(1.1758), tensor(-0.0503), tensor(-34.7737), tensor(0.9214), tensor(-71.3790), tensor(-34.6270), tensor(-1.2501), tensor(-557.4159), tensor(-303.6141), tensor(-2.2837), tensor(-6.0804), tensor(0.4416), tensor(-11.1790), tensor(-10.1218), tensor(-19.8154), tensor(-3.4413), tensor(-8.4143), tensor(-6.2890), tensor(-122.0254), tensor(-1.6938), tensor(0.7093), tensor(0.6374), tensor(-137.4934), tensor(-0.0678), tensor(-401.2696), tensor(0.6879), tensor(-1.6111), tensor(-32.4420), tensor(-1.4774), tensor(-8.5109), tensor(-130.1234), tensor(-5.7988), tensor(-0.9246), tensor(1.3705), tensor(-18.3469), tensor(-10.8054), tensor(-11.9828), tensor(-1.0558), tensor(-0.6443), tensor(-18.5928), tensor(-3.5796), tensor(-6.9582), tensor(-4.6587), tensor(-2.8810), tensor(0.1584), tensor(-27.4698), tensor(0.2799), tensor(-1.6643), tensor(-2780.3599), tensor(-2977.1436), tensor(-7.5960), tensor(-3.5655), tensor(-1.1411), tensor(-5.1273), tensor(-436.7641), tensor(-1100.1074), tensor(1.5742), tensor(-6.9586), tensor(2.2314), tensor(0.2232), tensor(-35.1083), tensor(2.1653), tensor(2.0037), tensor(-5.4763), tensor(-188.7769), tensor(-0.7859), tensor(-8.8488), tensor(1.8394), tensor(1.1746), tensor(-9.9859), tensor(1.9155), tensor(0.5208), tensor(-203.3655), tensor(-4.6377), tensor(0.1044), tensor(1.5689), tensor(0.1251), tensor(-1245.3558), tensor(2.0159), tensor(0.9747), tensor(1.4694), tensor(-25.9187), tensor(-6.9828), tensor(-6.2045), tensor(-0.8907), tensor(0.2070), tensor(-1.8857), tensor(-3.9508), tensor(-0.8168), tensor(-22.9973), tensor(1.9176), tensor(-13.1731), tensor(-15.8551), tensor(0.2004), tensor(-11.9619), tensor(-6.4774), tensor(-1.1975), tensor(-0.3914), tensor(-5.3273), tensor(-1.7130), tensor(-3.1730), tensor(-2.3581), tensor(-2.0744), tensor(-50.6064), tensor(-1.6441), tensor(0.0377), tensor(-18.1021), tensor(0.4809), tensor(-0.3092), tensor(-46.6785), tensor(-1.8744), tensor(-1.8551), tensor(-1.9062), tensor(-0.6784), tensor(-0.0331), tensor(-1.3258), tensor(-0.6686), tensor(-7.9261), tensor(-68.4441), tensor(-1009.8057), tensor(1.5221), tensor(-13.7669), tensor(1.5888), tensor(-31.0889), tensor(-24.8193), tensor(-27.6028), tensor(-26.1042), tensor(-459.6739), tensor(-5.2656), tensor(-24.2382), tensor(0.0647), tensor(-10.8265), tensor(-10.8265), tensor(-30.1497), tensor(-151.6856), tensor(0.7934), tensor(-4.4992), tensor(-139.8573), tensor(-0.3488), tensor(1.0115), tensor(-6.3064), tensor(0.9732), tensor(-50.4943), tensor(0.9373), tensor(-1.2119), tensor(-28.7722), tensor(-1.4910), tensor(1.0151), tensor(-665.4710), tensor(-2834.9924), tensor(-17.5400), tensor(0.3821), tensor(0.4514), tensor(-1.2891), tensor(-17.2509), tensor(-7.8948), tensor(-1.3595), tensor(-8.2157), tensor(1.6693), tensor(-4.2804), tensor(-1.3855), tensor(-14.9221), tensor(-100.3427), tensor(-152.0541), tensor(-929.3781), tensor(-1.3195), tensor(-0.6928), tensor(-1.2906), tensor(-0.7613), tensor(-970.7040), tensor(-5111.1260), tensor(-2730.6194), tensor(-3.9217), tensor(-692.5597), tensor(-1638.9863), tensor(-1350.2997), tensor(-0.7641), tensor(-7.3380), tensor(-13.1094), tensor(-9.1547), tensor(1.4484), tensor(-0.4925), tensor(-1.2177), tensor(-0.2807), tensor(-2.2944), tensor(-22.6296), tensor(-8.7093), tensor(-17.1114), tensor(-3.8210), tensor(-144.2783), tensor(-5.9541), tensor(-964.2514), tensor(-3168.1831), tensor(-5383.4028), tensor(-14.8011), tensor(-5.8409), tensor(-1.8396), tensor(-11.3381), tensor(0.8479), tensor(0.6957), tensor(-3.4978), tensor(1.0282), tensor(-72.7662), tensor(-8.0646), tensor(-2.8583), tensor(-181.1229), tensor(-216.8282), tensor(0.2085), tensor(0.4985), tensor(-1832.1636), tensor(-2.9895), tensor(-4.8005), tensor(-12.2050), tensor(1.1604), tensor(-1.5402), tensor(-4.2093), tensor(0.2609), tensor(0.7873), tensor(-429.8254), tensor(-610.9084), tensor(-40.3943), tensor(-2.5926), tensor(-2.5926), tensor(-2.9473), tensor(-18.4792), tensor(-507.2661), tensor(-13.5566), tensor(-4.5762), tensor(-0.3079), tensor(-7.1639), tensor(-0.8125), tensor(-23.3694), tensor(-16.1664), tensor(-3.1129), tensor(-0.2547), tensor(-20.6700), tensor(-3.9220), tensor(-16.4923), tensor(-0.3132), tensor(-9.0816), tensor(0.9012), tensor(0.6877), tensor(0.8713), tensor(-35.6253), tensor(-1.7232), tensor(-4.4182), tensor(-10.2054), tensor(-10.1385), tensor(-7.7145), tensor(-3.7376), tensor(-1.3012), tensor(-0.9058), tensor(-7.5306), tensor(-48.6181), tensor(-9.4100), tensor(1.9229), tensor(-14.7805), tensor(1.3874), tensor(-10.4308), tensor(0.1924), tensor(-4.5673), tensor(-23.4765), tensor(-16.2733), tensor(-52.0468), tensor(-52.0468)], 'param draws dict': parameters   theta.1   theta.2\n",
      "draws                         \n",
      "0          -2.654194  0.291622\n",
      "1          -2.924279  1.412490\n",
      "2          -2.641351  0.050411\n",
      "3          -2.872759  0.165554\n",
      "4          -1.254910  0.803473\n",
      "...              ...       ...\n",
      "995        -1.615975  0.564770\n",
      "996        -1.651768 -0.288135\n",
      "997        -1.437914 -0.228933\n",
      "998        -1.309339 -0.521771\n",
      "999        -1.309339 -0.521771\n",
      "\n",
      "[1000 rows x 2 columns]}\n"
     ]
    }
   ],
   "source": [
    "from metrics import calculate_mc_STAN\n",
    "# Perform MCMC\n",
    "MCMC_approx, MC_log = calculate_mc_STAN(\n",
    "    model, likelihood, 1000, log_param_path=True, log_full_likelihood=True)\n",
    "MC_logs = dict()\n",
    "print(MCMC_approx)\n",
    "print(MC_log)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sage",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
