{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(1, \"..\")\n",
    "import pprint\n",
    "import gpytorch\n",
    "import torch\n",
    "import numpy as np\n",
    "import metrics\n",
    "import copy\n",
    "import configparser\n",
    "from experiment_functions import Experiment\n",
    "from GaussianProcess import ExactGPModel\n",
    "from globalParams import options, hyperparameter_limits\n",
    "import gpytorch\n",
    "from helpFunctions import get_string_representation_of_kernel as gsr\n",
    "from helpFunctions import clean_kernel_expression\n",
    "from helpFunctions import get_kernels_in_kernel_expression\n",
    "from helpFunctions import amount_of_base_kernels\n",
    "from itertools import product\n",
    "import json\n",
    "from kernelSearch import *\n",
    "from matplotlib import pyplot as plt\n",
    "from metrics import *\n",
    "from multiprocessing import Pool\n",
    "import numpy as np\n",
    "import os\n",
    "import pdb\n",
    "import pickle\n",
    "import random\n",
    "import tikzplotlib\n",
    "import time\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "# To run STAN in a Jupyter notebook\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "def log_prior(model, theta_mu=None, sigma=None):\n",
    "    # params -\n",
    "    # TODO de-spaghettize this once the priors are coded properly\n",
    "    prior_dict = {'SE': {'raw_lengthscale' : {\"mean\": -0.21221139138922668 , \"std\":1.8895426067756804}},\n",
    "                  'MAT52': {'raw_lengthscale' :{\"mean\": 0.7993038925994188, \"std\":2.145122566357853 } },\n",
    "                  'MAT32': {'raw_lengthscale' :{\"mean\": 1.5711054238673443, \"std\":2.4453761235991216 } },\n",
    "                  'RQ': {'raw_lengthscale' :{\"mean\": -0.049841950913676276, \"std\":1.9426354614713097 },\n",
    "                          'raw_alpha' :{\"mean\": 1.882148553921053, \"std\":3.096431944989054 } },\n",
    "                  'PER':{'raw_lengthscale':{\"mean\": 0.7778461197268618, \"std\":2.288946656544974 },\n",
    "                          'raw_period_length':{\"mean\": 0.6485334993738499, \"std\":0.9930632050553377 } },\n",
    "                  'LIN':{'raw_variance' :{\"mean\": -0.8017903983055685, \"std\":0.9966569921354465 } },\n",
    "                  'c':{'raw_outputscale':{\"mean\": -1.6253091096349706, \"std\":2.2570021716661923 } },\n",
    "                  'noise': {'raw_noise':{\"mean\": -3.51640656386717, \"std\":3.5831320474767407 }}}\n",
    "    #prior_dict = {\"SE\": {\"raw_lengthscale\": {\"mean\": 0.891, \"std\": 2.195}},\n",
    "    #              \"MAT\": {\"raw_lengthscale\": {\"mean\": 1.631, \"std\": 2.554}},\n",
    "    #              \"PER\": {\"raw_lengthscale\": {\"mean\": 0.338, \"std\": 2.636},\n",
    "    #                      \"raw_period_length\": {\"mean\": 0.284, \"std\": 0.902}},\n",
    "    #              \"LIN\": {\"raw_variance\": {\"mean\": -1.463, \"std\": 1.633}},\n",
    "    #              \"c\": {\"raw_outputscale\": {\"mean\": -2.163, \"std\": 2.448}},\n",
    "    #              \"noise\": {\"raw_noise\": {\"mean\": -1.792, \"std\": 3.266}}}\n",
    "\n",
    "    variances_list = list()\n",
    "    debug_param_name_list = list()\n",
    "    theta_mu = list()\n",
    "    params = list()\n",
    "    covar_string = gsr(model.covar_module)\n",
    "    covar_string = covar_string.replace(\"(\", \"\")\n",
    "    covar_string = covar_string.replace(\")\", \"\")\n",
    "    covar_string = covar_string.replace(\" \", \"\")\n",
    "    covar_string = covar_string.replace(\"PER\", \"PER+PER\")\n",
    "    covar_string_list = [s.split(\"*\") for s in covar_string.split(\"+\")]\n",
    "    covar_string_list.insert(0, [\"LIKELIHOOD\"])\n",
    "    covar_string_list = list(chain.from_iterable(covar_string_list))\n",
    "    both_PER_params = False\n",
    "    for (param_name, param), cov_str in zip(model.named_parameters(), covar_string_list):\n",
    "        params.append(param.item())\n",
    "        debug_param_name_list.append(param_name)\n",
    "        # First param is (always?) noise and is always with the likelihood\n",
    "        if \"likelihood\" in param_name:\n",
    "            theta_mu.append(prior_dict[\"noise\"][\"raw_noise\"][\"mean\"])\n",
    "            variances_list.append(prior_dict[\"noise\"][\"raw_noise\"][\"std\"])\n",
    "            continue\n",
    "        else:\n",
    "            if (cov_str == \"PER\" or cov_str == \"RQ\") and not both_PER_params:\n",
    "                theta_mu.append(prior_dict[cov_str][param_name.split(\".\")[-1]][\"mean\"])\n",
    "                variances_list.append(prior_dict[cov_str][param_name.split(\".\")[-1]][\"std\"])\n",
    "                both_PER_params = True\n",
    "            elif (cov_str == \"PER\" or cov_str == \"RQ\") and both_PER_params:\n",
    "                theta_mu.append(prior_dict[cov_str][param_name.split(\".\")[-1]][\"mean\"])\n",
    "                variances_list.append(prior_dict[cov_str][param_name.split(\".\")[-1]][\"std\"])\n",
    "                both_PER_params = False\n",
    "            else:\n",
    "                try:\n",
    "                    theta_mu.append(prior_dict[cov_str][param_name.split(\".\")[-1]][\"mean\"])\n",
    "                    variances_list.append(prior_dict[cov_str][param_name.split(\".\")[-1]][\"std\"])\n",
    "                except Exception as E:\n",
    "                    import pdb\n",
    "                    pdb.set_trace()\n",
    "                    prev_cov = cov_str\n",
    "    theta_mu = torch.tensor(theta_mu)\n",
    "    theta_mu = theta_mu.unsqueeze(0).t()\n",
    "    sigma = torch.diag(torch.Tensor(variances_list))\n",
    "    sigma = sigma@sigma\n",
    "    prior = torch.distributions.MultivariateNormal(theta_mu.t(), sigma)\n",
    "\n",
    "    # for convention reasons I'm diving by the number of datapoints\n",
    "    return prior.log_prob(torch.Tensor(params)).item() / len(*model.train_inputs)\n",
    "\n",
    "def optimize_hyperparameters(model, likelihood, train_iterations, X, Y, with_BFGS=False, MAP=False, prior=None):\n",
    "    \"\"\"\n",
    "    find optimal hyperparameters either by BO or by starting from random initial values multiple times, using an optimizer every time\n",
    "    and then returning the best result\n",
    "    \"\"\"\n",
    "    # setup\n",
    "    best_loss = 1e400\n",
    "    optimal_parameters = dict()\n",
    "    limits = hyperparameter_limits\n",
    "    # start runs\n",
    "    for iteration in range(options[\"training\"][\"restarts\"]+1):\n",
    "    #for iteration in range(2):\n",
    "        # optimize and determine loss\n",
    "        # Perform a training for AIC and Laplace\n",
    "        model.train()\n",
    "        likelihood.train()\n",
    "\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=0.1)\n",
    "\n",
    "        mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "\n",
    "        for i in range(train_iterations):\n",
    "            # Zero gradients from previous iteration\n",
    "            optimizer.zero_grad()\n",
    "            # Output from model\n",
    "            output = model(X)\n",
    "            # Calc loss and backprop gradients\n",
    "            loss = -mll(output, Y)\n",
    "            if MAP:\n",
    "                log_p = log_prior(model)\n",
    "                loss -= log_p\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        if with_BFGS:\n",
    "            # Additional BFGS optimization to better ensure optimal parameters\n",
    "            # LBFGS_optimizer = torch.optim.LBFGS(model.parameters(), max_iter=50, line_search_fn='strong_wolfe')\n",
    "            LBFGS_optimizer = torch.optim.LBFGS(\n",
    "                model.parameters(), max_iter=50,\n",
    "                line_search_fn='strong_wolfe')\n",
    "            # define closure\n",
    "\n",
    "            def closure():\n",
    "                LBFGS_optimizer.zero_grad()\n",
    "                output = model(X)\n",
    "                loss = -mll(output, Y)\n",
    "                if MAP:\n",
    "                    log_p = log_prior(model)\n",
    "                    loss -= log_p\n",
    "                LBFGS_optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                return loss\n",
    "            LBFGS_optimizer.step(closure)\n",
    "\n",
    "        # Zero gradients from previous iteration\n",
    "        optimizer.zero_grad()\n",
    "        # Output from model\n",
    "        output = model(X)\n",
    "        # Calc loss and backprop gradients\n",
    "        loss = -mll(output, Y)\n",
    "        if MAP:\n",
    "            log_p = log_prior(model)\n",
    "            loss -= log_p\n",
    "\n",
    "#        model.train_model(with_BFGS=with_BFGS)\n",
    "        current_loss = loss\n",
    "        # check if the current run is better than previous runs\n",
    "        if current_loss < best_loss:\n",
    "            # if it is the best, save all used parameters\n",
    "            best_loss = current_loss\n",
    "            for param_name, param in model.named_parameters():\n",
    "                optimal_parameters[param_name] = copy.deepcopy(param)\n",
    "\n",
    "        # set new random inital values\n",
    "        model.likelihood.noise_covar.noise = torch.rand(1) * (limits[\"Noise\"][1] - limits[\"Noise\"][0]) + limits[\"Noise\"][0]\n",
    "        #self.mean_module.constant = torch.rand(1) * (limits[\"Mean\"][1] - limits[\"Mean\"][0]) + limits[\"Mean\"][0]\n",
    "        for kernel in get_kernels_in_kernel_expression(model.covar_module):\n",
    "            hypers = limits[kernel._get_name()]\n",
    "            for hyperparameter in hypers:\n",
    "                new_value = torch.rand(1) * (hypers[hyperparameter][1] - hypers[hyperparameter][0]) + hypers[hyperparameter][0]\n",
    "                setattr(kernel, hyperparameter, new_value)\n",
    "\n",
    "        # print output if enabled\n",
    "        if options[\"training\"][\"print_optimizing_output\"]:\n",
    "            print(f\"HYPERPARAMETER OPTIMIZATION: Random Restart {iteration}: loss: {current_loss}, optimal loss: {best_loss}\")\n",
    "\n",
    "    # finally, set the hyperparameters those in the optimal run\n",
    "    model.initialize(**optimal_parameters)\n",
    "    output = model(X)\n",
    "    mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "    loss = -mll(output, Y)\n",
    "    if MAP:\n",
    "        log_p = log_prior(model)\n",
    "        loss -= log_p\n",
    "    if not loss == best_loss:\n",
    "        import pdb\n",
    "        pdb.set_trace()\n",
    "        print(loss)\n",
    "        print(best_loss)\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = torch.Tensor([0, 0])\n",
    "train_y = torch.Tensor([0.1, 0.1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will use the simplest form of GP model, exact inference\n",
    "class ExactGPModel(gpytorch.models.ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood):\n",
    "        super(ExactGPModel, self).__init__(train_x, train_y, likelihood)\n",
    "        self.mean_module = gpytorch.means.ZeroMean()\n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel())\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "\n",
    "# initialize likelihood and model\n",
    "likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "model = ExactGPModel(train_x, train_y, likelihood)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "adam = torch.optim.Adam(model.parameters(), 0.1)\n",
    "mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "for i in range(50):\n",
    "    output = model(train_x)\n",
    "    loss = -mll(output, train_x)\n",
    "    loss.backward() \n",
    "    adam.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(-1.2098, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " {'neg MLL': tensor(-1.7903, grad_fn=<NegBackward0>),\n",
       "  'punish term': tensor(-3.0001, dtype=torch.float64),\n",
       "  'punish without replacement': tensor(inf, dtype=torch.float64),\n",
       "  'laplace without replacement': tensor(inf, dtype=torch.float64, grad_fn=<SubBackward0>),\n",
       "  'num_replaced': tensor(3),\n",
       "  'parameter list': ['likelihood.noise_covar.raw_noise',\n",
       "   'covar_module.raw_outputscale',\n",
       "   'covar_module.base_kernel.raw_lengthscale'],\n",
       "  'parameter values': tensor([[-5.9927],\n",
       "          [-5.9931],\n",
       "          [ 0.0000]]),\n",
       "  'corrected Hessian': tensor([[ 4.6427e+01, -6.5769e-16,  0.0000e+00],\n",
       "          [ 1.1284e-15,  4.6427e+01,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  4.6427e+01]], dtype=torch.float64),\n",
       "  'diag(constructed eigvals)': tensor([46.4268, 46.4268, 46.4268], dtype=torch.float64),\n",
       "  'original symmetrized Hessian': tensor([[ 0.0639, -0.0540,  0.0000],\n",
       "          [-0.0540,  0.0559,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000]], dtype=torch.float64),\n",
       "  'prior mean': tensor([[-3.5164],\n",
       "          [-1.6253],\n",
       "          [-0.2122]], dtype=torch.float64),\n",
       "  'diag(prior var)': tensor([12.8388,  5.0941,  3.5704], dtype=torch.float64),\n",
       "  'likelihood approximation': tensor(-1.2098, dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       "  'Derivative time': 0.001775503158569336,\n",
       "  'Approximation time': 0.0004525184631347656,\n",
       "  'Correction time': 0.00020241737365722656,\n",
       "  'Prior generation time': 0.0001373291015625,\n",
       "  'Total time': 0.002559185028076172})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = model(train_x)\n",
    "loss = -mll(output, train_x)\n",
    "metrics.calculate_laplace(model, -loss, with_prior=True, param_punish_term=-1.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/besginow/anaconda3/envs/sage/lib/python3.10/site-packages/gpytorch/models/exact_gp.py:274: GPInputWarning: The input matches the stored training data. Did you forget to call model.train()?\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Get into evaluation (predictive posterior) mode\n",
    "model.eval()\n",
    "likelihood.eval()\n",
    "\n",
    "# Test points are regularly spaced along [0,1]\n",
    "# Make predictions by feeding model through likelihood\n",
    "with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "    output = model(train_x)\n",
    "    observed_pred = likelihood(output)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0034, 0.0009],\n",
       "        [0.0009, 0.0034]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "observed_pred.covariance_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0658, 0.0658])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "observed_pred.loc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Building: found in cache, done.Messages from stanc:\n",
      "Warning in '/tmp/httpstan_njirf1m1/model_mo3nc6ll.stan', line 5, column 12: A\n",
      "    control flow statement inside function softplus depends on argument v. At\n",
      "    '/tmp/httpstan_njirf1m1/model_mo3nc6ll.stan', line 32, column 74 to\n",
      "    column 82, the value of v depends on parameter(s): theta.\n",
      "Warning in '/tmp/httpstan_njirf1m1/model_mo3nc6ll.stan', line 5, column 12: A\n",
      "    control flow statement inside function softplus depends on argument v. At\n",
      "    '/tmp/httpstan_njirf1m1/model_mo3nc6ll.stan', line 32, column 51 to\n",
      "    column 59, the value of v depends on parameter(s): theta.\n",
      "Warning in '/tmp/httpstan_njirf1m1/model_mo3nc6ll.stan', line 5, column 12: A\n",
      "    control flow statement inside function softplus depends on argument v. At\n",
      "    '/tmp/httpstan_njirf1m1/model_mo3nc6ll.stan', line 32, column 120 to\n",
      "    column 128, the value of v depends on parameter(s): theta.\n",
      "Warning: The parameter theta has no priors. This means either no prior is\n",
      "    provided, or the prior(s) depend on data variables. In the later case,\n",
      "    this may be a false positive.\n",
      "Sampling:   0%\n",
      "Sampling: 100% (2000/2000)\n",
      "Sampling: 100% (2000/2000), done.\n",
      "Messages received during sampling:\n",
      "  Gradient evaluation took 2.2e-05 seconds\n",
      "  1000 transitions using 10 leapfrog steps per transition would take 0.22 seconds.\n",
      "  Adjust your expectations accordingly!\n",
      "  Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n",
      "  Exception: multi_normal_lpdf: Covariance matrix is not symmetric. Covariance matrix[1,2] = inf, but Covariance matrix[2,1] = inf (in '/tmp/httpstan_nkkkdkya/model_mo3nc6ll.stan', line 34, column 8 to column 32)\n",
      "  If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n",
      "  but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n",
      "  Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n",
      "  Exception: multi_normal_lpdf: Covariance matrix is not symmetric. Covariance matrix[1,2] = -nan, but Covariance matrix[2,1] = -nan (in '/tmp/httpstan_nkkkdkya/model_mo3nc6ll.stan', line 34, column 8 to column 32)\n",
      "  If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n",
      "  but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n",
      "  Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n",
      "  Exception: multi_normal_lpdf: Covariance matrix is not symmetric. Covariance matrix[1,2] = inf, but Covariance matrix[2,1] = inf (in '/tmp/httpstan_nkkkdkya/model_mo3nc6ll.stan', line 34, column 8 to column 32)\n",
      "  If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n",
      "  but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n",
      "  Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n",
      "  Exception: multi_normal_lpdf: Covariance matrix is not symmetric. Covariance matrix[1,2] = inf, but Covariance matrix[2,1] = inf (in '/tmp/httpstan_nkkkdkya/model_mo3nc6ll.stan', line 34, column 8 to column 32)\n",
      "  If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n",
      "  but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n",
      "  Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n",
      "  Exception: multi_normal_lpdf: Covariance matrix is not symmetric. Covariance matrix[1,2] = -nan, but Covariance matrix[2,1] = -nan (in '/tmp/httpstan_nkkkdkya/model_mo3nc6ll.stan', line 34, column 8 to column 32)\n",
      "  If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n",
      "  but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n",
      "  Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n",
      "  Exception: multi_normal_lpdf: Covariance matrix is not symmetric. Covariance matrix[1,2] = -nan, but Covariance matrix[2,1] = -nan (in '/tmp/httpstan_nkkkdkya/model_mo3nc6ll.stan', line 34, column 8 to column 32)\n",
      "  If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n",
      "  but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n",
      "/home/besginow/anaconda3/envs/sage/lib/python3.10/site-packages/gpytorch/models/exact_gp.py:274: GPInputWarning: The input matches the stored training data. Did you forget to call model.train()?\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-1185.0641)\n",
      "{'Kernel code': '\\n    functions {\\n        array[] real softplus(array[] real v){\\n            array[num_elements(v)] real r;\\n            for (d in 1:num_elements(v)){\\n                r[d] = log(1.0 + exp(v[d]));\\n            }\\n            return r;\\n        }\\n        real softplus(real v){\\n            return log(1.0 + exp(v));\\n        }\\n    }\\n    \\n    data {\\n        int N;\\n        int D;\\n        array[N] real x;\\n        vector[N] y;\\n        vector[D] t_mu;\\n        matrix[D, D] t_sigma;\\n    }\\n    \\n    parameters {\\n        vector<lower=-3.0>[D] theta;\\n    }\\n    \\n    model {\\n        matrix[N, N] K;\\n        vector[N] mu;\\n        theta ~ multi_normal(t_mu, t_sigma);\\n        K = (identity_matrix(dims(x)[1]).*softplus(theta[1])) + (softplus(theta[2]) .* gp_exp_quad_cov(x, 1.0, softplus(theta[3])));\\n        mu = zeros_vector(N);\\n        y ~ multi_normal(mu, K);\\n    }\\n    ', 'seed': 336947, 'Likelihood time': 1.7930924892425537, 'Model compile time': 0.11720156669616699, 'Sampling time': 0.81217360496521, 'Total time': 4.99091911315918, 'Bad entries': 0, 'Parameter statistics': {'theta.1': {'mu': -1.698602763614497, 'var': 1.6008702873414664}, 'theta.2': {'mu': -1.2287886063814633, 'var': 1.7155257820189904}, 'theta.3': {'mu': -0.0023142617869117217, 'var': 2.555815162338632}}, 'Parameter prior': {'mu': tensor([[-3.5164],\n",
      "        [-1.6253],\n",
      "        [-0.2122]]), 'var': tensor([[12.8388,  0.0000,  0.0000],\n",
      "        [ 0.0000,  5.0941,  0.0000],\n",
      "        [ 0.0000,  0.0000,  3.5704]])}, 'likelihood approximation': tensor(-1185.0641)}\n"
     ]
    }
   ],
   "source": [
    "from metrics import calculate_mc_STAN\n",
    "# Perform MCMC\n",
    "MCMC_approx, MC_log = calculate_mc_STAN(\n",
    "    model, likelihood, 1000)\n",
    "MC_logs = dict()\n",
    "print(MCMC_approx)\n",
    "print(MC_log)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sage",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
