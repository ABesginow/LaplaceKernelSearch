{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(1, \"..\")\n",
    "from collections import namedtuple\n",
    "import dill\n",
    "from helpFunctions import get_string_representation_of_kernel as gsr\n",
    "from helpFunctions import get_full_kernels_in_kernel_expression\n",
    "from globalParams import options, hyperparameter_limits\n",
    "import gpytorch\n",
    "from itertools import chain\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "from matplotlib.gridspec import GridSpec\n",
    "from matplotlib.patches import Ellipse\n",
    "import matplotlib.transforms as transforms\n",
    "from metrics import calculate_AIC as AIC, calculate_BIC as BIC, calculate_laplace as Laplace, NestedSampling as Nested, log_normalized_prior\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas\n",
    "from pygranso.pygranso import pygranso\n",
    "from pygranso.pygransoStruct import pygransoStruct\n",
    "from pygranso.private.getNvar import getNvarTorch\n",
    "from scipy.stats import multivariate_normal, spearmanr\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def periodic_1D(X):\n",
    "    return torch.sin(X[:,0])\n",
    "\n",
    "def periodic_2D(X):\n",
    "    return torch.sin(X[:,0]) * torch.sin(X[:,1])\n",
    "\n",
    "def parabola_1D(X):\n",
    "    return X[:,0]**2\n",
    "\n",
    "def product(X):\n",
    "    return X[:,0] * X[:,1]\n",
    "\n",
    "def periodic_sum(X):\n",
    "    return torch.sin(X[:,0] + X[:,1])\n",
    "\n",
    "def periodic_sincos(X):\n",
    "    return torch.sin(X[:,0]) * torch.cos(X[:,1])\n",
    "\n",
    "def linear_1D(X):\n",
    "    return X[:,0]\n",
    "\n",
    "def linear_2D(X):\n",
    "    return X[:,0]+X[:,1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameter_limits = {\"RBFKernel\": {\"lengthscale\": [1e-3,1]},\n",
    "                         \"MaternKernel\": {\"lengthscale\": [1e-3,1]},\n",
    "                         \"LinearKernel\": {\"variance\": [1e-4,1]},\n",
    "                         \"AffineKernel\": {\"variance\": [1e-4,1]},\n",
    "                         \"RQKernel\": {\"lengthscale\": [1e-3,1],\n",
    "                                      \"alpha\": [1e-3,1]},\n",
    "                         \"CosineKernel\": {\"period_length\": [1e-3,3]},\n",
    "                         \"PeriodicKernel\": {\"lengthscale\": [1e-3,1],\n",
    "                                            \"period_length\": [1e-3,3]},\n",
    "                         \"ScaleKernel\": {\"outputscale\": [1e-3,10]},\n",
    "                         \"Noise\": [1e-2,1e-1],\n",
    "                         \"MyPeriodKernel\":{\"period_length\": [1e-3,3]}}\n",
    "\n",
    "def random_reinit(model):\n",
    "    #print(\"Random reparameterization\")\n",
    "    #print(\"old parameters: \", list(model.named_parameters()))\n",
    "    for i, (param, limit) in enumerate(zip(model.parameters(), [{\"Noise\": hyperparameter_limits[\"Noise\"]},*[hyperparameter_limits[kernel] for kernel in get_full_kernels_in_kernel_expression(model.covar_module)]])):\n",
    "        param_name = list(limit.keys())[0]\n",
    "        new_param_value = torch.randn_like(param) * (limit[param_name][1] - limit[param_name][0]) + limit[param_name][0]\n",
    "        param.data = new_param_value\n",
    "    #print(\"new parameters: \", list(model.named_parameters()))\n",
    "\n",
    "\n",
    "def fixed_reinit(model, parameters):\n",
    "    for i, (param, value) in enumerate(zip(model.parameters(), parameters)):\n",
    "        param.data = torch.full_like(param.data, value)\n",
    "\n",
    "\n",
    "\n",
    "# Define the training loop\n",
    "def optimize_hyperparameters(model, likelihood, **kwargs):\n",
    "    \"\"\"\n",
    "    find optimal hyperparameters either by BO or by starting from random initial values multiple times, using an optimizer every time\n",
    "    and then returning the best result\n",
    "    \"\"\"\n",
    "\n",
    "    random_restarts = kwargs.get(\"random_restarts\", options[\"training\"][\"restarts\"]+1)\n",
    "    uninformed = kwargs.get(\"uninformed\", False)\n",
    "    random_restarts = int(2)\n",
    "    train_log = [list() for _ in range(random_restarts)]\n",
    "\n",
    "    \"\"\"\n",
    "    # The call that comes from GRANSO\n",
    "    user_halt = halt_log_fn(0, x, self.penaltyfn_at_x, np.zeros((n,1)),\n",
    "                                        get_bfgs_state_fn, H_QP,\n",
    "                                        1, 0, 1, stat_vec, self.stat_val, 0          )\n",
    "    \"\"\"\n",
    "    def log_fnct(*args):\n",
    "        train_log[restart].append((args[1], args[2].f))\n",
    "        return False \n",
    "\n",
    "    train_x = kwargs.get(\"X\", model.train_inputs)\n",
    "    train_y = kwargs.get(\"Y\", model.train_targets)\n",
    "    MAP = kwargs.get(\"MAP\", True)\n",
    "    double_precision = kwargs.get(\"double_precision\", False)\n",
    "\n",
    "    # Set up the likelihood and model\n",
    "    #likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "    #model = GPModel(train_x, train_y, likelihood)\n",
    "\n",
    "    # Define the negative log likelihood\n",
    "    mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "\n",
    "    # Set up the PyGRANSO optimizer\n",
    "    opts = pygransoStruct()\n",
    "    opts.torch_device = torch.device('cpu')\n",
    "    nvar = getNvarTorch(model.parameters())\n",
    "    opts.x0 = torch.nn.utils.parameters_to_vector(model.parameters()).detach().reshape(nvar,1)\n",
    "    opts.opt_tol = float(1e-10)\n",
    "    opts.limited_mem_size = int(100)\n",
    "    opts.globalAD = True\n",
    "    opts.double_precision = double_precision\n",
    "    opts.quadprog_info_msg = False\n",
    "    opts.print_level = int(0)\n",
    "    opts.halt_on_linesearch_bracket = False\n",
    "    opts.halt_log_fn = log_fnct\n",
    "\n",
    "    # Define the objective function\n",
    "    def objective_function(model):\n",
    "        output = model(train_x)\n",
    "        try:\n",
    "            # TODO PyGRANSO dying is a severe problem. as it literally exits the program instead of raising an error\n",
    "            #import pdb;pdb.set_trace()\n",
    "            loss = -mll(output, train_y)\n",
    "        except Exception as E:\n",
    "            print(\"LOG ERROR: Severe PyGRANSO issue. Loss is inf+0\")\n",
    "            loss = torch.tensor(np.inf, requires_grad=True) + torch.tensor(0)\n",
    "        if MAP:\n",
    "            # log_normalized_prior is in metrics.py \n",
    "            log_p = log_normalized_prior(model, uninformed=uninformed)\n",
    "            loss -= log_p\n",
    "        return [loss, None, None]\n",
    "\n",
    "    best_model_state_dict = model.state_dict()\n",
    "    best_likelihood_state_dict = likelihood.state_dict()\n",
    "\n",
    "    best_f = np.inf\n",
    "    for restart in range(random_restarts):\n",
    "        # Train the model using PyGRANSO\n",
    "        try:\n",
    "            soln = pygranso(var_spec=model, combined_fn=objective_function, user_opts=opts)\n",
    "            print(f\"Restart {restart} : trained parameters: {list(model.named_parameters())}\")\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            import pdb\n",
    "            pdb.set_trace()\n",
    "            pass\n",
    "\n",
    "        if soln.final.f < best_f:\n",
    "            best_f = soln.final.f\n",
    "            best_model_state_dict = model.state_dict()\n",
    "            best_likelihood_state_dict = likelihood.state_dict()\n",
    "        random_reinit(model)\n",
    "        opts.x0 = torch.nn.utils.parameters_to_vector(model.parameters()).detach().reshape(nvar,1)\n",
    "\n",
    "    model.load_state_dict(best_model_state_dict)\n",
    "    likelihood.load_state_dict(best_likelihood_state_dict)\n",
    "    print(f\"----\")\n",
    "    print(f\"Final best parameters: {list(model.named_parameters())} w. loss: {soln.final.f} (smaller=better)\")\n",
    "    print(f\"----\")\n",
    "\n",
    "    loss = -mll(model(train_x), train_y)\n",
    "    if MAP:\n",
    "        log_p = log_normalized_prior(model)\n",
    "        loss -= log_p\n",
    "\n",
    "    #print(f\"post training (best): {list(model.named_parameters())} w. loss: {soln.best.f}\")\n",
    "    #print(f\"post training (final): {list(model.named_parameters())} w. loss: {soln.final.f}\")\n",
    "    \n",
    "    #print(torch.autograd.grad(loss, [p for p in model.parameters()], retain_graph=True, create_graph=True, allow_unused=True))\n",
    "    # Return the trained model\n",
    "    return loss, model, likelihood, train_log\n",
    "\n",
    "\n",
    "def get_std_points(mu, K):\n",
    "    x, y = np.mgrid[-3:3:.1, -3:3:.1]\n",
    "    rv = multivariate_normal(mu, K)\n",
    "    L = np.linalg.cholesky(K)\n",
    "\n",
    "    data = np.dstack((x, y))\n",
    "    z = rv.pdf(data)\n",
    "\n",
    "    # Drawing the unit circle\n",
    "    # x^2 + y^2 = 1\n",
    "    precision = 50\n",
    "    unit_x = torch.cat([torch.linspace(-1, 1, precision), torch.linspace(-1, 1, precision)])\n",
    "    unit_y = torch.cat([torch.sqrt(1 - torch.linspace(-1, 1, precision)**2), -torch.sqrt(1 - torch.linspace(-1, 1, precision)**2)])\n",
    "\n",
    "    new_unit_x = list()\n",
    "    new_unit_y = list()\n",
    "\n",
    "    for tx, ty in zip(unit_x, unit_y):\n",
    "        res = np.array([tx, ty]) @ L\n",
    "        new_unit_x.append(mu[0] + 2*res[0])\n",
    "        new_unit_y.append(mu[1] + 2*res[1])\n",
    "    return new_unit_x, new_unit_y\n",
    "\n",
    "\n",
    "# Find all points inside the confidence ellipse\n",
    "def percentage_inside_ellipse(mu, K, points, sigma_level=2):\n",
    "    L = np.linalg.cholesky(K)\n",
    "    threshold = sigma_level ** 2\n",
    "    count = 0\n",
    "    for point in points:\n",
    "        res = np.array(point - mu) @ np.linalg.inv(L)\n",
    "        if res @ res <= threshold:\n",
    "            count += 1\n",
    "    return count / len(points)\n",
    "\n",
    "\n",
    "def log_dill(data, filename):\n",
    "    with open(filename, 'wb') as f:\n",
    "        dill.dump(data, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_parameter_progression(parameter_progression, losses=None, xlabel=None, ylabel=None, fig=None, ax=None, xdim=0, ydim=1, display_figure=True, return_figure=False, title_add=\"\"):\n",
    "    if not (fig and ax):\n",
    "        fig, ax = plt.subplots(1, 1, figsize=(8, 6))\n",
    "    \n",
    "    for i in range(1, len(parameter_progression)):\n",
    "\n",
    "\n",
    "        if losses is None:\n",
    "            colormap = plt.cm.viridis(i/len(parameter_progression))\n",
    "        else:\n",
    "            colormap = plt.cm.viridis(losses[i]/min(losses))\n",
    "        ax.plot([parameter_progression[i-1][xdim], parameter_progression[i][xdim]], [parameter_progression[i-1][ydim], parameter_progression[i][ydim]], color=colormap)\n",
    "\n",
    "        ax.annotate(\"\",\n",
    "                xy=(parameter_progression[i][xdim], parameter_progression[i][ydim]), xytext=(parameter_progression[i-1][xdim], parameter_progression[i-1][ydim]),  # Arrow from second-last to last point\n",
    "                arrowprops=dict(arrowstyle=\"->\", color=colormap, lw=2))\n",
    "    ax.xaxis.set_label_text(xlabel)\n",
    "    ax.yaxis.set_label_text(ylabel)\n",
    "    ax.set_title(f\"Parameter progression {title_add}\")\n",
    "    fig.colorbar(plt.cm.ScalarMappable(cmap=\"viridis\"), ax=ax)\n",
    "    #plt.colorbar(likelihood_surface_scatter)\n",
    "\n",
    "    if return_figure:\n",
    "        return fig, ax\n",
    "    if display_figure:\n",
    "        plt.show()\n",
    "    return None, None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "\n",
    "def plot_parameter_progression(parameter_progression, losses=None, xlabel=None, ylabel=None, fig=None, ax=None, xdim=0, ydim=1, display_figure=True, return_figure=False, title_add=\"\"):\n",
    "    if not (fig and ax):\n",
    "        fig, ax = plt.subplots(1, 1, figsize=(8, 6))\n",
    "    \n",
    "    # Reverse colormap by using \"viridis_r\"\n",
    "    cmap = plt.cm.viridis_r  \n",
    "\n",
    "    # Define a normalization based on loss values\n",
    "    norm = None\n",
    "    if losses is not None:\n",
    "        norm = mcolors.Normalize(vmin=min(losses[1:]), vmax=max(losses[1:]))  # Keep vmin and vmax as usual\n",
    "    \n",
    "    for i in range(1, len(parameter_progression)):\n",
    "        if losses is None:\n",
    "            colormap = cmap(i / len(parameter_progression))  # Use reversed colormap\n",
    "        else:\n",
    "            colormap = cmap(norm(losses[i]))  # Normalize and apply reversed colormap\n",
    "        \n",
    "        ax.plot([parameter_progression[i-1][xdim], parameter_progression[i][xdim]], \n",
    "                [parameter_progression[i-1][ydim], parameter_progression[i][ydim]], \n",
    "                color=colormap)\n",
    "\n",
    "        ax.annotate(\"\", xy=(parameter_progression[i][xdim], parameter_progression[i][ydim]), \n",
    "                    xytext=(parameter_progression[i-1][xdim], parameter_progression[i-1][ydim]),  \n",
    "                    arrowprops=dict(arrowstyle=\"->\", color=colormap, lw=2))\n",
    "    \n",
    "    ax.set_xlabel(xlabel)\n",
    "    ax.set_ylabel(ylabel)\n",
    "    ax.set_title(f\"Parameter progression {title_add}\")\n",
    "\n",
    "    # Add colorbar with reversed colormap\n",
    "    if losses is not None:\n",
    "        sm = plt.cm.ScalarMappable(cmap=cmap, norm=norm)  # Use reversed colormap here\n",
    "        sm.set_array([])  # Required for colorbar\n",
    "        fig.colorbar(sm, ax=ax, label=\"Loss value (higher = worse)\")\n",
    "\n",
    "    if return_figure:\n",
    "        return fig, ax\n",
    "    if display_figure:\n",
    "        plt.show()\n",
    "    return None, None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nested"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Stolen from https://matplotlib.org/stable/gallery/statistics/confidence_ellipse.html\n",
    "def confidence_ellipse(mu, K, ax, n_std=3.0, facecolor='none', **kwargs):\n",
    "    \"\"\"\n",
    "    Create a plot of the covariance confidence ellipse of *x* and *y*.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x, y : array-like, shape (n, )\n",
    "        Input data.\n",
    "\n",
    "    ax : matplotlib.axes.Axes\n",
    "        The Axes object to draw the ellipse into.\n",
    "\n",
    "    n_std : float\n",
    "        The number of standard deviations to determine the ellipse's radiuses.\n",
    "\n",
    "    **kwargs\n",
    "        Forwarded to `~matplotlib.patches.Ellipse`\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    matplotlib.patches.Ellipse\n",
    "    \"\"\"\n",
    "\n",
    "    cov = K\n",
    "    pearson = cov[0, 1]/np.sqrt(cov[0, 0] * cov[1, 1])\n",
    "    # Using a special case to obtain the eigenvalues of this\n",
    "    # two-dimensional dataset.\n",
    "    ell_radius_x = np.sqrt(1 + pearson)\n",
    "    ell_radius_y = np.sqrt(1 - pearson)\n",
    "    ellipse = Ellipse((0, 0), width=ell_radius_x * 2, height=ell_radius_y * 2,\n",
    "                      facecolor=facecolor, **kwargs)\n",
    "\n",
    "    # Calculating the standard deviation of x from\n",
    "    # the squareroot of the variance and multiplying\n",
    "    # with the given number of standard deviations.\n",
    "    scale_x = np.sqrt(cov[0, 0]) * n_std\n",
    "    mean_x = mu[0] \n",
    "\n",
    "    # calculating the standard deviation of y ...\n",
    "    scale_y = np.sqrt(cov[1, 1]) * n_std\n",
    "    mean_y = mu[1]\n",
    "\n",
    "    transf = transforms.Affine2D() \\\n",
    "        .rotate_deg(45) \\\n",
    "        .scale(scale_x, scale_y) \\\n",
    "        .translate(mean_x, mean_y)\n",
    "\n",
    "    ellipse.set_transform(transf + ax.transData)\n",
    "    return ax.add_patch(ellipse)\n",
    "\n",
    "def nested_sampling_plot(model, model_evidence_log, xdim=0, ydim=1, filter_type=\"none\", std_filter=1e-0, return_figure=False, title_add=\"\", fig=None, ax=None, display_figure=True, plot_mll_opt=False, mll_opt_params=None, plot_lap=False, Lap0_logs=None, LapAIC_logs=None, LapBIC_logs=None, lap_colors = [\"r\", \"pink\", \"black\"]):\n",
    "\n",
    "    if not (fig and ax):\n",
    "        fig, ax = plt.subplots(1, 1, figsize=(8, 6))\n",
    "\n",
    "    with open(f\"{model_evidence_log['res file']}\", \"rb\") as f:\n",
    "        res = dill.load(f)\n",
    "    \n",
    "    # Plot the actual figure\n",
    "    param_names = [l[0] for l in list(model.named_parameters())]\n",
    "\n",
    "    # Find the best value and the corresponding hyperparameters\n",
    "    best_idx = np.argmax(res.logl)\n",
    "    best_hyperparameters = res.samples[best_idx]\n",
    "\n",
    "    # Do an outlier cleanup on res.logz\n",
    "    logz_std = np.std(res.logz)\n",
    "    if filter_type == \"max\":\n",
    "        mask = res.logz >= max(res.logz)+std_filter*logz_std\n",
    "    elif filter_type == \"mean\":\n",
    "        raise NotImplementedError(\"This filter type is not implemented yet\")\n",
    "        logz_mean = np.mean(res.logz)\n",
    "        mask = np.all(logz_mean - abs(std_filter) * logz_std <= res.logz <= logz_mean + abs(std_filter) * logz_std)\n",
    "    elif filter_type == \"none\":\n",
    "        mask = res.logz == res.logz\n",
    "\n",
    "\n",
    "\n",
    "    likelihood_surface_scatter = ax.scatter(res.samples[:,xdim][mask], res.samples[:,ydim][mask], c=res.logz[mask], s=3)\n",
    "    # Best found hyperparameters\n",
    "    ax.scatter(best_hyperparameters[xdim], best_hyperparameters[ydim], c=\"r\", s=10)\n",
    "\n",
    "    if plot_mll_opt and not mll_opt_params is None:\n",
    "        ax.scatter(mll_opt_params[xdim], mll_opt_params[ydim], c=\"black\", s=10)\n",
    "        # Add a small text beside the point saying \"MLL\"\n",
    "        ax.text(mll_opt_params[xdim], mll_opt_params[ydim], \"MLL\", fontsize=12, color=\"black\", verticalalignment='center', horizontalalignment='right')\n",
    "    \n",
    "    coverages = list()\n",
    "    if plot_lap:\n",
    "        # Plot the Laplace levels\n",
    "        for lap_log, lap_color in zip([Lap0_logs, LapAIC_logs, LapBIC_logs], lap_colors):\n",
    "            if lap_log is None:\n",
    "                continue\n",
    "            lap_param_mu = lap_log[\"parameter values\"]\n",
    "            # Wait a minute, isn't the Hessian the inverse of the covariance matrix? Yes, see Murphy PML 1 eq. (7.228)\n",
    "            lap_param_cov_matr = torch.linalg.inv(lap_log[\"corrected Hessian\"])\n",
    "            # Calculate the amount of samples that are covered by the 1 sigma and 2 sigma interval based on the lap_mu and lap_cov values\n",
    "            lap_2_sig_coverage = percentage_inside_ellipse(lap_param_mu.flatten().numpy(), lap_param_cov_matr.numpy(), res.samples[mask])\n",
    "            coverages.append(lap_2_sig_coverage)\n",
    "            #ax.scatter(lap_param_mu[xdim], lap_param_mu[ydim], c=\"b\", s=10)\n",
    "\n",
    "            # Plot the std points\n",
    "            lap_mu_filtered = lap_param_mu.numpy()[[xdim, ydim]] \n",
    "            lap_cov_filtered = lap_param_cov_matr.numpy()[[xdim, ydim]][:,[xdim, ydim]]\n",
    "            #lap_var_ellipse_x, lap_var_ellipse_y = get_std_points(lap_mu_filtered.flatten(), lap_cov_filtered)\n",
    "            #plt.scatter(lap_var_ellipse_x, lap_var_ellipse_y, c=\"b\", s=1)\n",
    "            confidence_ellipse(lap_mu_filtered, lap_cov_filtered, ax, n_std=2, edgecolor=lap_color, lw=1)\n",
    "\n",
    "    ax.set_title(f\"#Samples: {sum(res.ncall)}; {coverages[0]*100:.0f}% inside 2 sigma\")\n",
    "    ax.set_xlabel(param_names[xdim])\n",
    "\n",
    "    plt.colorbar(likelihood_surface_scatter)\n",
    "\n",
    "    if return_figure:\n",
    "        return fig, ax\n",
    "    if display_figure:\n",
    "        plt.show()\n",
    "    return None, None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_data(X, Y, return_figure=False, title_add=\"\", figure=None, ax=None, display_figure=True):\n",
    "    fig = plt.figure(figsize=(8, 6))\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.plot(X.numpy(), Y.numpy(), 'k.')\n",
    "    ax.set_xlabel('X')\n",
    "    ax.set_ylabel('Y')\n",
    "    ax.set_title(f\"Data {title_add}\")\n",
    "    if not return_figure and display_figure:\n",
    "        plt.show()\n",
    "    else:\n",
    "        return fig, ax\n",
    "\n",
    "def plot_model(model, likelihood, X, Y, display_figure=True, return_figure=False, figure=None,\n",
    "               ax=None, loss_val=None, loss_type = None):\n",
    "    interval_length = torch.max(X) - torch.min(X)\n",
    "    shift = interval_length * options[\"plotting\"][\"border_ratio\"]\n",
    "    test_x = torch.linspace(torch.min(\n",
    "        X) - shift, torch.max(X) + shift, options[\"plotting\"][\"sample_points\"])\n",
    "\n",
    "    with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "        observed_pred = likelihood(model(test_x))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        if not (figure and ax):\n",
    "            figure, ax = plt.subplots(1, 1, figsize=(8, 6))\n",
    "\n",
    "        lower, upper = observed_pred.confidence_region()\n",
    "        ax.plot(X.numpy(), Y.numpy(), 'k.', zorder=2)\n",
    "        ax.plot(test_x.numpy(), observed_pred.mean.numpy(), color=\"b\", zorder=3)\n",
    "        amount_of_gradient_steps = 30\n",
    "        alpha_min = 0.05\n",
    "        alpha_max = 0.8\n",
    "        alpha = (alpha_max-alpha_min)/amount_of_gradient_steps\n",
    "        c = ax.fill_between(test_x.numpy(), lower.numpy(), upper.numpy(\n",
    "        ), alpha=alpha+alpha_min, zorder=1).get_facecolor()\n",
    "\n",
    "        for i in range(1, amount_of_gradient_steps):\n",
    "            ax.fill_between(test_x.numpy(), (lower+(i/amount_of_gradient_steps)*(upper-lower)).numpy(),\n",
    "                            (upper-(i/amount_of_gradient_steps)*(upper-lower)).numpy(), alpha=alpha, color=c, zorder=1)\n",
    "        if options[\"plotting\"][\"legend\"]:\n",
    "            ax.plot([], [], 'k.', label=\"Data\")\n",
    "            ax.plot([], [], 'b', label=\"Mean\")\n",
    "            ax.plot([], [], color=c, alpha=1.0, label=\"Confidence\")\n",
    "            ax.legend(loc=\"upper left\")\n",
    "        ax.set_xlabel(\"Normalized Input\")\n",
    "        ax.set_ylabel(\"Normalized Output\")\n",
    "        ax.set_title(f\"{loss_type}: {loss_val:.2f}\")\n",
    "    if not return_figure and display_figure:\n",
    "        plt.show()\n",
    "    else:\n",
    "        return figure, ax\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_3d_data(samples, xx, yy, return_figure=False, fig=None, ax=None, display_figure=True, title_add = \"\"):\n",
    "    \"\"\"\n",
    "    Similar to plot_3d_gp_samples, but color-codes each (xx, yy) point in 3D.\n",
    "    'samples' can be a single 1D tensor or multiple samples in a 2D tensor.\n",
    "    \"\"\"\n",
    "    if not (fig and ax):\n",
    "        fig = plt.figure(figsize=(8, 6))\n",
    "        ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "    #if samples.ndim == 1:\n",
    "    #    samples = samples.unsqueeze(0)\n",
    "\n",
    "    #z_vals = samples.reshape(xx.shape)\n",
    "    z_vals = samples\n",
    "    ax.scatter(xx.numpy(), yy.numpy(), z_vals.numpy(),\n",
    "                c=z_vals.numpy(), cmap='viridis', alpha=0.8)\n",
    "\n",
    "\n",
    "    # Plot shadows (projection on X-Y plane at z=0)\n",
    "    ax.scatter(xx.numpy(), yy.numpy(), \n",
    "               np.ones_like(z_vals)*min(z_vals.numpy()), \n",
    "               c='gray', alpha=0.3, marker='o')\n",
    "\n",
    "\n",
    "\n",
    "    ax.set_title(f'Data {title_add}')\n",
    "    ax.set_xlabel('X')\n",
    "    ax.set_ylabel('Y')\n",
    "    ax.set_zlabel('Output Value')\n",
    "    if not return_figure and display_figure:\n",
    "        plt.show()\n",
    "    else:\n",
    "        return fig, ax\n",
    "\n",
    "\n",
    "def plot_3d_gp_samples(samples, xx, yy, return_figure=False, fig=None, ax=None, display_figure=True):\n",
    "    \"\"\"\n",
    "    Visualize multiple samples drawn from a 2D-input (xx, yy) -> 1D-output GP in 3D.\n",
    "    Each sample in 'samples' should be a 1D tensor that can be reshaped to match xx, yy.\n",
    "    \"\"\"\n",
    "    if not (fig and ax):\n",
    "        fig = plt.figure(figsize=(8, 6))\n",
    "        ax = fig.add_subplot(111, projection='3d')\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    if samples.ndim == 1:\n",
    "        samples = samples.unsqueeze(0)\n",
    "    for i, sample in enumerate(samples):\n",
    "        z_vals = sample.reshape(xx.shape)\n",
    "        ax.plot_surface(xx.numpy(), yy.numpy(), z_vals.numpy(), alpha=0.4)\n",
    "\n",
    "    ax.set_title('GP Samples in 3D')\n",
    "    ax.set_xlabel('X')\n",
    "    ax.set_ylabel('Y')\n",
    "    ax.set_zlabel('Output')\n",
    "    if not return_figure and display_figure:\n",
    "        plt.show()\n",
    "    else:\n",
    "        return fig, ax\n",
    "\n",
    "\n",
    "def plot_3d_gp(model, likelihood, data=None, x_min=0.0, x_max=1.0, y_min=0.0, y_max=1.0,\n",
    "                resolution=50, return_figure=False, fig=None, ax=None, display_figure=True):\n",
    "    if not (fig and ax):\n",
    "        fig = plt.figure(figsize=(8, 6))\n",
    "        ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "\n",
    "    if data is not None:\n",
    "        if data.ndim == 1:\n",
    "            data = data.unsqueeze(0)\n",
    "\n",
    "        for sample in data:\n",
    "            xx = sample[0]\n",
    "            yy = sample[1]\n",
    "            zz = sample[2]\n",
    "            ax.scatter(xx.numpy(), yy.numpy(), zz.numpy(), color=\"red\", alpha=0.8)\n",
    "\n",
    "\n",
    "    model.eval()\n",
    "    likelihood.eval()\n",
    "\n",
    "    x_vals = torch.linspace(x_min, x_max, resolution)\n",
    "    y_vals = torch.linspace(y_min, y_max, resolution)\n",
    "    xx, yy = torch.meshgrid(x_vals, y_vals)\n",
    "    test_x = torch.stack([xx.reshape(-1), yy.reshape(-1)], dim=-1)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        preds = likelihood(model(test_x))\n",
    "        mean = preds.mean.reshape(resolution, resolution)\n",
    "        lower, upper = preds.confidence_region()\n",
    "        lower = lower.reshape(resolution, resolution)\n",
    "        upper = upper.reshape(resolution, resolution)\n",
    "\n",
    "\n",
    "    # Plot mean surface\n",
    "    ax.plot_surface(xx.numpy(), yy.numpy(), mean.numpy(), cmap='viridis', alpha=0.8)\n",
    "\n",
    "    # Plot lower and upper surfaces\n",
    "    ax.plot_surface(xx.numpy(), yy.numpy(), lower.numpy(), color='gray', alpha=0.2)\n",
    "    ax.plot_surface(xx.numpy(), yy.numpy(), upper.numpy(), color='gray', alpha=0.2)\n",
    "\n",
    "    ax.set_title('2D GP in 3D')\n",
    "    ax.set_xlabel('X1')\n",
    "    ax.set_ylabel('X2')\n",
    "    ax.set_zlabel('Mean and Variance Range')\n",
    "\n",
    "    if not return_figure and display_figure:\n",
    "        plt.show()\n",
    "    else:\n",
    "        return fig, ax\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DatasetInfo = namedtuple('DatasetInfo', ['name', 'path', 'dimension', 'description'])\n",
    "datasets_1d = {\n",
    "    \"alternating\" : DatasetInfo('alternating', 'datasets/alternating.csv', 1, 'Alternates between high and low'),\n",
    "    \"broaden\" : DatasetInfo('broaden', 'datasets/broaden.csv', 1, 'Data with increasing variance'),\n",
    "    \"large gap\" : DatasetInfo('large gap', 'datasets/large_gap.csv', 1, 'Contains large gap between X values'),\n",
    "    \"linear outlier\" : DatasetInfo('linear outlier', 'datasets/linear_outlier.csv', 1, 'Linear data with outlier in last entry'),\n",
    "    \"parabola\" : DatasetInfo('parabola', 'datasets/parabola.csv', 1, 'Parabola'),\n",
    "    \"periodic linear\" : DatasetInfo('periodic linear', 'datasets/periodic_linear.csv', 1, 'Periodic data that changes to becoming purely linear'),\n",
    "    \"periodic\" : DatasetInfo('periodic', 'datasets/periodic.csv', 1, 'Periodic data'),\n",
    "    \"v_lines\" : DatasetInfo('v_lines', 'datasets/v_lines.csv', 1, 'Vertical lines'),\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "# named tuple or dictionary for the datasets, same as 1D case\n",
    "DatasetInfo = namedtuple('DatasetInfo', ['name', 'path', 'dimension', 'description'])\n",
    "datasets_2d = {\n",
    "    \"away\": DatasetInfo('away', 'datasets/2D/away.csv', 2, 'Away from the center'),\n",
    "    \"bullseye\": DatasetInfo('bullseye', 'datasets/2D/bullseye.csv', 2, 'Two circles forming a bullseye'),\n",
    "    \"circle\": DatasetInfo('circle', 'datasets/2D/circle.csv', 2, 'Circle'),\n",
    "    \"dots\": DatasetInfo('dots', 'datasets/2D/dots.csv', 2, 'An equidistant dot grid'),\n",
    "    \"h_lines\": DatasetInfo('h_lines', 'datasets/2D/h_lines.csv', 2, 'Horizontal lines'),\n",
    "    \"high lines\": DatasetInfo('high lines', 'datasets/2D/high_lines.csv', 2, 'Horizontal lines with a large gap between them'),\n",
    "    \"slant down\": DatasetInfo('slant down', 'datasets/2D/slant_down.csv', 2, 'Straight lines slanting downwards'),\n",
    "    \"slant up\": DatasetInfo('slant up', 'datasets/2D/slant_up.csv', 2, 'Straight lines slanting upwards'),\n",
    "    \"star\": DatasetInfo('star', 'datasets/2D/star.csv', 2, 'Star shape'),\n",
    "    \"v_lines\": DatasetInfo('v_lines', 'datasets/2D/v_lines.csv', 2, 'Vertical lines'),\n",
    "    \"wide lines\": DatasetInfo('wide lines', 'datasets/2D/wide_lines.csv', 2, 'Vertical lines with a large gap between them'),\n",
    "    \"x_shape\": DatasetInfo('x_shape', 'datasets/2D/x_shape.csv', 2, 'X shape'),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GP definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class ExactGPModel(gpytorch.models.ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood, kernel_text=\"RBF\", weights=None):\n",
    "        super(ExactGPModel, self).__init__(train_x, train_y, likelihood)\n",
    "        self.mean_module = gpytorch.means.ZeroMean()\n",
    "\n",
    "        if kernel_text == \"C*C*SE\":\n",
    "            self.covar_module = gpytorch.kernels.ScaleKernel(\n",
    "                gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel()))\n",
    "        elif kernel_text == \"SE\":\n",
    "            self.covar_module = gpytorch.kernels.RBFKernel()\n",
    "        elif kernel_text == \"C*SE\":\n",
    "            self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel())\n",
    "        elif kernel_text == \"LIN\":\n",
    "            self.covar_module = gpytorch.kernels.LinearKernel()\n",
    "        elif kernel_text == \"C*LIN\":\n",
    "            self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.LinearKernel())\n",
    "        elif kernel_text == \"LIN*SE\":\n",
    "            self.covar_module = gpytorch.kernels.LinearKernel() * gpytorch.kernels.RBFKernel()\n",
    "        elif kernel_text == \"LIN*PER\":\n",
    "            self.covar_module = gpytorch.kernels.LinearKernel() * gpytorch.kernels.PeriodicKernel()\n",
    "        elif kernel_text == \"SE+SE\":\n",
    "            self.covar_module =  gpytorch.kernels.RBFKernel() + gpytorch.kernels.RBFKernel()\n",
    "        elif kernel_text == \"RQ\":\n",
    "            self.covar_module = gpytorch.kernels.RQKernel()\n",
    "        elif kernel_text == \"PER\":\n",
    "            self.covar_module = gpytorch.kernels.PeriodicKernel()\n",
    "        #elif kernel_text == \"PER+SE\":\n",
    "        #    if weights is None:\n",
    "        #        self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.PeriodicKernel()) + gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel())\n",
    "        #    else:\n",
    "        #        self.covar_module = weights[0]*gpytorch.kernels.PeriodicKernel() + weights[1]*gpytorch.kernels.RBFKernel()\n",
    "        elif kernel_text == \"PER*SE\":\n",
    "            self.covar_module = gpytorch.kernels.PeriodicKernel() * gpytorch.kernels.RBFKernel()\n",
    "        #elif kernel_text == \"PER*LIN\":\n",
    "        #    self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.PeriodicKernel()) * gpytorch.kernels.ScaleKernel(gpytorch.kernels.LinearKernel())\n",
    "        elif kernel_text == \"MAT32\":\n",
    "            self.covar_module = gpytorch.kernels.MaternKernel(nu=1.5)\n",
    "        elif kernel_text == \"MAT32+MAT52\":\n",
    "            self.covar_module =  gpytorch.kernels.MaternKernel(nu=1.5) + gpytorch.kernels.MaternKernel()\n",
    "        elif kernel_text == \"MAT32*PER\":\n",
    "            self.covar_module =  gpytorch.kernels.MaternKernel(nu=1.5) * gpytorch.kernels.PeriodicKernel()\n",
    "        elif kernel_text == \"MAT32+PER\":\n",
    "            self.covar_module =  gpytorch.kernels.MaternKernel(nu=1.5) + gpytorch.kernels.PeriodicKernel()\n",
    "        elif kernel_text == \"MAT32*SE\":\n",
    "            self.covar_module =  gpytorch.kernels.MaternKernel(nu=1.5) * gpytorch.kernels.RBFKernel()\n",
    "        elif kernel_text == \"MAT32+SE\":\n",
    "            self.covar_module =  gpytorch.kernels.MaternKernel(nu=1.5) + gpytorch.kernels.RBFKernel()\n",
    "        elif kernel_text == \"MAT52\":\n",
    "            self.covar_module = gpytorch.kernels.MaternKernel()\n",
    "        elif kernel_text == \"MAT52*PER\":\n",
    "            self.covar_module =  gpytorch.kernels.MaternKernel() * gpytorch.kernels.PeriodicKernel()\n",
    "        elif kernel_text == \"MAT52+SE\":\n",
    "            self.covar_module =  gpytorch.kernels.MaternKernel() + gpytorch.kernels.RBFKernel()\n",
    "        elif kernel_text == \"SE*SE\":\n",
    "            self.covar_module =  gpytorch.kernels.RBFKernel() * gpytorch.kernels.RBFKernel()\n",
    "        elif kernel_text == \"(SE+RQ)*PER\":\n",
    "            self.covar_module =  (gpytorch.kernels.RBFKernel() + gpytorch.kernels.RQKernel()) * gpytorch.kernels.PeriodicKernel()\n",
    "        elif kernel_text == \"SE+SE+SE\":\n",
    "            self.covar_module =  gpytorch.kernels.RBFKernel() + gpytorch.kernels.RBFKernel() + gpytorch.kernels.RBFKernel()\n",
    "        elif kernel_text == \"MAT32+(MAT52*PER)\":\n",
    "            self.covar_module =  gpytorch.kernels.MaternKernel(nu=1.5) + (gpytorch.kernels.MaternKernel() * gpytorch.kernels.PeriodicKernel())\n",
    "\n",
    "        #elif kernel_text == \"RQ*PER\":\n",
    "        #    self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RQKernel()) * gpytorch.kernels.ScaleKernel(gpytorch.kernels.PeriodicKernel())\n",
    "        #elif kernel_text == \"RQ*MAT32\":\n",
    "        #    self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RQKernel()) * gpytorch.kernels.ScaleKernel(gpytorch.kernels.MaternKernel(nu=1.5))\n",
    "        #elif kernel_text == \"RQ*SE\":\n",
    "        #    self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RQKernel()) * gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel())\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "\n",
    "\n",
    "\n",
    "class ExactMIGPModel(gpytorch.models.ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood, kernel_text=\"RBF\", weights=None):\n",
    "        super(ExactMIGPModel, self).__init__(train_x, train_y, likelihood)\n",
    "        self.mean_module = gpytorch.means.ZeroMean()\n",
    "        if kernel_text == \"[RBF; RBF]\":\n",
    "            self.covar_module = gpytorch.kernels.AdditiveStructureKernel(gpytorch.kernels.RBFKernel(active_dims=0) + gpytorch.kernels.RBFKernel(active_dims=1), num_dims=2)\n",
    "        elif kernel_text == \"[RBF; LIN]\":\n",
    "            self.covar_module = gpytorch.kernels.AdditiveStructureKernel(gpytorch.kernels.RBFKernel(active_dims=0) + gpytorch.kernels.LinearKernel(active_dims=1), num_dims=2)\n",
    "        elif kernel_text == \"[LIN; RBF]\":\n",
    "            self.covar_module = gpytorch.kernels.AdditiveStructureKernel(gpytorch.kernels.LinearKernel(active_dims=0) + gpytorch.kernels.RBFKernel(active_dims=1), num_dims=2)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Various 1D and 2D datasets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "possible_kenels = [\n",
    "    \"SE\",\n",
    "    \"C*SE\",\n",
    "    \"C*C*SE\",\n",
    "    \"LIN\",\n",
    "    \"C*LIN\",\n",
    "    \"LIN*SE\",\n",
    "    \"LIN*PER\",\n",
    "    \"SE+SE\",\n",
    "    \"RQ\",\n",
    "    \"PER\",\n",
    "    \"PER*SE\",\n",
    "    \"MAT32\",\n",
    "    \"MAT32+MAT52\",\n",
    "    \"MAT32*PER\",\n",
    "    \"MAT32+PER\",\n",
    "    \"MAT32*SE\",\n",
    "    \"MAT32+SE\",\n",
    "    \"MAT52\",\n",
    "    \"MAT52*PER\",\n",
    "    \"MAT52+SE\",\n",
    "    \"SE*SE\",\n",
    "    \"(SE+RQ)*PER\",\n",
    "    \"SE+SE+SE\",\n",
    "    \"MAT32+(MAT52*PER)\"]\n",
    "\n",
    "possible_datasets = [\n",
    "   \"alternating\",\n",
    "   \"broaden\",\n",
    "   \"large gap\",\n",
    "   \"linear outlier\",\n",
    "   \"parabola\",\n",
    "   \"periodic linear\",\n",
    "   \"periodic\",\n",
    "   \"v_lines\"\n",
    "]\n",
    "\n",
    "kernel_name = \"C*C*SE\"\n",
    "dataset_name = \"alternating\"\n",
    "dataset_addendum = \"\"#\"51 points\"\n",
    "data_normalization = True\n",
    "data_norm_y = False \n",
    "levels = [1e+4, 1e+5]\n",
    "uninformed = True\n",
    "\n",
    "log_path = f\"logs/{'x-normalized' if data_normalization else ''}_{dataset_name}_{dataset_addendum}/{kernel_name}\"\n",
    "if not os.path.exists(log_path):\n",
    "    os.makedirs(log_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1D data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Load the dataset\n",
    "if dataset_name in datasets_1d:\n",
    "    df = pandas.read_csv(datasets_1d[dataset_name].path, header=None)\n",
    "    train_x = torch.tensor(df[0], dtype=torch.float32)\n",
    "    train_y = torch.tensor(df[1], dtype=torch.float32)\n",
    "\n",
    "elif dataset_name == \"linear\":\n",
    "    train_x = torch.linspace(0, int(dataset_addendum.split(\" \")[0])-1, int(dataset_addendum.split(\" \")[0]))\n",
    "    train_y = train_x.clone()\n",
    "    #train_y[-1] = 0 \n",
    "\n",
    "\n",
    "# Z score normalization\n",
    "if data_normalization:\n",
    "    train_x = (train_x - train_x.mean()) / train_x.std()\n",
    "    if data_norm_y:\n",
    "        train_y = (train_y - train_y.mean()) / train_y.std()\n",
    "\n",
    "fig, ax = plot_data(train_x, train_y, title_add=dataset_name, return_figure=True)\n",
    "fig.savefig(f\"{log_path}/data.png\", bbox_inches='tight')\n",
    "#fig.savefig(f\"{log_path}/data.pgf\", bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define the GP model\n",
    "likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "model = ExactGPModel(train_x, train_y, likelihood, kernel_text=kernel_name)\n",
    "likelihood_MAP = gpytorch.likelihoods.GaussianLikelihood()\n",
    "model_MAP = ExactGPModel(train_x, train_y, likelihood_MAP, kernel_text=kernel_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_names = [l[0] for l in list(model.named_parameters())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Train the GPs\n",
    "model.train()\n",
    "likelihood.train()\n",
    "mll_loss, model, likelihood, mll_train_log = optimize_hyperparameters(model, likelihood, X=train_x, Y=train_y, MAP=False, uninformed=uninformed)\n",
    "mll_opt_params = [p.item() for p in model.parameters() if p.requires_grad]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the optimization path of the trainings, each in an individual plot, given the parameter of interest\n",
    "# The parameter progression is plotted as arrows pointing to the next parameter position\n",
    "# The arrows are colored according to the lateness of their progression\n",
    "# The colorbar shows the progression of the parameter\n",
    "\n",
    "\n",
    "random_restarts = 2\n",
    "\n",
    "f, axs = plt.subplots(1, (random_restarts), figsize=(4*(random_restarts), 4), sharey=True, layout=\"constrained\")\n",
    "if len(levels) == 1:\n",
    "    axs = [axs]\n",
    "\n",
    "xdim = 0\n",
    "ydim = 1\n",
    "# extract all the parameters\n",
    "\n",
    "parameter_paths = [[log[0] for log in mll_train_log[i]] for i in range(len(mll_train_log))]\n",
    "parameter_values = [[log[1] for log in mll_train_log[i]] for i in range(len(mll_train_log))]\n",
    "\n",
    "for i, ax in enumerate(axs):\n",
    "    # train log consists of a list of tuples, where the first element is the parameter vector and the second element is the loss\n",
    "    plot_parameter_progression(parameter_paths[i], losses=parameter_values[i], xlabel=param_names[xdim], ylabel=param_names[ydim], xdim=xdim, ydim=ydim, fig=f, ax=ax, display_figure=False, title_add=f\"restart {i}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_MAP.train()\n",
    "likelihood_MAP.train()\n",
    "map_loss, model_MAP, likelihood_MAP, map_train_log = optimize_hyperparameters(model_MAP, likelihood_MAP, X=train_x, Y=train_y, MAP=True, uninformed=uninformed)\n",
    "map_opt_params = [p.item() for p in model_MAP.parameters() if p.requires_grad]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "f, axs = plt.subplots(1, (random_restarts), figsize=(4*(random_restarts), 4), sharey=True, layout=\"constrained\")\n",
    "if len(levels) == 1:\n",
    "    axs = [axs]\n",
    "\n",
    "# extract all the parameters\n",
    "parameter_paths = [[log[0] for log in map_train_log[i]] for i in range(len(map_train_log))]\n",
    "parameter_values = [[log[1] for log in map_train_log[i]] for i in range(len(map_train_log))]\n",
    "\n",
    "xdim = 0\n",
    "ydim = 1\n",
    "\n",
    "for i, ax in enumerate(axs):\n",
    "    plot_parameter_progression(parameter_paths[i], losses=parameter_values[i], xlabel=param_names[xdim], ylabel=param_names[ydim], xdim=xdim, ydim=ydim, fig=f, ax=ax, display_figure=False, title_add=f\"restart {i}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "likelihood.eval()\n",
    "model_MAP.eval()\n",
    "likelihood_MAP.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "likelihood.eval()\n",
    "model_MAP.eval()\n",
    "likelihood_MAP.eval()\n",
    "fig, axs = plt.subplots(1, 2, figsize=(16, 6), sharey=True)\n",
    "ax = axs[0]\n",
    "ax_MAP = axs[1]\n",
    "plot_model(model, likelihood, train_x, train_y, return_figure=False, figure=fig, ax=ax, loss_val=mll_loss.item(), loss_type=\"mll\", display_figure=False)\n",
    "plot_model(model_MAP, likelihood_MAP, train_x, train_y, return_figure=False, figure=fig, ax=ax_MAP, loss_val=map_loss.item(), loss_type=\"map\")\n",
    "fig.savefig(f\"{log_path}/posterior.png\", bbox_inches='tight')\n",
    "#fig.savefig(f\"{log_path}/posterior.pgf\", bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()\n",
    "likelihood.train()\n",
    "model_MAP.train()\n",
    "likelihood_MAP.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unscaled_MLL = -mll_loss*len(*model.train_inputs)\n",
    "unscaled_MAP = -map_loss*len(*model.train_inputs)\n",
    "num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "## Calculate the metrics\n",
    "AIC_val, AIC_logs = AIC(unscaled_MLL, num_params)\n",
    "BIC_val, BIC_logs = BIC(unscaled_MLL, num_params, torch.tensor(len(train_x)))\n",
    "AIC_MAP_val, AIC_MAP_logs = AIC(unscaled_MAP, num_params)\n",
    "BIC_MAP_val, BIC_MAP_logs = BIC(unscaled_MAP, num_params, torch.tensor(len(train_x)))\n",
    "Lap0_val, Lap0_logs =     Laplace(model_MAP, unscaled_MAP.clone(), uninformed=uninformed, param_punish_term=0)\n",
    "LapAIC_val, LapAIC_logs = Laplace(model_MAP, unscaled_MAP.clone(), uninformed=uninformed, param_punish_term=-1)\n",
    "LapBIC_val, LapBIC_logs = Laplace(model_MAP, unscaled_MAP.clone(), uninformed=uninformed, param_punish_term=\"BIC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Lap0_logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LapAIC_logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LapBIC_logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_evidences = list()\n",
    "model_evidence_logs = list()\n",
    "uninformed = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxcall = 1e+1\n",
    "if maxcall in levels:\n",
    "    print(maxcall)\n",
    "    Nested_val_e1, Nested_logs_e1 = Nested(model, store_full=True, pickle_directory=os.path.join(log_path, \"Nested_log\"), maxcall=maxcall)\n",
    "    model_evidences.append(Nested_val_e1)\n",
    "    model_evidence_logs.append(Nested_logs_e1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxcall = 1e+2\n",
    "if maxcall in levels:\n",
    "    print(maxcall)\n",
    "    Nested_val_e2, Nested_logs_e2 = Nested(model, store_full=True, pickle_directory=os.path.join(log_path, \"Nested_log\"), maxcall=maxcall)\n",
    "    model_evidences.append(Nested_val_e2)\n",
    "    model_evidence_logs.append(Nested_logs_e2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxcall = 1e+3\n",
    "if maxcall in levels:\n",
    "    print(maxcall)\n",
    "    Nested_val_e3, Nested_logs_e3 = Nested(model, store_full=True, pickle_directory=os.path.join(log_path, \"Nested_log\"), maxcall=maxcall, uninformed=uninformed)\n",
    "    model_evidences.append(Nested_val_e3)\n",
    "    model_evidence_logs.append(Nested_logs_e3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxcall = 1e+4\n",
    "if maxcall in levels:\n",
    "    print(maxcall)\n",
    "    Nested_val_e4, Nested_logs_e4 = Nested(model, store_full=True, pickle_directory=os.path.join(log_path, \"Nested_log\"), maxcall=maxcall, uninformed=uninformed)\n",
    "    model_evidences.append(Nested_val_e4)\n",
    "    model_evidence_logs.append(Nested_logs_e4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxcall = 1e+5\n",
    "if maxcall in levels:\n",
    "    print(maxcall)\n",
    "    Nested_val_e5, Nested_logs_e5 = Nested(model, store_full=True, pickle_directory=os.path.join(log_path, \"Nested_log\"), maxcall=maxcall, uninformed=uninformed)\n",
    "    model_evidences.append(Nested_val_e5)\n",
    "    model_evidence_logs.append(Nested_logs_e5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for mod_log in model_evidence_logs:\n",
    "    with open(f\"{mod_log['res file']}\", \"rb\") as f:\n",
    "        res = dill.load(f)\n",
    "    print(f\"Num calls: {sum(res.ncall)}\")\n",
    "    print(f\"logz Error: {res[\"logzerr\"][-1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plot_mll_opt = True\n",
    "std_filter = -1e-3\n",
    "filter_type = \"max\" # (\"mean\"), \"max\", \"none\"\n",
    "xdim = 0\n",
    "ydim = 1\n",
    "\n",
    "param_names = [l[0] for l in list(model.named_parameters())]\n",
    "\n",
    "f, axs = plt.subplots(1, len(levels), figsize=(4*len(levels), 4), sharey=True, layout=\"constrained\")\n",
    "if len(levels) == 1:\n",
    "    axs = [axs]\n",
    "for ax, level, model_evidence_log in zip(axs, levels, model_evidence_logs):\n",
    "    if ax == axs[-1]:\n",
    "        nested_sampling_plot(model, model_evidence_log, xdim = xdim, ydim = ydim, filter_type=filter_type, std_filter=std_filter, return_figure=False, title_add=\"\", fig=f, ax=ax, display_figure=True, plot_mll_opt=plot_mll_opt, mll_opt_params=mll_opt_params, plot_lap=True, Lap0_logs=Lap0_logs, LapAIC_logs=LapAIC_logs, LapBIC_logs=LapBIC_logs, lap_colors = [\"r\", \"pink\", \"black\"])\n",
    "    else:\n",
    "        nested_sampling_plot(model, model_evidence_log, xdim = xdim, ydim = ydim, filter_type=filter_type, std_filter=std_filter, return_figure=False, title_add=\"\", fig=f, ax=ax, display_figure=False, plot_mll_opt=plot_mll_opt, mll_opt_params=mll_opt_params, plot_lap=True, Lap0_logs=Lap0_logs, LapAIC_logs=LapAIC_logs, LapBIC_logs=LapBIC_logs, lap_colors = [\"r\", \"pink\", \"black\"])\n",
    "    if ax == axs[0]:\n",
    "        ax.set_ylabel(param_names[ydim])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# Make a \"loop\" out of the Nested levels to compare them to each other\n",
    "f, axs = plt.subplots(1, len(levels), figsize=(4*len(levels), 4), sharey=True, layout=\"constrained\")\n",
    "if len(levels) == 1:\n",
    "    axs = [axs]\n",
    "for ax, level, model_evidence_log in zip(axs, levels, model_evidence_logs):\n",
    "    with open(f\"{model_evidence_log['res file']}\", \"rb\") as f:\n",
    "        res = dill.load(f)\n",
    "    \n",
    "    # Plot the actual figure\n",
    "    param_names = [l[0] for l in list(model.named_parameters())]\n",
    "    xdim = 0\n",
    "    ydim = 1\n",
    "\n",
    "    # Find the best value and the corresponding hyperparameters\n",
    "    best_idx = np.argmax(res.logl)\n",
    "    best_hyperparameters = res.samples[best_idx]\n",
    "\n",
    "    # Do an outlier cleanup on res.logz\n",
    "    logz_mean = np.mean(res.logz) # model_evidence_log[\"parameter statistics\"][\"mu\"] #\n",
    "    logz_std = np.std(res.logz)   # model_evidence_log[\"parameter statistics\"][\"std\"]#\n",
    "    std_filter = -1e-4\n",
    "    filter_type = \"max\" # \"mean\", \"max\", \"none\"\n",
    "    if filter_type == \"max\":\n",
    "        mask = res.logz >= max(res.logz)+std_filter*logz_std\n",
    "    elif filter_type == \"mean\":\n",
    "        mask = res.logz >= logz_mean+std_filter*logz_std\n",
    "    elif filter_type == \"none\":\n",
    "        mask = res.logz == res.logz\n",
    "\n",
    "\n",
    "\n",
    "    likelihood_surface_scatter = ax.scatter(res.samples[:,xdim][mask], res.samples[:,ydim][mask], c=res.logz[mask], s=3)\n",
    "    # Best found hyperparameters\n",
    "    ax.scatter(best_hyperparameters[xdim], best_hyperparameters[ydim], c=\"r\", s=10)\n",
    "\n",
    "    plot_mll_opt = False\n",
    "    if plot_mll_opt:\n",
    "        ax.scatter(mll_opt_params[xdim], mll_opt_params[ydim], c=\"black\", s=10)\n",
    "        # Add a small text beside the point saying \"MLL\"\n",
    "        ax.text(mll_opt_params[xdim], mll_opt_params[ydim], \"MLL\", fontsize=12, color=\"black\", verticalalignment='center', horizontalalignment='right')\n",
    "    #ax.scatter(map_opt_params[xdim], map_opt_params[ydim], c=\"b\", s=10)\n",
    "    ## Add a small text beside the point saying \"MAP\"\n",
    "    #ax.text(map_opt_params[xdim], map_opt_params[ydim], \"MAP\", fontsize=12, color=\"b\", verticalalignment='center', horizontalalignment='right')\n",
    "    \n",
    "    coverages = list()\n",
    "    # Plot the Laplace levels\n",
    "    for lap_log, lap_color in zip([Lap0_logs, LapAIC_logs, LapBIC_logs], [\"r\", \"pink\", \"black\"]):\n",
    "        lap_param_mu = lap_log[\"parameter values\"]\n",
    "        # Wait a minute, isn't the Hessian the inverse of the covariance matrix? Yes, see Murphy PML 1 eq. (7.228)\n",
    "        lap_param_cov_matr = torch.linalg.inv(lap_log[\"corrected Hessian\"])\n",
    "        # Calculate the amount of samples that are covered by the 1 sigma and 2 sigma interval based on the lap_mu and lap_cov values\n",
    "        lap_2_sig_coverage = percentage_inside_ellipse(lap_param_mu.flatten().numpy(), lap_param_cov_matr.numpy(), res.samples[mask])\n",
    "        coverages.append(lap_2_sig_coverage)\n",
    "        #ax.scatter(lap_param_mu[xdim], lap_param_mu[ydim], c=\"b\", s=10)\n",
    "\n",
    "        # Plot the std points\n",
    "        lap_mu_filtered = lap_param_mu.numpy()[[xdim, ydim]] \n",
    "        lap_cov_filtered = lap_param_cov_matr.numpy()[[xdim, ydim]][:,[xdim, ydim]]\n",
    "        #lap_var_ellipse_x, lap_var_ellipse_y = get_std_points(lap_mu_filtered.flatten(), lap_cov_filtered)\n",
    "        #plt.scatter(lap_var_ellipse_x, lap_var_ellipse_y, c=\"b\", s=1)\n",
    "        confidence_ellipse(lap_mu_filtered, lap_cov_filtered, ax, n_std=2, edgecolor=lap_color, lw=1)\n",
    "\n",
    "    ax.set_title(f\"#Samples: {sum(res.ncall)}; {coverages[0]*100:.0f}% inside 2 sigma\")\n",
    "    if ax == axs[0]:\n",
    "        ax.set_ylabel(param_names[ydim])\n",
    "    ax.set_xlabel(param_names[xdim])\n",
    "\n",
    "    plt.colorbar(likelihood_surface_scatter)\n",
    "\n",
    "# This doesn't do anything, somehow\n",
    "fig.suptitle(f\"{dataset_name} - {kernel_name}\")\n",
    "fig.savefig(f\"{log_path}/Nested_{filter_type}{std_filter}.png\", bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the best value and the corresponding hyperparameters\n",
    "best_idx = np.argmax(res.logl)\n",
    "best_hyperparameters = res.samples[best_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimal GP according to Nested sampling\n",
    "fixed_reinit(model, torch.tensor(best_hyperparameters))\n",
    "model.eval()\n",
    "likelihood.eval()\n",
    "# mll of best parameterization according to Nested sampling:\n",
    "mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "nested_top_mll = (mll(model(train_x), train_y))*len(train_x)\n",
    "nested_top_map = nested_top_mll+log_normalized_prior(model)*len(train_x)\n",
    "fig, ax = plot_model(model, likelihood, train_x, train_y, return_figure=True, loss_val=res.logl[best_idx], loss_type=\"Nested logl\")\n",
    "ax.title.set_text(f\"{ax.title.get_text()} ; MLL: {nested_top_mll.item():.2f}; MAP: {nested_top_map.item():.2f}\\n MLL (u) {-nested_top_mll/len(train_x):.2f}; MAP (u) {-nested_top_map/len(train_x):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect metric names and values into a list of tuples\n",
    "metrics_data = [\n",
    "    (\"MLL\", f\"{unscaled_MLL.item():.3f}\"),\n",
    "    (\"MLL (loss)\", f\"{mll_loss.item():.3f}\"),\n",
    "    (\"AIC\", f\"{AIC_val.item():.3f}\"),\n",
    "    (\"AIC (s)\", f\"{(AIC_val.item()*(-0.5)):.3f}\"),\n",
    "    (\"BIC\", f\"{BIC_val.item():.3f}\"),\n",
    "    (\"BIC (s)\", f\"{(BIC_val.item()*(-0.5)):.3f}\"),\n",
    "    \n",
    "\n",
    "]\n",
    "labels = [m[0] for m in metrics_data]\n",
    "values = [m[1] for m in metrics_data]\n",
    "# Print in transposed form\n",
    "print(\"|        | \" + \" | \".join(labels) + \" |\")\n",
    "print(\"|--------|\" + \"|\".join([\"-----------\" for _ in labels]) + \"|\")\n",
    "print(\"| Value  | \" + \" | \".join(values) + \" |\")\n",
    "print()\n",
    "\n",
    "metrics_data = [(\"MAP\", f\"{unscaled_MAP.item():.3f}\"),\n",
    "    (\"MAP (loss)\", f\"{map_loss.item():.3f}\"),\n",
    "    (\"AIC_M\", f\"{AIC_MAP_val.item():.3f}\"),\n",
    "    (\"AIC_M (s)\", f\"{(AIC_MAP_val.item()*(-0.5)):.3f}\"),\n",
    "    (\"BIC_M\", f\"{BIC_MAP_val.item():.3f}\"),\n",
    "    (\"BIC_M (s)\", f\"{(BIC_MAP_val.item()*(-0.5)):.3f}\"),\n",
    "]\n",
    "\n",
    "labels = [m[0] for m in metrics_data]\n",
    "values = [m[1] for m in metrics_data]\n",
    "# Print in transposed form\n",
    "print(\"|        | \" + \" | \".join(labels) + \" |\")\n",
    "print(\"|--------|\" + \"|\".join([\"-----------\" for _ in labels]) + \"|\")\n",
    "print(\"| Value  | \" + \" | \".join(values) + \" |\")\n",
    "print()\n",
    "\n",
    "\n",
    "metrics_data=[    \n",
    "    (\"Lap\", f\"{Lap0_logs[\"laplace without replacement\"].item():.3f}\"),\n",
    "    (\"Lap0\", f\"{Lap0_val.item():.3f}\"),\n",
    "    (\"LapAIC\", f\"{LapAIC_val.item():.3f}\"),\n",
    "    (\"LapBIC\", f\"{LapBIC_val.item():.3f}\"),\n",
    "    ]\n",
    "\n",
    "\n",
    "labels = [m[0] for m in metrics_data]\n",
    "values = [m[1] for m in metrics_data]\n",
    "# Print in transposed form\n",
    "print(\"|        | \" + \" | \".join(labels) + \" |\")\n",
    "print(\"|--------|\" + \"|\".join([\"-----------\" for _ in labels]) + \"|\")\n",
    "print(\"| Value  | \" + \" | \".join(values) + \" |\")\n",
    "print()\n",
    "metrics_data = [    \n",
    "    #(\"Nested (1)\", f\"{Nested_val_e1:.3f}\"),\n",
    "    #(\"Nested (3)\", f\"{Nested_val_e3:.3f}\"),\n",
    "    (\"Nested (4)\", f\"{Nested_val_e4:.3f}\"),\n",
    "    (\"Nested (5)\", f\"{Nested_val_e5:.3f}\")\n",
    "    ]\n",
    "# Transpose the table: one row for the label, another for the value\n",
    "labels = [m[0] for m in metrics_data]\n",
    "values = [m[1] for m in metrics_data]\n",
    "\n",
    "# Print in transposed form\n",
    "print(\"|        | \" + \" | \".join(labels) + \" |\")\n",
    "print(\"|--------|\" + \"|\".join([\"-----------\" for _ in labels]) + \"|\")\n",
    "print(\"| Value  | \" + \" | \".join(values) + \" |\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fixed_reinit(model_MAP, torch.tensor(best_hyperparameters))\n",
    "mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood_MAP, model_MAP)\n",
    "map_loss = -mll(model_MAP(train_x), train_y)-log_normalized_prior(model_MAP)\n",
    "unscaled_MAP = -map_loss*len(*model.train_inputs)\n",
    "num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "Nested_Lap0_val, Nested_Lap0_logs = Laplace(model_MAP, unscaled_MAP, param_punish_term=0)\n",
    "print(f\"Lap0/nested = {Lap0_val/Nested_val_e5:.3f}\")\n",
    "print(f\"AIC (s)/nested = {AIC_val*(-0.5)/Nested_val_e5:.3f}\") \n",
    "print(f\"nested Lap0/nested = {Nested_Lap0_val/Nested_val_e5:.3f}\\n\")\n",
    "print(\"```python\")\n",
    "print(Nested_Lap0_val)\n",
    "\n",
    "print(Nested_Lap0_logs)\n",
    "print(\"```\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2D data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "possible_kenels = [\n",
    "        \"[RBF; RBF]\",\n",
    "        \"[RBF; LIN]\",\n",
    "        \"[LIN; RBF]\",\n",
    "    ]\n",
    "\n",
    "possible_datasets = {\n",
    "    \"periodic_1D\" : periodic_1D,\n",
    "    \"periodic_2D\" : periodic_2D,\n",
    "    \"parabola_1D\" : parabola_1D,\n",
    "    \"product\" : product,\n",
    "    \"periodic_sum\": periodic_sum,\n",
    "    \"periodic_sincos\": periodic_sincos,\n",
    "    \"linear_1D\": linear_1D,\n",
    "    \"linear_2D\": linear_2D,\n",
    "}\n",
    "\n",
    "possible_X_patterns = [\n",
    "    \"away\",\n",
    "    \"bullseye\",\n",
    "    \"circle\",\n",
    "    \"dots\",\n",
    "    \"h_lines\",\n",
    "    \"high lines\",\n",
    "    \"slant down\",\n",
    "    \"slant up\",\n",
    "    \"star\",\n",
    "    \"v_lines\",\n",
    "    \"wide lines\",\n",
    "    \"x_shape\",\n",
    "]\n",
    "\n",
    "kernel_name = possible_kenels[0]\n",
    "dataset_name = \"linear_2D\"\n",
    "pattern = \"away\"\n",
    "\n",
    "dataset_addendum = \"\"#\"51 points\"\n",
    "data_normalization = True\n",
    "data_norm_y = True \n",
    "levels = [1e+4, 1e+5]\n",
    "uninformed = True\n",
    "\n",
    "log_path = f\"logs/2D/{'x-normalized' if data_normalization else ''}_{dataset_name}_{dataset_addendum}/{kernel_name}\"\n",
    "if not os.path.exists(log_path):\n",
    "    os.makedirs(log_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load the dataset\n",
    "if pattern in datasets_2d:\n",
    "    df = pandas.read_csv(datasets_2d[pattern].path, header=None)\n",
    "    x1 = torch.tensor(df[0], dtype=torch.float32)\n",
    "    x2 = torch.tensor(df[1], dtype=torch.float32)\n",
    "    train_x = torch.stack([x1, x2], dim=-1)\n",
    "\n",
    "# Generate the z axis\n",
    "train_y = possible_datasets[dataset_name](train_x)\n",
    "\n",
    "# Z score normalization\n",
    "if data_normalization:\n",
    "    train_x = (train_x - train_x.mean()) / train_x.std()\n",
    "    if data_norm_y:\n",
    "        train_y = (train_y - train_y.mean()) / train_y.std()\n",
    "\n",
    "fig, ax = plot_3d_data(train_y, train_x[:,0], train_x[:,1], title_add=f\"{dataset_name}-{pattern}\", return_figure=True)\n",
    "fig.savefig(f\"{log_path}/data.png\", bbox_inches='tight')\n",
    "#fig.savefig(f\"{log_path}/data.pgf\", bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define the GP model\n",
    "likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "model = ExactMIGPModel(train_x, train_y, likelihood, kernel_text=kernel_name)\n",
    "likelihood_MAP = gpytorch.likelihoods.GaussianLikelihood()\n",
    "model_MAP = ExactMIGPModel(train_x, train_y, likelihood_MAP, kernel_text=kernel_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "mll(model(train_x), train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_names = [l[0] for l in list(model.named_parameters())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Train the GPs\n",
    "model.train()\n",
    "likelihood.train()\n",
    "mll_loss, model, likelihood, mll_train_log = optimize_hyperparameters(model, likelihood, X=train_x, Y=train_y, MAP=False, uninformed=uninformed)\n",
    "mll_opt_params = [p.item() for p in model.parameters() if p.requires_grad]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the optimization path of the trainings, each in an individual plot, given the parameter of interest\n",
    "# The parameter progression is plotted as arrows pointing to the next parameter position\n",
    "# The arrows are colored according to the lateness of their progression\n",
    "# The colorbar shows the progression of the parameter\n",
    "\n",
    "\n",
    "random_restarts = 2\n",
    "\n",
    "f, axs = plt.subplots(1, (random_restarts), figsize=(4*(random_restarts), 4), sharey=True, layout=\"constrained\")\n",
    "if len(levels) == 1:\n",
    "    axs = [axs]\n",
    "\n",
    "xdim = 0\n",
    "ydim = 1\n",
    "# extract all the parameters\n",
    "\n",
    "parameter_paths = [[log[0] for log in mll_train_log[i]] for i in range(len(mll_train_log))]\n",
    "parameter_values = [[log[1] for log in mll_train_log[i]] for i in range(len(mll_train_log))]\n",
    "\n",
    "for i, ax in enumerate(axs):\n",
    "    # train log consists of a list of tuples, where the first element is the parameter vector and the second element is the loss\n",
    "    plot_parameter_progression(parameter_paths[i], losses=parameter_values[i], xlabel=param_names[xdim], ylabel=param_names[ydim], xdim=xdim, ydim=ydim, fig=f, ax=ax, display_figure=False, title_add=f\"restart {i}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_MAP.train()\n",
    "likelihood_MAP.train()\n",
    "map_loss, model_MAP, likelihood_MAP, map_train_log = optimize_hyperparameters(model_MAP, likelihood_MAP, X=train_x, Y=train_y, MAP=True, uninformed=uninformed)\n",
    "map_opt_params = [p.item() for p in model_MAP.parameters() if p.requires_grad]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "f, axs = plt.subplots(1, (random_restarts), figsize=(4*(random_restarts), 4), sharey=True, layout=\"constrained\")\n",
    "if len(levels) == 1:\n",
    "    axs = [axs]\n",
    "\n",
    "# extract all the parameters\n",
    "parameter_paths = [[log[0] for log in map_train_log[i]] for i in range(len(map_train_log))]\n",
    "parameter_values = [[log[1] for log in map_train_log[i]] for i in range(len(map_train_log))]\n",
    "\n",
    "xdim = 0\n",
    "ydim = 1\n",
    "\n",
    "for i, ax in enumerate(axs):\n",
    "    plot_parameter_progression(parameter_paths[i], losses=parameter_values[i], xlabel=param_names[xdim], ylabel=param_names[ydim], xdim=xdim, ydim=ydim, fig=f, ax=ax, display_figure=False, title_add=f\"restart {i}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "likelihood.eval()\n",
    "model_MAP.eval()\n",
    "likelihood_MAP.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "likelihood.eval()\n",
    "model_MAP.eval()\n",
    "likelihood_MAP.eval()\n",
    "fig, axs = plt.subplots(1, 2, figsize=(16, 6), sharey=True)\n",
    "ax = axs[0]\n",
    "ax_MAP = axs[1]\n",
    "plot_model(model, likelihood, train_x, train_y, return_figure=False, figure=fig, ax=ax, loss_val=mll_loss.item(), loss_type=\"mll\", display_figure=False)\n",
    "plot_model(model_MAP, likelihood_MAP, train_x, train_y, return_figure=False, figure=fig, ax=ax_MAP, loss_val=map_loss.item(), loss_type=\"map\")\n",
    "fig.savefig(f\"{log_path}/posterior.png\", bbox_inches='tight')\n",
    "#fig.savefig(f\"{log_path}/posterior.pgf\", bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()\n",
    "likelihood.train()\n",
    "model_MAP.train()\n",
    "likelihood_MAP.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unscaled_MLL = -mll_loss*len(*model.train_inputs)\n",
    "unscaled_MAP = -map_loss*len(*model.train_inputs)\n",
    "num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "## Calculate the metrics\n",
    "AIC_val, AIC_logs = AIC(unscaled_MLL, num_params)\n",
    "BIC_val, BIC_logs = BIC(unscaled_MLL, num_params, torch.tensor(len(train_x)))\n",
    "AIC_MAP_val, AIC_MAP_logs = AIC(unscaled_MAP, num_params)\n",
    "BIC_MAP_val, BIC_MAP_logs = BIC(unscaled_MAP, num_params, torch.tensor(len(train_x)))\n",
    "Lap0_val, Lap0_logs =     Laplace(model_MAP, unscaled_MAP.clone(), uninformed=uninformed, param_punish_term=0)\n",
    "LapAIC_val, LapAIC_logs = Laplace(model_MAP, unscaled_MAP.clone(), uninformed=uninformed, param_punish_term=-1)\n",
    "LapBIC_val, LapBIC_logs = Laplace(model_MAP, unscaled_MAP.clone(), uninformed=uninformed, param_punish_term=\"BIC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Lap0_logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LapAIC_logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LapBIC_logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_evidences = list()\n",
    "model_evidence_logs = list()\n",
    "uninformed = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxcall = 1e+1\n",
    "if maxcall in levels:\n",
    "    print(maxcall)\n",
    "    Nested_val_e1, Nested_logs_e1 = Nested(model, store_full=True, pickle_directory=os.path.join(log_path, \"Nested_log\"), maxcall=maxcall)\n",
    "    model_evidences.append(Nested_val_e1)\n",
    "    model_evidence_logs.append(Nested_logs_e1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxcall = 1e+2\n",
    "if maxcall in levels:\n",
    "    print(maxcall)\n",
    "    Nested_val_e2, Nested_logs_e2 = Nested(model, store_full=True, pickle_directory=os.path.join(log_path, \"Nested_log\"), maxcall=maxcall)\n",
    "    model_evidences.append(Nested_val_e2)\n",
    "    model_evidence_logs.append(Nested_logs_e2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxcall = 1e+3\n",
    "if maxcall in levels:\n",
    "    print(maxcall)\n",
    "    Nested_val_e3, Nested_logs_e3 = Nested(model, store_full=True, pickle_directory=os.path.join(log_path, \"Nested_log\"), maxcall=maxcall, uninformed=uninformed)\n",
    "    model_evidences.append(Nested_val_e3)\n",
    "    model_evidence_logs.append(Nested_logs_e3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxcall = 1e+4\n",
    "if maxcall in levels:\n",
    "    print(maxcall)\n",
    "    Nested_val_e4, Nested_logs_e4 = Nested(model, store_full=True, pickle_directory=os.path.join(log_path, \"Nested_log\"), maxcall=maxcall, uninformed=uninformed)\n",
    "    model_evidences.append(Nested_val_e4)\n",
    "    model_evidence_logs.append(Nested_logs_e4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxcall = 1e+5\n",
    "if maxcall in levels:\n",
    "    print(maxcall)\n",
    "    Nested_val_e5, Nested_logs_e5 = Nested(model, store_full=True, pickle_directory=os.path.join(log_path, \"Nested_log\"), maxcall=maxcall, uninformed=uninformed)\n",
    "    model_evidences.append(Nested_val_e5)\n",
    "    model_evidence_logs.append(Nested_logs_e5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for mod_log in model_evidence_logs:\n",
    "    with open(f\"{mod_log['res file']}\", \"rb\") as f:\n",
    "        res = dill.load(f)\n",
    "    print(f\"Num calls: {sum(res.ncall)}\")\n",
    "    print(f\"logz Error: {res[\"logzerr\"][-1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plot_mll_opt = True\n",
    "std_filter = -1e-3\n",
    "filter_type = \"max\" # (\"mean\"), \"max\", \"none\"\n",
    "xdim = 0\n",
    "ydim = 1\n",
    "\n",
    "param_names = [l[0] for l in list(model.named_parameters())]\n",
    "\n",
    "f, axs = plt.subplots(1, len(levels), figsize=(4*len(levels), 4), sharey=True, layout=\"constrained\")\n",
    "if len(levels) == 1:\n",
    "    axs = [axs]\n",
    "for ax, level, model_evidence_log in zip(axs, levels, model_evidence_logs):\n",
    "    if ax == axs[-1]:\n",
    "        nested_sampling_plot(model, model_evidence_log, xdim = xdim, ydim = ydim, filter_type=filter_type, std_filter=std_filter, return_figure=False, title_add=\"\", fig=f, ax=ax, display_figure=True, plot_mll_opt=plot_mll_opt, mll_opt_params=mll_opt_params, plot_lap=True, Lap0_logs=Lap0_logs, LapAIC_logs=LapAIC_logs, LapBIC_logs=LapBIC_logs, lap_colors = [\"r\", \"pink\", \"black\"])\n",
    "    else:\n",
    "        nested_sampling_plot(model, model_evidence_log, xdim = xdim, ydim = ydim, filter_type=filter_type, std_filter=std_filter, return_figure=False, title_add=\"\", fig=f, ax=ax, display_figure=False, plot_mll_opt=plot_mll_opt, mll_opt_params=mll_opt_params, plot_lap=True, Lap0_logs=Lap0_logs, LapAIC_logs=LapAIC_logs, LapBIC_logs=LapBIC_logs, lap_colors = [\"r\", \"pink\", \"black\"])\n",
    "    if ax == axs[0]:\n",
    "        ax.set_ylabel(param_names[ydim])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Make a \"loop\" out of the Nested levels to compare them to each other\n",
    "f, axs = plt.subplots(1, len(levels), figsize=(4*len(levels), 4), sharey=True, layout=\"constrained\")\n",
    "if len(levels) == 1:\n",
    "    axs = [axs]\n",
    "for ax, level, model_evidence_log in zip(axs, levels, model_evidence_logs):\n",
    "    with open(f\"{model_evidence_log['res file']}\", \"rb\") as f:\n",
    "        res = dill.load(f)\n",
    "    \n",
    "    # Plot the actual figure\n",
    "    param_names = [l[0] for l in list(model.named_parameters())]\n",
    "    xdim = 0\n",
    "    ydim = 1\n",
    "\n",
    "    # Find the best value and the corresponding hyperparameters\n",
    "    best_idx = np.argmax(res.logl)\n",
    "    best_hyperparameters = res.samples[best_idx]\n",
    "\n",
    "    # Do an outlier cleanup on res.logz\n",
    "    logz_mean = np.mean(res.logz) # model_evidence_log[\"parameter statistics\"][\"mu\"] #\n",
    "    logz_std = np.std(res.logz)   # model_evidence_log[\"parameter statistics\"][\"std\"]#\n",
    "    std_filter = -1e-4\n",
    "    filter_type = \"max\" # \"mean\", \"max\", \"none\"\n",
    "    if filter_type == \"max\":\n",
    "        mask = res.logz >= max(res.logz)+std_filter*logz_std\n",
    "    elif filter_type == \"mean\":\n",
    "        mask = res.logz >= logz_mean+std_filter*logz_std\n",
    "    elif filter_type == \"none\":\n",
    "        mask = res.logz == res.logz\n",
    "\n",
    "\n",
    "\n",
    "    likelihood_surface_scatter = ax.scatter(res.samples[:,xdim][mask], res.samples[:,ydim][mask], c=res.logz[mask], s=3)\n",
    "    # Best found hyperparameters\n",
    "    ax.scatter(best_hyperparameters[xdim], best_hyperparameters[ydim], c=\"r\", s=10)\n",
    "\n",
    "    plot_mll_opt = False\n",
    "    if plot_mll_opt:\n",
    "        ax.scatter(mll_opt_params[xdim], mll_opt_params[ydim], c=\"black\", s=10)\n",
    "        # Add a small text beside the point saying \"MLL\"\n",
    "        ax.text(mll_opt_params[xdim], mll_opt_params[ydim], \"MLL\", fontsize=12, color=\"black\", verticalalignment='center', horizontalalignment='right')\n",
    "    #ax.scatter(map_opt_params[xdim], map_opt_params[ydim], c=\"b\", s=10)\n",
    "    ## Add a small text beside the point saying \"MAP\"\n",
    "    #ax.text(map_opt_params[xdim], map_opt_params[ydim], \"MAP\", fontsize=12, color=\"b\", verticalalignment='center', horizontalalignment='right')\n",
    "    \n",
    "    coverages = list()\n",
    "    # Plot the Laplace levels\n",
    "    for lap_log, lap_color in zip([Lap0_logs, LapAIC_logs, LapBIC_logs], [\"r\", \"pink\", \"black\"]):\n",
    "        lap_param_mu = lap_log[\"parameter values\"]\n",
    "        # Wait a minute, isn't the Hessian the inverse of the covariance matrix? Yes, see Murphy PML 1 eq. (7.228)\n",
    "        lap_param_cov_matr = torch.linalg.inv(lap_log[\"corrected Hessian\"])\n",
    "        # Calculate the amount of samples that are covered by the 1 sigma and 2 sigma interval based on the lap_mu and lap_cov values\n",
    "        lap_2_sig_coverage = percentage_inside_ellipse(lap_param_mu.flatten().numpy(), lap_param_cov_matr.numpy(), res.samples[mask])\n",
    "        coverages.append(lap_2_sig_coverage)\n",
    "        #ax.scatter(lap_param_mu[xdim], lap_param_mu[ydim], c=\"b\", s=10)\n",
    "\n",
    "        # Plot the std points\n",
    "        lap_mu_filtered = lap_param_mu.numpy()[[xdim, ydim]] \n",
    "        lap_cov_filtered = lap_param_cov_matr.numpy()[[xdim, ydim]][:,[xdim, ydim]]\n",
    "        #lap_var_ellipse_x, lap_var_ellipse_y = get_std_points(lap_mu_filtered.flatten(), lap_cov_filtered)\n",
    "        #plt.scatter(lap_var_ellipse_x, lap_var_ellipse_y, c=\"b\", s=1)\n",
    "        confidence_ellipse(lap_mu_filtered, lap_cov_filtered, ax, n_std=2, edgecolor=lap_color, lw=1)\n",
    "\n",
    "    ax.set_title(f\"#Samples: {sum(res.ncall)}; {coverages[0]*100:.0f}% inside 2 sigma\")\n",
    "    if ax == axs[0]:\n",
    "        ax.set_ylabel(param_names[ydim])\n",
    "    ax.set_xlabel(param_names[xdim])\n",
    "\n",
    "    plt.colorbar(likelihood_surface_scatter)\n",
    "\n",
    "# This doesn't do anything, somehow\n",
    "fig.suptitle(f\"{dataset_name} - {kernel_name}\")\n",
    "fig.savefig(f\"{log_path}/Nested_{filter_type}{std_filter}.png\", bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the best value and the corresponding hyperparameters\n",
    "best_idx = np.argmax(res.logl)\n",
    "best_hyperparameters = res.samples[best_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimal GP according to Nested sampling\n",
    "fixed_reinit(model, torch.tensor(best_hyperparameters))\n",
    "model.eval()\n",
    "likelihood.eval()\n",
    "# mll of best parameterization according to Nested sampling:\n",
    "mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "nested_top_mll = (mll(model(train_x), train_y))*len(train_x)\n",
    "nested_top_map = nested_top_mll+log_normalized_prior(model)*len(train_x)\n",
    "fig, ax = plot_model(model, likelihood, train_x, train_y, return_figure=True, loss_val=res.logl[best_idx], loss_type=\"Nested logl\")\n",
    "ax.title.set_text(f\"{ax.title.get_text()} ; MLL: {nested_top_mll.item():.2f}; MAP: {nested_top_map.item():.2f}\\n MLL (u) {-nested_top_mll/len(train_x):.2f}; MAP (u) {-nested_top_map/len(train_x):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect metric names and values into a list of tuples\n",
    "metrics_data = [\n",
    "    (\"MLL\", f\"{unscaled_MLL.item():.3f}\"),\n",
    "    (\"MLL (loss)\", f\"{mll_loss.item():.3f}\"),\n",
    "    (\"AIC\", f\"{AIC_val.item():.3f}\"),\n",
    "    (\"AIC (s)\", f\"{(AIC_val.item()*(-0.5)):.3f}\"),\n",
    "    (\"BIC\", f\"{BIC_val.item():.3f}\"),\n",
    "    (\"BIC (s)\", f\"{(BIC_val.item()*(-0.5)):.3f}\"),\n",
    "    \n",
    "\n",
    "]\n",
    "labels = [m[0] for m in metrics_data]\n",
    "values = [m[1] for m in metrics_data]\n",
    "# Print in transposed form\n",
    "print(\"|        | \" + \" | \".join(labels) + \" |\")\n",
    "print(\"|--------|\" + \"|\".join([\"-----------\" for _ in labels]) + \"|\")\n",
    "print(\"| Value  | \" + \" | \".join(values) + \" |\")\n",
    "print()\n",
    "\n",
    "metrics_data = [(\"MAP\", f\"{unscaled_MAP.item():.3f}\"),\n",
    "    (\"MAP (loss)\", f\"{map_loss.item():.3f}\"),\n",
    "    (\"AIC_M\", f\"{AIC_MAP_val.item():.3f}\"),\n",
    "    (\"AIC_M (s)\", f\"{(AIC_MAP_val.item()*(-0.5)):.3f}\"),\n",
    "    (\"BIC_M\", f\"{BIC_MAP_val.item():.3f}\"),\n",
    "    (\"BIC_M (s)\", f\"{(BIC_MAP_val.item()*(-0.5)):.3f}\"),\n",
    "]\n",
    "\n",
    "labels = [m[0] for m in metrics_data]\n",
    "values = [m[1] for m in metrics_data]\n",
    "# Print in transposed form\n",
    "print(\"|        | \" + \" | \".join(labels) + \" |\")\n",
    "print(\"|--------|\" + \"|\".join([\"-----------\" for _ in labels]) + \"|\")\n",
    "print(\"| Value  | \" + \" | \".join(values) + \" |\")\n",
    "print()\n",
    "\n",
    "\n",
    "metrics_data=[    \n",
    "    (\"Lap\", f\"{Lap0_logs[\"laplace without replacement\"].item():.3f}\"),\n",
    "    (\"Lap0\", f\"{Lap0_val.item():.3f}\"),\n",
    "    (\"LapAIC\", f\"{LapAIC_val.item():.3f}\"),\n",
    "    (\"LapBIC\", f\"{LapBIC_val.item():.3f}\"),\n",
    "    ]\n",
    "\n",
    "\n",
    "labels = [m[0] for m in metrics_data]\n",
    "values = [m[1] for m in metrics_data]\n",
    "# Print in transposed form\n",
    "print(\"|        | \" + \" | \".join(labels) + \" |\")\n",
    "print(\"|--------|\" + \"|\".join([\"-----------\" for _ in labels]) + \"|\")\n",
    "print(\"| Value  | \" + \" | \".join(values) + \" |\")\n",
    "print()\n",
    "metrics_data = [    \n",
    "    #(\"Nested (1)\", f\"{Nested_val_e1:.3f}\"),\n",
    "    #(\"Nested (3)\", f\"{Nested_val_e3:.3f}\"),\n",
    "    (\"Nested (4)\", f\"{Nested_val_e4:.3f}\"),\n",
    "    (\"Nested (5)\", f\"{Nested_val_e5:.3f}\")\n",
    "    ]\n",
    "# Transpose the table: one row for the label, another for the value\n",
    "labels = [m[0] for m in metrics_data]\n",
    "values = [m[1] for m in metrics_data]\n",
    "\n",
    "# Print in transposed form\n",
    "print(\"|        | \" + \" | \".join(labels) + \" |\")\n",
    "print(\"|--------|\" + \"|\".join([\"-----------\" for _ in labels]) + \"|\")\n",
    "print(\"| Value  | \" + \" | \".join(values) + \" |\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fixed_reinit(model_MAP, torch.tensor(best_hyperparameters))\n",
    "mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood_MAP, model_MAP)\n",
    "map_loss = -mll(model_MAP(train_x), train_y)-log_normalized_prior(model_MAP)\n",
    "unscaled_MAP = -map_loss*len(*model.train_inputs)\n",
    "num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "Nested_Lap0_val, Nested_Lap0_logs = Laplace(model_MAP, unscaled_MAP, param_punish_term=0)\n",
    "print(f\"Lap0/nested = {Lap0_val/Nested_val_e5:.3f}\")\n",
    "print(f\"AIC (s)/nested = {AIC_val*(-0.5)/Nested_val_e5:.3f}\") \n",
    "print(f\"nested Lap0/nested = {Nested_Lap0_val/Nested_val_e5:.3f}\\n\")\n",
    "print(\"```python\")\n",
    "print(Nested_Lap0_val)\n",
    "\n",
    "print(Nested_Lap0_logs)\n",
    "print(\"```\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main loop"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "for dataset in datasets_1d:\n",
    "    # Load the dataset\n",
    "    df = pandas.read_csv(datasets_1d[dataset].path, header=None)\n",
    "    train_x = torch.tensor(df[0])\n",
    "    train_y = torch.tensor(df[1])\n",
    "    plot_data(train_x, train_y)\n",
    "    # Define the GP model\n",
    "    likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "    model = ExactGPModel(train_x, train_y, likelihood, kernel_text=kernel_name)\n",
    "    likelihood_MAP = gpytorch.likelihoods.GaussianLikelihood()\n",
    "    model_MAP = ExactGPModel(train_x, train_y, likelihood_MAP, kernel_text=kernel_name)\n",
    "    # Train the GPs\n",
    "    mll_loss, ... = train_model(model, likelihood, train_x, train_y)\n",
    "    map_loss, ... = train_model(model_MAP, likelihood_MAP, train_x, train_y, MAP=True)\n",
    "    # Plot the posteriors\n",
    "    plot_posterior(model, likelihood, train_x, train_y)\n",
    "    plot_posterior(model_MAP, likelihood_MAP, train_x, train_y)\n",
    "    # Calculate the metrics\n",
    "    AIC()\n",
    "    BIC()\n",
    "    Laplace()\n",
    "    Nested()\n",
    "    "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Collect metric names and values into a list of tuples\n",
    "metrics_data = [\n",
    "    (\"MLL\", f\"{unscaled_MLL.item():.3f}\"),\n",
    "    (\"MLL (unsc)\", f\"{mll_loss.item():.3f}\"),\n",
    "    (\"AIC\", f\"{AIC_val.item():.3f}\"),\n",
    "    (\"AIC (s)\", f\"{(AIC_val.item()*(-0.5)):.3f}\"),\n",
    "    (\"BIC\", f\"{BIC_val.item():.3f}\"),\n",
    "    (\"BIC (s)\", f\"{(BIC_val.item()*(-0.5)):.3f}\"),\n",
    "    (\"MAP\", f\"{unscaled_MAP.item():.3f}\"),\n",
    "    (\"MAP (unsc)\", f\"{map_loss.item():.3f}\"),\n",
    "    (\"AIC_MAP\", f\"{AIC_MAP_val.item():.3f}\"),\n",
    "    (\"AIC_MAP (s)\", f\"{(AIC_MAP_val.item()*(-0.5)):.3f}\"),\n",
    "    (\"BIC_MAP\", f\"{BIC_MAP_val.item():.3f}\"),\n",
    "    (\"BIC_MAP (s)\", f\"{(BIC_MAP_val.item()*(-0.5)):.3f}\"),\n",
    "    (\"Lap0\", f\"{Lap0_val.item():.3f}\"),\n",
    "    (\"LapAIC\", f\"{LapAIC_val.item():.3f}\"),\n",
    "    (\"LapBIC\", f\"{LapBIC_val.item():.3f}\"),\n",
    "    (\"Nested (1)\", f\"{Nested_val_e1:.3f}\"),\n",
    "    (\"Nested (3)\", f\"{Nested_val_e3:.3f}\"),\n",
    "    (\"Nested (4)\", f\"{Nested_val_e4:.3f}\"),\n",
    "    (\"Nested (5)\", f\"{Nested_val_e5:.3f}\")\n",
    "]\n",
    "\n",
    "# Transpose the table: one row for the label, another for the value\n",
    "labels = [m[0] for m in metrics_data]\n",
    "values = [m[1] for m in metrics_data]\n",
    "\n",
    "# Print in transposed form\n",
    "print(\"========================================================\")\n",
    "print(\"|        | \" + \" | \".join(labels) + \" |\")\n",
    "print(\"|--------|\" + \"|\".join([\"-----------\" for _ in labels]) + \"|\")\n",
    "print(\"| Value  | \" + \" | \".join(values) + \" |\")\n",
    "print(\"========================================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The big figure with rows for each dataset and columns for [Dataset, Posterior (MLL), Posterior (MAP), Nested plot]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XYZ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sage3_129",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
