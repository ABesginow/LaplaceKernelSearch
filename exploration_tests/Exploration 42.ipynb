{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(1, \"..\")\n",
    "from collections import namedtuple\n",
    "import dill\n",
    "from helpFunctions import get_string_representation_of_kernel as gsr\n",
    "from helpFunctions import get_full_kernels_in_kernel_expression\n",
    "from globalParams import options\n",
    "import gpytorch\n",
    "import itertools\n",
    "import matplotlib.colors as mcolors\n",
    "from matplotlib.patches import Ellipse\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.transforms as transforms\n",
    "from metrics import calculate_AIC as AIC, calculate_BIC as BIC, calculate_laplace as Laplace, NestedSampling as Nested, log_normalized_prior, prior_distribution\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from pygranso.pygranso import pygranso\n",
    "from pygranso.pygransoStruct import pygransoStruct\n",
    "from pygranso.private.getNvar import getNvarTorch\n",
    "import re\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def periodic_1D(X):\n",
    "    \"\"\"\n",
    "    $\\sin(x_0)$\n",
    "    \"\"\"\n",
    "    return torch.sin(X[:,0])\n",
    "\n",
    "def periodic_2D(X):\n",
    "    \"\"\"\n",
    "    $\\sin(x_0) \\cdot \\sin(x_1)$\n",
    "    \"\"\"\n",
    "    return torch.sin(X[:,0]) * torch.sin(X[:,1])\n",
    "\n",
    "def parabola_1D(X):\n",
    "    \"\"\"\n",
    "    $x_0^2$\n",
    "    \"\"\"\n",
    "    return X[:,0]**2\n",
    "\n",
    "def parabola_2D(X):\n",
    "    \"\"\"\n",
    "    $x_0^2 \\cdot x_1^2$\n",
    "    \"\"\"\n",
    "    return X[:,0]**2 + X[:,1]**2\n",
    "\n",
    "def product(X):\n",
    "    \"\"\"\n",
    "    $x_0 \\cdot x_1$\n",
    "    \"\"\"\n",
    "    return X[:,0] * X[:,1]\n",
    "\n",
    "def periodic_sum(X):\n",
    "    \"\"\"\n",
    "    $\\sin(x_0 + x_1)$\n",
    "    \"\"\"\n",
    "    return torch.sin(X[:,0] + X[:,1])\n",
    "\n",
    "def periodic_sincos(X):\n",
    "    \"\"\"\n",
    "    $\\sin(x_0) \\cdot \\cos(x_1)$\n",
    "    \"\"\"\n",
    "    return torch.sin(X[:,0]) * torch.cos(X[:,1])\n",
    "\n",
    "def linear_1D(X):\n",
    "    \"\"\"\n",
    "    $x_0$\n",
    "    \"\"\"\n",
    "    return X[:,0]\n",
    "\n",
    "def linear_2D(X):\n",
    "    \"\"\"\n",
    "    $x_0 + x_1$\n",
    "    \"\"\"\n",
    "    return X[:,0]+X[:,1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameter_limits = {\"RBFKernel\": {\"lengthscale\": [1e-3,5]},\n",
    "                         \"MaternKernel\": {\"lengthscale\": [1e-3,1]},\n",
    "                         \"LinearKernel\": {\"variance\": [1e-4,1]},\n",
    "                         \"AffineKernel\": {\"variance\": [1e-4,1]},\n",
    "                         \"RQKernel\": {\"lengthscale\": [1e-3,1],\n",
    "                                      \"alpha\": [1e-3,1]},\n",
    "                         \"CosineKernel\": {\"period_length\": [1e-3,3]},\n",
    "                         \"PeriodicKernel\": {\"lengthscale\": [1e-3,5],\n",
    "                                            \"period_length\": [1e-3,10]},\n",
    "                         \"ScaleKernel\": {\"outputscale\": [1e-3,10]},\n",
    "                         \"Noise\": [1e-3, 1],\n",
    "                         \"MyPeriodKernel\":{\"period_length\": [1e-3,3]}}\n",
    "\n",
    "def random_reinit(model, logarithmic=False):\n",
    "    #print(\"Random reparameterization\")\n",
    "    #print(\"old parameters: \", list(model.named_parameters()))\n",
    "    model_params = model.parameters()\n",
    "    relevant_hyper_limits = list()\n",
    "    relevant_hyper_limits.append(hyperparameter_limits[\"Noise\"])\n",
    "    kernel_name_list = [kernel for kernel in get_full_kernels_in_kernel_expression(model.covar_module)]\n",
    "    for kernel in kernel_name_list:\n",
    "        for param_name in hyperparameter_limits[kernel]:\n",
    "            relevant_hyper_limits.append(hyperparameter_limits[kernel][param_name])\n",
    "\n",
    "    for i, (param, limit) in enumerate(zip(model_params, relevant_hyper_limits)):\n",
    "        param_name = limit\n",
    "        if logarithmic:\n",
    "            new_param_value = torch.rand_like(param)* (torch.log(torch.tensor(limit[1])) - torch.log(torch.tensor(limit[0]))) + torch.log(torch.tensor(limit[0]))\n",
    "        else:\n",
    "            new_param_value = torch.rand_like(param)* (limit[1] - limit[0]) + limit[0]\n",
    "        param.data = new_param_value\n",
    "    #print(\"new parameters: \", list(model.named_parameters()))\n",
    "\n",
    "\n",
    "def fixed_reinit(model, parameters: torch.tensor) -> None:\n",
    "    for i, (param, value) in enumerate(zip(model.parameters(), parameters)):\n",
    "        param.data = torch.full_like(param.data, value)\n",
    "\n",
    "# Define the training loop\n",
    "def optimize_hyperparameters(model, likelihood, **kwargs):\n",
    "    \"\"\"\n",
    "    find optimal hyperparameters either by BO or by starting from random initial values multiple times, using an optimizer every time\n",
    "    and then returning the best result\n",
    "    \"\"\"\n",
    "\n",
    "    # I think this is very ugly to define the class inside the training function and then use a parameter from the function within the class scope. But we all need to make sacrifices...\n",
    "    # Original class taken from https://ncvx.org/examples/A1_rosenbrock.html\n",
    "    class HaltLog:\n",
    "        def __init__(self):\n",
    "            pass\n",
    "\n",
    "        def haltLog(self, iteration, x, penaltyfn_parts, d,get_BFGS_state_fn, H_regularized,\n",
    "                    ls_evals, alpha, n_gradients, stat_vec, stat_val, fallback_level):\n",
    "\n",
    "            # DON'T CHANGE THIS\n",
    "            # increment the index/count\n",
    "            self.index += 1\n",
    "\n",
    "            # EXAMPLE:\n",
    "            # store history of x iterates in a preallocated cell array\n",
    "            self.x_iterates[restart].append(x)\n",
    "            self.neg_loss[restart].append(penaltyfn_parts.f)\n",
    "            self.tv[restart].append(penaltyfn_parts.tv)\n",
    "            self.hessians[restart].append(get_BFGS_state_fn())\n",
    "\n",
    "            # keep this false unless you want to implement a custom termination\n",
    "            # condition\n",
    "            halt = False\n",
    "            return halt\n",
    "\n",
    "        # Once PyGRANSO has run, you may call this function to get retreive all\n",
    "        # the logging data stored in the shared variables, which is populated\n",
    "        # by haltLog being called on every iteration of PyGRANSO.\n",
    "        def getLog(self):\n",
    "            # EXAMPLE\n",
    "            # return x_iterates, trimmed to correct size\n",
    "            log = pygransoStruct()\n",
    "            log.x        = self.x_iterates\n",
    "            log.neg_loss = self.neg_loss\n",
    "            log.tv       = self.tv\n",
    "            log.hessians = self.hessians\n",
    "            #log = pygransoStruct()\n",
    "            #log.x   = self.x_iterates[0:self.index]\n",
    "            #log.f   = self.f[0:self.index]\n",
    "            #log.tv  = self.tv[0:self.index]\n",
    "            #log.hessians  = self.hessians[0:self.index]\n",
    "            return log\n",
    "\n",
    "        def makeHaltLogFunctions(self, restarts=1):\n",
    "            # don't change these lambda functions\n",
    "            halt_log_fn = lambda iteration, x, penaltyfn_parts, d,get_BFGS_state_fn, H_regularized, ls_evals, alpha, n_gradients, stat_vec, stat_val, fallback_level: self.haltLog(iteration, x, penaltyfn_parts, d,get_BFGS_state_fn, H_regularized, ls_evals, alpha, n_gradients, stat_vec, stat_val, fallback_level)\n",
    "\n",
    "            get_log_fn = lambda : self.getLog()\n",
    "\n",
    "            # Make your shared variables here to store PyGRANSO history data\n",
    "            # EXAMPLE - store history of iterates x_0,x_1,...,x_k\n",
    "\n",
    "            # restart the index and empty the log\n",
    "            self.index       = 0\n",
    "            self.x_iterates  = [list() for _ in range(restarts)]\n",
    "            self.neg_loss    = [list() for _ in range(restarts)]\n",
    "            self.tv          = [list() for _ in range(restarts)]\n",
    "            self.hessians    = [list() for _ in range(restarts)]\n",
    "\n",
    "            # Only modify the body of logIterate(), not its name or arguments.\n",
    "            # Store whatever data you wish from the current PyGRANSO iteration info,\n",
    "            # given by the input arguments, into shared variables of\n",
    "            # makeHaltLogFunctions, so that this data can be retrieved after PyGRANSO\n",
    "            # has been terminated.\n",
    "            #\n",
    "            # DESCRIPTION OF INPUT ARGUMENTS\n",
    "            #   iter                current iteration number\n",
    "            #   x                   current iterate x\n",
    "            #   penaltyfn_parts     struct containing the following\n",
    "            #       OBJECTIVE AND CONSTRAINTS VALUES\n",
    "            #       .f              objective value at x\n",
    "            #       .f_grad         objective gradient at x\n",
    "            #       .ci             inequality constraint at x\n",
    "            #       .ci_grad        inequality gradient at x\n",
    "            #       .ce             equality constraint at x\n",
    "            #       .ce_grad        equality gradient at x\n",
    "            #       TOTAL VIOLATION VALUES (inf norm, for determining feasibiliy)\n",
    "            #       .tvi            total violation of inequality constraints at x\n",
    "            #       .tve            total violation of equality constraints at x\n",
    "            #       .tv             total violation of all constraints at x\n",
    "            #       TOTAL VIOLATION VALUES (one norm, for L1 penalty function)\n",
    "            #       .tvi_l1         total violation of inequality constraints at x\n",
    "            #       .tvi_l1_grad    its gradient\n",
    "            #       .tve_l1         total violation of equality constraints at x\n",
    "            #       .tve_l1_grad    its gradient\n",
    "            #       .tv_l1          total violation of all constraints at x\n",
    "            #       .tv_l1_grad     its gradient\n",
    "            #       PENALTY FUNCTION VALUES\n",
    "            #       .p              penalty function value at x\n",
    "            #       .p_grad         penalty function gradient at x\n",
    "            #       .mu             current value of the penalty parameter\n",
    "            #       .feasible_to_tol logical indicating whether x is feasible\n",
    "            #   d                   search direction\n",
    "            #   get_BFGS_state_fn   function handle to get the (L)BFGS state data\n",
    "            #                       FULL MEMORY:\n",
    "            #                       - returns BFGS inverse Hessian approximation\n",
    "            #                       LIMITED MEMORY:\n",
    "            #                       - returns a struct with current L-BFGS state:\n",
    "            #                           .S          matrix of the BFGS s vectors\n",
    "            #                           .Y          matrix of the BFGS y vectors\n",
    "            #                           .rho        row vector of the 1/sty values\n",
    "            #                           .gamma      H0 scaling factor\n",
    "            #   H_regularized       regularized version of H\n",
    "            #                       [] if no regularization was applied to H\n",
    "            #   fn_evals            number of function evaluations incurred during\n",
    "            #                       this iteration\n",
    "            #   alpha               size of accepted size\n",
    "            #   n_gradients         number of previous gradients used for computing\n",
    "            #                       the termination QP\n",
    "            #   stat_vec            stationarity measure vector\n",
    "            #   stat_val            approximate value of stationarity:\n",
    "            #                           norm(stat_vec)\n",
    "            #                       gradients (result of termination QP)\n",
    "            #   fallback_level      number of strategy needed for a successful step\n",
    "            #                       to be taken.  See bfgssqpOptionsAdvanced.\n",
    "            #\n",
    "            # OUTPUT ARGUMENT\n",
    "            #   halt                set this to true if you wish optimization to\n",
    "            #                       be halted at the current iterate.  This can be\n",
    "            #                       used to create a custom termination condition,\n",
    "            return [halt_log_fn, get_log_fn]\n",
    "\n",
    "\n",
    "    random_restarts = kwargs.get(\"random_restarts\", options[\"training\"][\"restarts\"]+1)\n",
    "    uninformed = kwargs.get(\"uninformed\", False)\n",
    "    logarithmic_reinit = kwargs.get(\"logarithmic_reinit\", False)\n",
    "    train_log = [list() for _ in range(random_restarts)]\n",
    "\n",
    "    \"\"\"\n",
    "    # The call that comes from GRANSO\n",
    "    user_halt = halt_log_fn(0, x, self.penaltyfn_at_x, np.zeros((n,1)),\n",
    "                                        get_bfgs_state_fn, H_QP,\n",
    "                                        1, 0, 1, stat_vec, self.stat_val, 0          )\n",
    "    \"\"\"\n",
    "    def log_fnct(*args):\n",
    "        train_log[restart].append((args[1], args[2].neg_loss))\n",
    "        return False \n",
    "\n",
    "\n",
    "    train_x = kwargs.get(\"X\", model.train_inputs)\n",
    "    train_y = kwargs.get(\"Y\", model.train_targets)\n",
    "    MAP = kwargs.get(\"MAP\", True)\n",
    "    double_precision = kwargs.get(\"double_precision\", False)\n",
    "\n",
    "    # Set up the likelihood and model\n",
    "    #likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "    #model = GPModel(train_x, train_y, likelihood)\n",
    "\n",
    "    # Define the negative log likelihood\n",
    "    mll_fkt = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "\n",
    "    # Set up the PyGRANSO optimizer\n",
    "    opts = pygransoStruct()\n",
    "    opts.torch_device = torch.device('cpu')\n",
    "    nvar = getNvarTorch(model.parameters())\n",
    "    opts.x0 = torch.nn.utils.parameters_to_vector(model.parameters()).detach().reshape(nvar,1)\n",
    "    opts.opt_tol = float(1e-10)\n",
    "    #opts.limited_mem_size = int(100)\n",
    "    opts.limited_mem_size = 0\n",
    "    opts.globalAD = True\n",
    "    opts.double_precision = double_precision\n",
    "    opts.quadprog_info_msg = False\n",
    "    opts.print_level = int(0)\n",
    "    opts.halt_on_linesearch_bracket = False\n",
    "    mHLF_obj = HaltLog()\n",
    "    [halt_log_fn, get_log_fn] = mHLF_obj.makeHaltLogFunctions(restarts=random_restarts)\n",
    "\n",
    "    #  Set PyGRANSO's logging function in opts\n",
    "    opts.halt_log_fn = halt_log_fn\n",
    "\n",
    "    # Define the objective function\n",
    "    def objective_function(model):\n",
    "        output = model(train_x)\n",
    "        try:\n",
    "            # TODO PyGRANSO dying is a severe problem. as it literally exits the program instead of raising an error\n",
    "            # negative scaled MLL\n",
    "            loss = -mll_fkt(output, train_y)\n",
    "        except Exception as E:\n",
    "            print(\"LOG ERROR: Severe PyGRANSO issue. Loss is inf+0\")\n",
    "            loss = torch.tensor(np.inf, requires_grad=True) + torch.tensor(0)\n",
    "        if MAP:\n",
    "            # log_normalized_prior is in metrics.py \n",
    "            log_p = log_normalized_prior(model, uninformed=uninformed)\n",
    "            # negative scaled MAP\n",
    "            loss -= log_p\n",
    "        #print(f\"LOG: {loss}\")\n",
    "        return [loss, None, None]\n",
    "\n",
    "    best_model_state_dict = model.state_dict()\n",
    "    best_likelihood_state_dict = likelihood.state_dict()\n",
    "\n",
    "    best_f = np.inf\n",
    "    for restart in range(random_restarts):\n",
    "        print(\"---\")\n",
    "        print(\"start parameters: \", opts.x0)\n",
    "        # Train the model using PyGRANSO\n",
    "        try:\n",
    "            soln = pygranso(var_spec=model, combined_fn=objective_function, user_opts=opts)\n",
    "            print(f\"Restart {restart} : trained parameters: {list(model.named_parameters())}\")\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            import pdb\n",
    "            pdb.set_trace()\n",
    "            pass\n",
    "\n",
    "        if soln.final.f < best_f:\n",
    "            print(f\"LOG: Found new best solution: {soln.final.f}\")\n",
    "            best_f = soln.final.f\n",
    "            best_model_state_dict = model.state_dict()\n",
    "            best_likelihood_state_dict = likelihood.state_dict()\n",
    "        random_reinit(model, logarithmic=logarithmic_reinit)\n",
    "        opts.x0 = torch.nn.utils.parameters_to_vector(model.parameters()).detach().reshape(nvar,1)\n",
    "\n",
    "    model.load_state_dict(best_model_state_dict)\n",
    "    likelihood.load_state_dict(best_likelihood_state_dict)\n",
    "    print(f\"----\")\n",
    "    print(f\"Final best parameters: {list(model.named_parameters())} w. loss: {best_f} (smaller=better)\")\n",
    "    print(f\"----\")\n",
    "\n",
    "    loss = -mll_fkt(model(train_x), train_y)\n",
    "    #print(f\"LOG: Final MLL: {loss}\")\n",
    "    if MAP:\n",
    "        log_p = log_normalized_prior(model, uninformed=uninformed)\n",
    "        loss -= log_p\n",
    "        #print(f\"LOG: Final MAP: {loss}\")\n",
    "\n",
    "    #print(f\"post training (best): {list(model.named_parameters())} w. loss: {soln.best.f}\")\n",
    "    #print(f\"post training (final): {list(model.named_parameters())} w. loss: {soln.final.f}\")\n",
    "    \n",
    "    #print(torch.autograd.grad(loss, [p for p in model.parameters()], retain_graph=True, create_graph=True, allow_unused=True))\n",
    "    # Return the trained model\n",
    "    return loss, model, likelihood, get_log_fn()\n",
    "\n",
    "\n",
    "def get_std_points(mu, K):\n",
    "    x, y = np.mgrid[-3:3:.1, -3:3:.1]\n",
    "    L = np.linalg.cholesky(K)\n",
    "\n",
    "    data = np.dstack((x, y))\n",
    "\n",
    "    # Drawing the unit circle\n",
    "    # x^2 + y^2 = 1\n",
    "    precision = 50\n",
    "    unit_x = torch.cat([torch.linspace(-1, 1, precision), torch.linspace(-1, 1, precision)])\n",
    "    unit_y = torch.cat([torch.sqrt(1 - torch.linspace(-1, 1, precision)**2), -torch.sqrt(1 - torch.linspace(-1, 1, precision)**2)])\n",
    "\n",
    "    new_unit_x = list()\n",
    "    new_unit_y = list()\n",
    "\n",
    "    for tx, ty in zip(unit_x, unit_y):\n",
    "        res = np.array([tx, ty]) @ L\n",
    "        new_unit_x.append(mu[0] + 2*res[0])\n",
    "        new_unit_y.append(mu[1] + 2*res[1])\n",
    "    return new_unit_x, new_unit_y\n",
    "\n",
    "\n",
    "# Find all points inside the confidence ellipse\n",
    "def percentage_inside_ellipse(mu, K, points, sigma_level=2):\n",
    "    L = np.linalg.cholesky(K)\n",
    "    threshold = sigma_level ** 2\n",
    "    count = 0\n",
    "    for point in points:\n",
    "        res = np.array(point - mu) @ np.linalg.inv(L)\n",
    "        if res @ res <= threshold:\n",
    "            count += 1\n",
    "    return count / len(points)\n",
    "\n",
    "\n",
    "def log_dill(data, filename):\n",
    "    with open(filename, 'wb') as f:\n",
    "        dill.dump(data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.sfu.ca/sasdoc/sashtml/iml/chap11/sect8.htm\n",
    "# Also https://en.wikipedia.org/wiki/Finite_difference_coefficient\n",
    "def finite_difference_second_derivative_GP_neg_unscaled_map(model, likelihood, train_x, train_y, uninformed=False, h_i_step=1e-3, h_j_step=1e-3, h_i_vec=[0.0, 0.0, 0.0], h_j_vec=[0.0, 0.0, 0.0]):\n",
    "    curr_params = torch.tensor(list(model.parameters()))\n",
    "    mll_fkt = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "\n",
    "    h_i = h_i_step * torch.tensor(h_i_vec)\n",
    "    h_j = h_j_step * torch.tensor(h_j_vec)\n",
    "\n",
    "    fixed_reinit(model, curr_params+h_i + h_j)\n",
    "    f_plus = (-mll_fkt(model(train_x), train_y) - log_normalized_prior(model, uninformed=uninformed))*len(*model.train_inputs)\n",
    "\n",
    "    fixed_reinit(model, curr_params+h_i - h_j)\n",
    "    f1 = (-mll_fkt(model(train_x), train_y) - log_normalized_prior(model, uninformed=uninformed))*len(*model.train_inputs)\n",
    "    fixed_reinit(model, curr_params-h_i + h_j)\n",
    "    f2 = (-mll_fkt(model(train_x), train_y) - log_normalized_prior(model, uninformed=uninformed))*len(*model.train_inputs)\n",
    "\n",
    "    fixed_reinit(model, curr_params - h_i - h_j)\n",
    "    f_minus = (-mll_fkt(model(train_x), train_y) - log_normalized_prior(model, uninformed=uninformed))*len(*model.train_inputs)\n",
    "\n",
    "    # Reverse model reparameterization\n",
    "    fixed_reinit(model, curr_params)\n",
    "\n",
    "    return (f_plus - f1 - f2 + f_minus) / (4*h_i_step*h_j_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def finite_difference_hessian(model, likelihood, num_params, train_x, train_y, uninformed=False, h_i_step=5e-2, h_j_step=5e-2):\n",
    "    hessian_finite_differences_neg_unscaled_map = np.zeros((num_params, num_params))\n",
    "    for i, j in itertools.product(range(num_params), range(num_params)):\n",
    "        h_i_vec = np.zeros(num_params)\n",
    "        h_j_vec = np.zeros(num_params)\n",
    "        h_i_vec[i] = 1.0\n",
    "        h_j_vec[j] = 1.0\n",
    "        hessian_finite_differences_neg_unscaled_map[i][j] = finite_difference_second_derivative_GP_neg_unscaled_map(model, likelihood, train_x, train_y, uninformed=uninformed, h_i_step=h_i_step, h_j_step=h_j_step, h_i_vec=h_i_vec, h_j_vec=h_j_vec)\n",
    "    return hessian_finite_differences_neg_unscaled_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "def plot_parameter_progression(parameter_progression, losses=None, xlabel=None, ylabel=None, fig=None, ax=None, xdim=0, ydim=1, display_figure=True, return_figure=False, title_add=\"\"):\n",
    "    if not (fig and ax):\n",
    "        fig, ax = plt.subplots(1, 1, figsize=(8, 6))\n",
    "    \n",
    "    for i in range(1, len(parameter_progression)):\n",
    "\n",
    "\n",
    "        if losses is None:\n",
    "            colormap = plt.cm.viridis(i/len(parameter_progression))\n",
    "        else:\n",
    "            colormap = plt.cm.viridis(losses[i]/min(losses))\n",
    "        ax.plot([parameter_progression[i-1][xdim], parameter_progression[i][xdim]], [parameter_progression[i-1][ydim], parameter_progression[i][ydim]], color=colormap)\n",
    "\n",
    "        ax.annotate(\"\",\n",
    "                xy=(parameter_progression[i][xdim], parameter_progression[i][ydim]), xytext=(parameter_progression[i-1][xdim], parameter_progression[i-1][ydim]),  # Arrow from second-last to last point\n",
    "                arrowprops=dict(arrowstyle=\"->\", color=colormap, lw=2))\n",
    "    ax.xaxis.set_label_text(xlabel)\n",
    "    ax.yaxis.set_label_text(ylabel)\n",
    "    ax.set_title(f\"Parameter progression {title_add}\")\n",
    "    fig.colorbar(plt.cm.ScalarMappable(cmap=\"viridis\"), ax=ax)\n",
    "    #plt.colorbar(likelihood_surface_scatter)\n",
    "\n",
    "    if return_figure:\n",
    "        return fig, ax\n",
    "    if display_figure:\n",
    "        plt.show()\n",
    "    return None, None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_parameter_progression(parameter_progression, losses=None, xlabel=None, ylabel=None, fig=None, ax=None, xdim=0, ydim=1, display_figure=True, return_figure=False, title_add=\"\"):\n",
    "    if not (fig and ax):\n",
    "        fig, ax = plt.subplots(1, 1, figsize=(8, 6))\n",
    "    \n",
    "    # Reverse colormap by using \"viridis_r\"\n",
    "    cmap = plt.cm.viridis_r  \n",
    "\n",
    "    # Define a normalization based on loss values\n",
    "    norm = None\n",
    "    if losses is not None:\n",
    "        norm = mcolors.Normalize(vmin=min(losses[1:]), vmax=max(losses[1:]))  # Keep vmin and vmax as usual\n",
    "    \n",
    "    for i in range(1, len(parameter_progression)):\n",
    "        if losses is None:\n",
    "            colormap = cmap(i / len(parameter_progression))  # Use reversed colormap\n",
    "        else:\n",
    "            colormap = cmap(norm(losses[i]))  # Normalize and apply reversed colormap\n",
    "        \n",
    "        ax.plot([parameter_progression[i-1][xdim], parameter_progression[i][xdim]], \n",
    "                [parameter_progression[i-1][ydim], parameter_progression[i][ydim]], \n",
    "                color=colormap)\n",
    "\n",
    "        ax.annotate(\"\", xy=(parameter_progression[i][xdim], parameter_progression[i][ydim]), \n",
    "                    xytext=(parameter_progression[i-1][xdim], parameter_progression[i-1][ydim]),  \n",
    "                    arrowprops=dict(arrowstyle=\"->\", color=colormap, lw=2))\n",
    "    \n",
    "    ax.set_xlabel(xlabel)\n",
    "    ax.set_ylabel(ylabel)\n",
    "    ax.set_title(f\"Parameter progression {title_add}\")\n",
    "\n",
    "    # Add colorbar with reversed colormap\n",
    "    if losses is not None:\n",
    "        sm = plt.cm.ScalarMappable(cmap=cmap, norm=norm)  # Use reversed colormap here\n",
    "        sm.set_array([])  # Required for colorbar\n",
    "        fig.colorbar(sm, ax=ax, label=\"Loss value (higher = worse)\")\n",
    "\n",
    "    if return_figure:\n",
    "        return fig, ax\n",
    "    if display_figure:\n",
    "        plt.show()\n",
    "    return None, None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nested"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Stolen from https://matplotlib.org/stable/gallery/statistics/confidence_ellipse.html\n",
    "def confidence_ellipse(mu, K, ax, n_std=2.0, facecolor='none', **kwargs):\n",
    "    \"\"\"\n",
    "    Create a plot of the covariance confidence ellipse of *x* and *y*.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x, y : array-like, shape (n, )\n",
    "        Input data.\n",
    "\n",
    "    ax : matplotlib.axes.Axes\n",
    "        The Axes object to draw the ellipse into.\n",
    "\n",
    "    n_std : float\n",
    "        The number of standard deviations to determine the ellipse's radiuses.\n",
    "\n",
    "    **kwargs\n",
    "        Forwarded to `~matplotlib.patches.Ellipse`\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    matplotlib.patches.Ellipse\n",
    "    \"\"\"\n",
    "\n",
    "    cov = K\n",
    "    pearson = cov[0, 1]/np.sqrt(cov[0, 0] * cov[1, 1])\n",
    "    # Using a special case to obtain the eigenvalues of this\n",
    "    # two-dimensional dataset.\n",
    "    ell_radius_x = np.sqrt(1 + pearson)\n",
    "    ell_radius_y = np.sqrt(1 - pearson)\n",
    "    ellipse = Ellipse((0, 0), width=ell_radius_x * 2, height=ell_radius_y * 2,\n",
    "                      facecolor=facecolor, **kwargs)\n",
    "\n",
    "    # Calculating the standard deviation of x from\n",
    "    # the squareroot of the variance and multiplying\n",
    "    # with the given number of standard deviations.\n",
    "    scale_x = np.sqrt(cov[0, 0]) * n_std\n",
    "    mean_x = mu[0] \n",
    "\n",
    "    # calculating the standard deviation of y ...\n",
    "    scale_y = np.sqrt(cov[1, 1]) * n_std\n",
    "    mean_y = mu[1]\n",
    "\n",
    "    transf = transforms.Affine2D() \\\n",
    "        .rotate_deg(45) \\\n",
    "        .scale(scale_x, scale_y) \\\n",
    "        .translate(mean_x, mean_y)\n",
    "\n",
    "    ellipse.set_transform(transf + ax.transData)\n",
    "    return ax.add_patch(ellipse)\n",
    "\n",
    "def nested_sampling_plot(model, model_evidence_log, xdim=0, ydim=1, filter_type=\"none\", std_filter=None, show_last_num=None, return_figure=False, title_add=\"\", fig=None, ax=None, display_figure=True, plot_mll_opt=False, mll_opt_params=None, plot_lap=False, Lap0_logs=None, LapAIC_logs=None, LapBIC_logs=None, lap_colors = [\"r\", \"pink\", \"black\"], Lap_hess=None):\n",
    "\n",
    "    if not (fig and ax):\n",
    "        fig, ax = plt.subplots(1, 1, figsize=(8, 6))\n",
    "\n",
    "    with open(f\"{model_evidence_log['res file']}\", \"rb\") as f:\n",
    "        res = dill.load(f)\n",
    "    \n",
    "    # Plot the actual figure\n",
    "    param_names = [l[0] for l in list(model.named_parameters())]\n",
    "\n",
    "    # Find the best value and the corresponding hyperparameters\n",
    "    best_idx = np.argmax(res.logl)\n",
    "    best_hyperparameters = res.samples[best_idx]\n",
    "\n",
    "    # Do an outlier cleanup based on the std_filter or the last \"show_last_num\" samples\n",
    "    if show_last_num is not None:\n",
    "        if type(show_last_num) is int:\n",
    "            # Find value of \"show_last_num\" sample\n",
    "            filter_val = sorted(res.logl, reverse=True)[show_last_num]\n",
    "            mask = res.logl > filter_val\n",
    "        elif type(show_last_num) is float:\n",
    "            # Raise an error if show_last_num is not between 0 and 1\n",
    "            if show_last_num < 0 or show_last_num > 1:\n",
    "                raise ValueError(\"show_last_num must be between 0 and 1\")\n",
    "            # assume that it is a percentage of the total samples\n",
    "            filter_val = sorted(res.logl, reverse=True)[int(len(res.logl)*show_last_num)]\n",
    "            mask = res.logl > filter_val\n",
    "\n",
    "    # Do an outlier cleanup on res.logz\n",
    "    if std_filter is None and not filter_type == \"none\" and not show_last_num is None:\n",
    "        raise ValueError(\"Cannot use both filter_type and show_last_num at the same time\")\n",
    "    if show_last_num is None and not std_filter is None:\n",
    "        logz_std = np.std(res.logl)\n",
    "        if filter_type == \"max\":\n",
    "            mask = res.logl >= max(res.logl)+std_filter*logz_std\n",
    "        elif filter_type == \"mean\":\n",
    "            raise NotImplementedError(\"This filter type is not implemented yet\")\n",
    "            logz_mean = np.mean(res.logz)\n",
    "            mask = np.all(logz_mean - abs(std_filter) * logz_std <= res.logz <= logz_mean + abs(std_filter) * logz_std)\n",
    "        elif filter_type == \"none\":\n",
    "            mask = res.logl == res.logl\n",
    "\n",
    "\n",
    "\n",
    "    likelihood_surface_scatter = ax.scatter(res.samples[:,xdim][mask], res.samples[:,ydim][mask], c=res.logl[mask], s=3)\n",
    "    # Best found hyperparameters\n",
    "    ax.scatter(best_hyperparameters[xdim], best_hyperparameters[ydim], c=\"r\", s=10)\n",
    "\n",
    "    if plot_mll_opt and not mll_opt_params is None:\n",
    "        ax.scatter(mll_opt_params[xdim], mll_opt_params[ydim], c=\"black\", s=10)\n",
    "        # Add a small text beside the point saying \"MLL\"\n",
    "        ax.text(mll_opt_params[xdim], mll_opt_params[ydim], \"MLL\", fontsize=12, color=\"black\", verticalalignment='center', horizontalalignment='right')\n",
    "    \n",
    "    coverages = list()\n",
    "    if plot_lap:\n",
    "        # Plot the Laplace levels\n",
    "        for lap_log, lap_color in zip([Lap0_logs, LapAIC_logs, LapBIC_logs], lap_colors):\n",
    "            if lap_log is None:\n",
    "                continue\n",
    "            lap_param_mu = lap_log[\"parameter values\"]\n",
    "            # Wait a minute, isn't the Hessian the inverse of the covariance matrix? Yes, see Murphy PML 1 eq. (7.228)\n",
    "            lap_param_cov_matr = torch.linalg.inv(lap_log[\"corrected Hessian\"])\n",
    "            # Calculate the amount of samples that are covered by the 1 sigma and 2 sigma interval based on the lap_mu and lap_cov values\n",
    "            lap_2_sig_coverage = percentage_inside_ellipse(lap_param_mu.flatten().numpy(), lap_param_cov_matr.numpy(), res.samples[mask])\n",
    "            coverages.append(lap_2_sig_coverage)\n",
    "            #ax.scatter(lap_param_mu[xdim], lap_param_mu[ydim], c=\"b\", s=10)\n",
    "\n",
    "            # Plot the std points\n",
    "            lap_mu_filtered = lap_param_mu.numpy()[[xdim, ydim]] \n",
    "            lap_cov_filtered = lap_param_cov_matr.numpy()[[xdim, ydim]][:,[xdim, ydim]]\n",
    "            #lap_var_ellipse_x, lap_var_ellipse_y = get_std_points(lap_mu_filtered.flatten(), lap_cov_filtered)\n",
    "            #plt.scatter(lap_var_ellipse_x, lap_var_ellipse_y, c=\"b\", s=1)\n",
    "            confidence_ellipse(lap_mu_filtered, lap_cov_filtered, ax, n_std=2, edgecolor=lap_color, lw=1)\n",
    "        if not Lap_hess is None:\n",
    "            # It can happen that the Hessian is not invertible, in that case drawing the confidence ellipse is not possible\n",
    "            try:\n",
    "                lap_param_cov_matr = torch.linalg.inv(Lap_hess)\n",
    "                lap_2_sig_coverage = percentage_inside_ellipse(lap_param_mu.flatten().numpy(), lap_param_cov_matr.numpy(), res.samples[mask])\n",
    "\n",
    "                coverages.append(lap_2_sig_coverage)\n",
    "\n",
    "                lap_mu_filtered = lap_param_mu.numpy()[[xdim, ydim]] \n",
    "                lap_cov_filtered = lap_param_cov_matr.numpy()[[xdim, ydim]][:,[xdim, ydim]]\n",
    "                confidence_ellipse(lap_mu_filtered, lap_cov_filtered, ax, n_std=2, edgecolor=\"black\", lw=1)\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "\n",
    "\n",
    "    if show_last_num is not None:\n",
    "        ax.set_title(f\"#Samples: {sum(res.ncall)}; {coverages[0]*100:.0f}% inside 2 sigma\\n{show_last_num} best accepted samples\")\n",
    "    elif not std_filter is None:\n",
    "        ax.set_title(f\"#Samples: {sum(res.ncall)}; {coverages[0]*100:.0f}% inside 2 sigma\\n{filter_type}: {std_filter:.0e}\")\n",
    "    else:\n",
    "        ax.set_title(f\"#Samples: {sum(res.ncall)}; {coverages[0]*100:.0f}% inside 2 sigma\\n\")\n",
    "    ax.set_xlabel(param_names[xdim])\n",
    "\n",
    "    plt.colorbar(likelihood_surface_scatter)\n",
    "\n",
    "    if return_figure:\n",
    "        return fig, ax\n",
    "    if display_figure:\n",
    "        plt.show()\n",
    "    return None, None\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def outlier_cleanup(values, mode=\"none\", filter=None):\n",
    "    \"\"\"\n",
    "    Outlier cleanup function for the nested sampling results.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    values : array-like\n",
    "        The values to be filtered.\n",
    "    mode : str\n",
    "        The mode of filtering. Can be \"none\", \"max\", \"min\", \"mean\", or \"value\".\n",
    "    filter : float or int\n",
    "        The filter value. If mode is \"max\" or \"min\", it can be a percentage (float) or a number of samples (int). \n",
    "        If mode is \"mean\", it is the number of standard deviations from the mean. If mode is \"value\", it is the threshold value.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Filter modes: \"none\", \"max\", \"mean\", \"value\"\n",
    "    # \"none\": There is no filtering, just pass everything\n",
    "    # \"max\": Only display the highest \"std_filter\" samples. If \"std_filter\" is float, use percent, if int, use the number of samples\n",
    "    # \"min\": Only display the smallest \"std_filter\" samples. If \"std_filter\" is float, use percent, if int, use the number of samples\n",
    "    # \"mean\": Only display the samples that are within \"std_filter\" standard deviations of the mean\n",
    "    # \"value\": Only display the samples that are greater than \"std_filter\"\n",
    "\n",
    "    masked=None\n",
    "\n",
    "    if mode == \"none\":\n",
    "        masked = values == values\n",
    "    elif mode == \"min\":\n",
    "        if type(filter) is float:\n",
    "            # Raise an error if show_last_num is not between 0 and 1\n",
    "            if filter < 0 or filter > 1:\n",
    "                raise ValueError(\"filter must be between 0 and 1\")\n",
    "            # assume that it is a percentage of the total samples\n",
    "            filter_val = sorted(values)[int(len(values)*filter)]\n",
    "            masked = values < filter_val\n",
    "        elif type(filter) is int:\n",
    "            # Find value of \"show_last_num\" sample\n",
    "            filter_val = sorted(values)[filter]\n",
    "            masked = values < filter_val \n",
    "    elif mode == \"max\":\n",
    "        if type(filter) is float:\n",
    "            # Raise an error if show_last_num is not between 0 and 1\n",
    "            if filter < 0 or filter > 1:\n",
    "                raise ValueError(\"filter must be between 0 and 1\")\n",
    "            # assume that it is a percentage of the total samples\n",
    "            filter_val = sorted(values, reverse=True)[int(len(values)*filter)]\n",
    "            masked = values > filter_val\n",
    "        elif type(filter) is int:\n",
    "            # Find value of \"show_last_num\" sample\n",
    "            filter_val = sorted(values, reverse=True)[filter]\n",
    "            masked = values > filter_val \n",
    "    elif mode == \"mean\":\n",
    "        raise NotImplementedError(\"This filter type is not implemented yet\")\n",
    "        logz_mean = np.mean(res.logz)\n",
    "        masked = np.all(logz_mean - abs(filter) * logz_std <= res.logz <= logz_mean + abs(std_filter) * logz_std)\n",
    "    elif mode == \"value\":\n",
    "        masked = values > filter\n",
    "    return masked\n",
    "\n",
    "\n",
    "\n",
    "# More general version of the nested sampling plotting function\n",
    "def scatter_function_visualization(positions, values, fig=None, ax=None, axis_names=None, show_best=False, xdim=0, ydim=1, filter_mode=\"none\", std_filter=None, show_last_num=None, return_figure=False, title_add=\"\", display_figure=True):\n",
    "   \n",
    "    if not (fig and ax):\n",
    "        fig, ax = plt.subplots(1, 1, figsize=(8, 6)) \n",
    "\n",
    "    mask = outlier_cleanup(values, filter_mode, std_filter)\n",
    "    \n",
    "    filtered_x_dim = positions[:,xdim][mask]\n",
    "    filtered_y_dim = positions[:,ydim][mask]\n",
    "    filtered_colors = values[mask]\n",
    "    likelihood_surface_scatter = ax.scatter(filtered_x_dim, filtered_y_dim, c=filtered_colors, s=3)\n",
    "\n",
    "    if show_best:\n",
    "        # Find the best value and the corresponding hyperparameters\n",
    "        best_idx = np.argmax(values)\n",
    "        best_hyperparameters = positions[best_idx]\n",
    "        # Best found hyperparameters\n",
    "        ax.scatter(best_hyperparameters[xdim], best_hyperparameters[ydim], c=\"r\", s=10)\n",
    "\n",
    "    if show_last_num is not None:\n",
    "        ax.set_title(f\"#Samples: {len(values)}; {show_last_num} best accepted samples\")\n",
    "    elif not std_filter is None:\n",
    "        ax.set_title(f\"#Samples: {len(values)}; {filter_mode}: {std_filter:.0e}\")\n",
    "    else:\n",
    "        ax.set_title(f\"#Samples: {len(values)}; \")\n",
    "    ax.set_xlabel(axis_names[0])\n",
    "    ax.set_ylabel(axis_names[1])\n",
    "\n",
    "    plt.colorbar(likelihood_surface_scatter)\n",
    "\n",
    "    if return_figure:\n",
    "        return fig, ax\n",
    "    if display_figure:\n",
    "        plt.show()\n",
    "    return None, None\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_data(X, Y, return_figure=False, title_add=\"\", figure=None, ax=None, display_figure=True):\n",
    "    fig = plt.figure(figsize=(8, 6))\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.plot(X.numpy(), Y.numpy(), 'k.')\n",
    "    ax.set_xlabel('X')\n",
    "    ax.set_ylabel('Y')\n",
    "    ax.set_title(f\"Data {title_add}\")\n",
    "    if not return_figure and display_figure:\n",
    "        plt.show()\n",
    "    else:\n",
    "        return fig, ax\n",
    "\n",
    "def plot_model(model, likelihood, X, Y, display_figure=True, return_figure=False, figure=None,\n",
    "               ax=None, loss_val=None, loss_type = None):\n",
    "    interval_length = torch.max(X) - torch.min(X)\n",
    "    shift = interval_length * options[\"plotting\"][\"border_ratio\"]\n",
    "    test_x = torch.linspace(torch.min(\n",
    "        X) - shift, torch.max(X) + shift, options[\"plotting\"][\"sample_points\"])\n",
    "\n",
    "    with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "        observed_pred = likelihood(model(test_x))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        if not (figure and ax):\n",
    "            figure, ax = plt.subplots(1, 1, figsize=(8, 6))\n",
    "\n",
    "        lower, upper = observed_pred.confidence_region()\n",
    "        ax.plot(X.numpy(), Y.numpy(), 'k.', zorder=2)\n",
    "        ax.plot(test_x.numpy(), observed_pred.mean.numpy(), color=\"b\", zorder=3)\n",
    "        amount_of_gradient_steps = 30\n",
    "        alpha_min = 0.05\n",
    "        alpha_max = 0.8\n",
    "        alpha = (alpha_max-alpha_min)/amount_of_gradient_steps\n",
    "        c = ax.fill_between(test_x.numpy(), lower.numpy(), upper.numpy(\n",
    "        ), alpha=alpha+alpha_min, zorder=1).get_facecolor()\n",
    "\n",
    "        for i in range(1, amount_of_gradient_steps):\n",
    "            ax.fill_between(test_x.numpy(), (lower+(i/amount_of_gradient_steps)*(upper-lower)).numpy(),\n",
    "                            (upper-(i/amount_of_gradient_steps)*(upper-lower)).numpy(), alpha=alpha, color=c, zorder=1)\n",
    "        if options[\"plotting\"][\"legend\"]:\n",
    "            ax.plot([], [], 'k.', label=\"Data\")\n",
    "            ax.plot([], [], 'b', label=\"Mean\")\n",
    "            ax.plot([], [], color=c, alpha=1.0, label=\"Confidence\")\n",
    "            ax.legend(loc=\"upper left\")\n",
    "        ax.set_xlabel(\"Normalized Input\")\n",
    "        ax.set_ylabel(\"Normalized Output\")\n",
    "        ax.set_title(f\"{loss_type}: {loss_val:.2f}\")\n",
    "    if not return_figure and display_figure:\n",
    "        plt.show()\n",
    "    else:\n",
    "        return figure, ax\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a nested sampling plot, but multiply each point with the respective prior likelihood, thus generating the Posterior surface instead of the likelihood surface\n",
    "\n",
    "\n",
    "def posterior_surface_plot(model, model_evidence_log, xdim=0, ydim=1, filter_type=\"none\", std_filter=None, show_last_num=None, return_figure=False, title_add=\"\", fig=None, ax=None, display_figure=True, plot_mll_opt=False, mll_opt_params=None, plot_lap=False, Lap0_logs=None, LapAIC_logs=None, LapBIC_logs=None, lap_colors = [\"r\", \"pink\", \"black\"], Lap_hess=None, uninformed=False):\n",
    "\n",
    "    if not (fig and ax):\n",
    "        fig, ax = plt.subplots(1, 1, figsize=(8, 6))\n",
    "\n",
    "    with open(f\"{model_evidence_log['res file']}\", \"rb\") as f:\n",
    "        res = dill.load(f)\n",
    "    \n",
    "    # Plot the actual figure\n",
    "    param_names = [l[0] for l in list(model.named_parameters())]\n",
    "\n",
    "\n",
    "    # rescale the \"res.samples\" by the prior likelihood\n",
    "    prior_mu, prior_sigma = prior_distribution(model=model, uninformed=uninformed)\n",
    "    prior = torch.distributions.MultivariateNormal(prior_mu.t(), prior_sigma)\n",
    "    posterior_samples = np.array([logl+prior.log_prob(torch.tensor(sample)).item() for sample, logl in zip(res.samples, res.logl)])\n",
    "\n",
    "    # Find the best value and the corresponding hyperparameters\n",
    "    best_idx = np.argmax(posterior_samples)\n",
    "    best_hyperparameters = res.samples[best_idx]\n",
    "\n",
    "    # Do an outlier cleanup based on the std_filter or the last \"show_last_num\" samples\n",
    "    if show_last_num is not None:\n",
    "        if type(show_last_num) is int:\n",
    "            # Find value of \"show_last_num\" sample\n",
    "            filter_val = sorted(posterior_samples, reverse=True)[show_last_num]\n",
    "            mask = posterior_samples > filter_val\n",
    "        elif type(show_last_num) is float:\n",
    "            # Raise an error if show_last_num is not between 0 and 1\n",
    "            if show_last_num < 0 or show_last_num > 1:\n",
    "                raise ValueError(\"show_last_num must be between 0 and 1\")\n",
    "            # assume that it is a percentage of the total samples\n",
    "            filter_val = sorted(posterior_samples, reverse=True)[int(len(posterior_samples)*show_last_num)]\n",
    "            mask = posterior_samples > filter_val\n",
    "\n",
    "    # Do an outlier cleanup on res.logz\n",
    "    if std_filter is None and not filter_type == \"none\" and not show_last_num is None:\n",
    "        raise ValueError(\"Cannot use both filter_type and show_last_num at the same time\")\n",
    "    if show_last_num is None and not std_filter is None:\n",
    "        logz_std = np.std(posterior_samples)\n",
    "        if filter_type == \"max\":\n",
    "            mask = posterior_samples >= max(posterior_samples)+std_filter*logz_std\n",
    "        elif filter_type == \"mean\":\n",
    "            raise NotImplementedError(\"This filter type is not implemented yet\")\n",
    "            logz_mean = np.mean(res.logz)\n",
    "            mask = np.all(logz_mean - abs(std_filter) * logz_std <= res.logz <= logz_mean + abs(std_filter) * logz_std)\n",
    "        elif filter_type == \"none\":\n",
    "            mask = posterior_samples == posterior_samples\n",
    "\n",
    "\n",
    "    posterior_surface_scatter = ax.scatter(res.samples[:,xdim][mask], res.samples[:,ydim][mask], c=posterior_samples[mask], s=3)\n",
    "    # Best found hyperparameters\n",
    "    ax.scatter(best_hyperparameters[xdim], best_hyperparameters[ydim], c=\"r\", s=10)\n",
    "\n",
    "    if plot_mll_opt and not mll_opt_params is None:\n",
    "        ax.scatter(mll_opt_params[xdim], mll_opt_params[ydim], c=\"black\", s=10)\n",
    "        # Add a small text beside the point saying \"MLL\"\n",
    "        ax.text(mll_opt_params[xdim], mll_opt_params[ydim], \"MLL\", fontsize=12, color=\"black\", verticalalignment='center', horizontalalignment='right')\n",
    "    \n",
    "    coverages = list()\n",
    "    if plot_lap:\n",
    "        # Plot the Laplace levels\n",
    "        for lap_log, lap_color in zip([Lap0_logs, LapAIC_logs, LapBIC_logs], lap_colors):\n",
    "            if lap_log is None:\n",
    "                continue\n",
    "            lap_param_mu = lap_log[\"parameter values\"]\n",
    "            # Wait a minute, isn't the Hessian the inverse of the covariance matrix? Yes, see Murphy PML 1 eq. (7.228)\n",
    "            lap_param_cov_matr = torch.linalg.inv(lap_log[\"corrected Hessian\"])\n",
    "            # Calculate the amount of samples that are covered by the 1 sigma and 2 sigma interval based on the lap_mu and lap_cov values\n",
    "            lap_2_sig_coverage = percentage_inside_ellipse(lap_param_mu.flatten().numpy(), lap_param_cov_matr.numpy(), res.samples[mask])\n",
    "            coverages.append(lap_2_sig_coverage)\n",
    "            #ax.scatter(lap_param_mu[xdim], lap_param_mu[ydim], c=\"b\", s=10)\n",
    "\n",
    "            # Plot the std points\n",
    "            lap_mu_filtered = lap_param_mu.numpy()[[xdim, ydim]] \n",
    "            lap_cov_filtered = lap_param_cov_matr.numpy()[[xdim, ydim]][:,[xdim, ydim]]\n",
    "            #lap_var_ellipse_x, lap_var_ellipse_y = get_std_points(lap_mu_filtered.flatten(), lap_cov_filtered)\n",
    "            #plt.scatter(lap_var_ellipse_x, lap_var_ellipse_y, c=\"b\", s=1)\n",
    "            confidence_ellipse(lap_mu_filtered, lap_cov_filtered, ax, n_std=2, edgecolor=lap_color, lw=1)\n",
    "        if not Lap_hess is None:\n",
    "            # It can happen that the Hessian is not invertible, in that case drawing the confidence ellipse is not possible\n",
    "            try:\n",
    "                lap_param_cov_matr = torch.linalg.inv(Lap_hess)\n",
    "                lap_2_sig_coverage = percentage_inside_ellipse(lap_param_mu.flatten().numpy(), lap_param_cov_matr.numpy(), res.samples[mask])\n",
    "\n",
    "                coverages.append(lap_2_sig_coverage)\n",
    "\n",
    "                lap_mu_filtered = lap_param_mu.numpy()[[xdim, ydim]] \n",
    "                lap_cov_filtered = lap_param_cov_matr.numpy()[[xdim, ydim]][:,[xdim, ydim]]\n",
    "                confidence_ellipse(lap_mu_filtered, lap_cov_filtered, ax, n_std=2, edgecolor=\"black\", lw=1)\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "\n",
    "\n",
    "    if show_last_num is not None:\n",
    "        ax.set_title(f\"#Samples: {sum(res.ncall)}; {coverages[0]*100:.0f}% inside 2 sigma\\n{show_last_num} best accepted samples\")\n",
    "    elif not std_filter is None:\n",
    "        ax.set_title(f\"#Samples: {sum(res.ncall)}; {coverages[0]*100:.0f}% inside 2 sigma\\n{filter_type}: {std_filter:.0e}\")\n",
    "    else:\n",
    "        ax.set_title(f\"#Samples: {sum(res.ncall)}; {coverages[0]*100:.0f}% inside 2 sigma\\n\")\n",
    "    ax.set_xlabel(param_names[xdim])\n",
    "\n",
    "    plt.colorbar(posterior_surface_scatter)\n",
    "\n",
    "    if return_figure:\n",
    "        return fig, ax\n",
    "    if display_figure:\n",
    "        plt.show()\n",
    "    return None, None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_3d_data(samples, xx, yy, return_figure=False, fig=None, ax=None, display_figure=True, title_add = \"\", shadow=True):\n",
    "    \"\"\"\n",
    "    Similar to plot_3d_gp_samples, but color-codes each (xx, yy) point in 3D.\n",
    "    'samples' can be a single 1D tensor or multiple samples in a 2D tensor.\n",
    "    \"\"\"\n",
    "    if not (fig and ax):\n",
    "        fig = plt.figure(figsize=(8, 6))\n",
    "        ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "    #if samples.ndim == 1:\n",
    "    #    samples = samples.unsqueeze(0)\n",
    "\n",
    "    #z_vals = samples.reshape(xx.shape)\n",
    "    z_vals = samples\n",
    "    ax.scatter(xx.numpy(), yy.numpy(), z_vals.numpy(),\n",
    "                c=z_vals.numpy(), cmap='viridis', alpha=0.8)\n",
    "\n",
    "\n",
    "    if shadow:\n",
    "        # Plot shadows (projection on X-Y plane at z=0)\n",
    "        ax.scatter(xx.numpy(), yy.numpy(), \n",
    "                np.ones_like(z_vals)*min(z_vals.numpy()), \n",
    "                c='gray', alpha=0.3, marker='o')\n",
    "\n",
    "\n",
    "\n",
    "    ax.set_title(f'Data {title_add}')\n",
    "    ax.set_xlabel('X')\n",
    "    ax.set_ylabel('Y')\n",
    "    ax.set_zlabel('Output Value')\n",
    "    if not return_figure and display_figure:\n",
    "        plt.show()\n",
    "    else:\n",
    "        return fig, ax\n",
    "\n",
    "\n",
    "def plot_3d_gp_samples(samples, xx, yy, return_figure=False, fig=None, ax=None, display_figure=True):\n",
    "    \"\"\"\n",
    "    Visualize multiple samples drawn from a 2D-input (xx, yy) -> 1D-output GP in 3D.\n",
    "    Each sample in 'samples' should be a 1D tensor that can be reshaped to match xx, yy.\n",
    "    \"\"\"\n",
    "    if not (fig and ax):\n",
    "        fig = plt.figure(figsize=(8, 6))\n",
    "        ax = fig.add_subplot(111, projection='3d')\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    if samples.ndim == 1:\n",
    "        samples = samples.unsqueeze(0)\n",
    "    for i, sample in enumerate(samples):\n",
    "        z_vals = sample.reshape(xx.shape)\n",
    "        ax.plot_surface(xx.numpy(), yy.numpy(), z_vals.numpy(), alpha=0.4)\n",
    "\n",
    "    ax.set_title('GP Samples in 3D')\n",
    "    ax.set_xlabel('X')\n",
    "    ax.set_ylabel('Y')\n",
    "    ax.set_zlabel('Output')\n",
    "    if not return_figure and display_figure:\n",
    "        plt.show()\n",
    "    else:\n",
    "        return fig, ax\n",
    "\n",
    "\n",
    "def plot_3d_gp(model, likelihood, data=None, x_min=0.0, x_max=1.0, y_min=0.0, y_max=1.0,\n",
    "                resolution=50, return_figure=False, fig=None, ax=None, \n",
    "                display_figure=True, loss_val=None, loss_type=None, shadow=False,\n",
    "                title_add = \"\"):\n",
    "    if not (fig and ax):\n",
    "        fig = plt.figure(figsize=(8, 6))\n",
    "        ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "\n",
    "    if data is not None:\n",
    "        if data.ndim == 1:\n",
    "            data = data.unsqueeze(0)\n",
    "\n",
    "        for sample in data:\n",
    "            xx = sample[0]\n",
    "            yy = sample[1]\n",
    "            zz = sample[2]\n",
    "            ax.scatter(xx.numpy(), yy.numpy(), zz.numpy(), color=\"red\", alpha=0.8)\n",
    "\n",
    "            if shadow:\n",
    "                # Plot shadows (projection on X-Y plane at z=0)\n",
    "                ax.scatter(xx.numpy(), yy.numpy(), \n",
    "                        min(data[:,2].numpy()), \n",
    "                        c='gray', alpha=0.3, marker='o')\n",
    "    model.eval()\n",
    "    likelihood.eval()\n",
    "\n",
    "    x_vals = torch.linspace(x_min, x_max, resolution)\n",
    "    y_vals = torch.linspace(y_min, y_max, resolution)\n",
    "    xx, yy = torch.meshgrid(x_vals, y_vals)\n",
    "    test_x = torch.stack([xx.reshape(-1), yy.reshape(-1)], dim=-1)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        preds = likelihood(model(test_x))\n",
    "        mean = preds.mean.reshape(resolution, resolution)\n",
    "        lower, upper = preds.confidence_region()\n",
    "        lower = lower.reshape(resolution, resolution)\n",
    "        upper = upper.reshape(resolution, resolution)\n",
    "\n",
    "\n",
    "    # Plot mean surface\n",
    "    ax.plot_surface(xx.numpy(), yy.numpy(), mean.numpy(), cmap='viridis', alpha=0.8)\n",
    "\n",
    "    # Plot lower and upper surfaces\n",
    "    ax.plot_surface(xx.numpy(), yy.numpy(), lower.numpy(), color='gray', alpha=0.2)\n",
    "    ax.plot_surface(xx.numpy(), yy.numpy(), upper.numpy(), color='gray', alpha=0.2)\n",
    "\n",
    "    ax.set_title(f'2D GP in 3D {title_add}')\n",
    "    if loss_val is not None:\n",
    "        ax.set_title(f\"{ax.title.get_text()}; {loss_type}: {loss_val}\")\n",
    "    ax.set_xlabel('X1')\n",
    "    ax.set_ylabel('X2')\n",
    "    ax.set_zlabel('Mean and Variance Range')\n",
    "\n",
    "    if not return_figure and display_figure:\n",
    "        plt.show()\n",
    "    else:\n",
    "        return fig, ax\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DatasetInfo = namedtuple('DatasetInfo', ['name', 'path', 'dimension', 'description'])\n",
    "datasets_1d = {\n",
    "    \"alternating\" : DatasetInfo('alternating', 'datasets/alternating.csv', 1, 'Alternates between high and low'),\n",
    "    \"broaden\" : DatasetInfo('broaden', 'datasets/broaden.csv', 1, 'Data with increasing variance'),\n",
    "    \"large gap\" : DatasetInfo('large gap', 'datasets/large_gap.csv', 1, 'Contains large gap between X values'),\n",
    "    \"linear outlier\" : DatasetInfo('linear outlier', 'datasets/linear_outlier.csv', 1, 'Linear data with outlier in last entry'),\n",
    "    \"parabola\" : DatasetInfo('parabola', 'datasets/parabola.csv', 1, 'Parabola'),\n",
    "    \"periodic linear\" : DatasetInfo('periodic linear', 'datasets/periodic_linear.csv', 1, 'Periodic data that changes to becoming purely linear'),\n",
    "    \"periodic\" : DatasetInfo('periodic', 'datasets/periodic.csv', 1, 'Periodic data'),\n",
    "    \"periodic_1D flattened\" : DatasetInfo('periodic', 'datasets/periodic_1D flattened.csv', 1, 'Copy of the 2D dataset that effectively only happens on the X axis'),\n",
    "    \"v_lines\" : DatasetInfo('v_lines', 'datasets/v_lines.csv', 1, 'Vertical lines'),\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "# named tuple or dictionary for the datasets, same as 1D case\n",
    "DatasetInfo = namedtuple('DatasetInfo', ['name', 'path', 'dimension', 'description'])\n",
    "datasets_2d = {\n",
    "    \"away\": DatasetInfo('away', 'datasets/2D/away.csv', 2, 'Away from the center'),\n",
    "    \"bullseye\": DatasetInfo('bullseye', 'datasets/2D/bullseye.csv', 2, 'Two circles forming a bullseye'),\n",
    "    \"circle\": DatasetInfo('circle', 'datasets/2D/circle.csv', 2, 'Circle'),\n",
    "    \"dots\": DatasetInfo('dots', 'datasets/2D/dots.csv', 2, 'An equidistant dot grid'),\n",
    "    \"dots favouring lower values\": DatasetInfo('dots favouring lower values', 'datasets/2D/dots favouring lower values.csv', 2, 'An equidistant dot grid where the high point of Periodic_1D is alone and there are multiple duplicates of the low values'),\n",
    "    \"dots minimal\": DatasetInfo('dots minimal', 'datasets/2D/dots minimal.csv', 2, 'An equidistant dot grid with 9 points'),\n",
    "    \"dots minimal no shift\": DatasetInfo('dots minimal', 'datasets/2D/dots minimal no shift.csv', 2, 'An equidistant dot grid with 9 points'),\n",
    "    \"dots minimal shifted\": DatasetInfo('dots minimal', 'datasets/2D/dots minimal shifted.csv', 2, 'An equidistant dot grid with 9 points, all shifted by 0.5 to each other'),\n",
    "    \"dots minimal shifted to zero\": DatasetInfo('dots minimal', 'datasets/2D/dots minimal shifted to zero.csv', 2, 'Basically \"dots minimal shifted\", but such that zero is the centre.'),\n",
    "    \"dots minimal shifted around zero\": DatasetInfo('dots minimal', 'datasets/2D/dots minimal shifted around zero.csv', 2, 'An equidistant dot grid with 9 points, centred around zero.'),\n",
    "    \"line\": DatasetInfo('line', 'datasets/2D/line.csv', 2, 'One diagonal line across X-Y'),\n",
    "    \"line X\": DatasetInfo('line', 'datasets/2D/line X.csv', 2, 'One diagonal line that strictly follows X'),\n",
    "    \"line Y\": DatasetInfo('line', 'datasets/2D/line Y.csv', 2, 'One diagonal line that strictly follows Y'),\n",
    "    \"line 05X1Y\": DatasetInfo('line', 'datasets/2D/line 05X1Y.csv', 2, 'One diagonal line across X-Y, where X is 0.5 slower than Y'),\n",
    "    \"h_lines\": DatasetInfo('h_lines', 'datasets/2D/h_lines.csv', 2, 'Horizontal lines'),\n",
    "    \"high lines\": DatasetInfo('high lines', 'datasets/2D/high_lines.csv', 2, 'Horizontal lines with a large gap between them'),\n",
    "    \"slant down\": DatasetInfo('slant down', 'datasets/2D/slant_down.csv', 2, 'Straight lines slanting downwards'),\n",
    "    \"slant up\": DatasetInfo('slant up', 'datasets/2D/slant_up.csv', 2, 'Straight lines slanting upwards'),\n",
    "    \"star\": DatasetInfo('star', 'datasets/2D/star.csv', 2, 'Star shape'),\n",
    "    \"v_lines\": DatasetInfo('v_lines', 'datasets/2D/v_lines.csv', 2, 'Vertical lines'),\n",
    "    \"wide lines\": DatasetInfo('wide lines', 'datasets/2D/wide_lines.csv', 2, 'Vertical lines with a large gap between them'),\n",
    "    \"x_shape\": DatasetInfo('x_shape', 'datasets/2D/x_shape.csv', 2, 'X shape'),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GP definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class ExactGPModel(gpytorch.models.ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood, kernel_text=\"RBF\", weights=None):\n",
    "        super(ExactGPModel, self).__init__(train_x, train_y, likelihood)\n",
    "        self.mean_module = gpytorch.means.ZeroMean()\n",
    "\n",
    "        if kernel_text == \"C*C*SE\":\n",
    "            self.covar_module = gpytorch.kernels.ScaleKernel(\n",
    "                gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel()))\n",
    "        elif kernel_text == \"SE\":\n",
    "            self.covar_module = gpytorch.kernels.RBFKernel()\n",
    "        elif kernel_text == \"C*SE\":\n",
    "            self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel())\n",
    "        elif kernel_text == \"LIN\":\n",
    "            self.covar_module = gpytorch.kernels.LinearKernel()\n",
    "        elif kernel_text == \"C*LIN\":\n",
    "            self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.LinearKernel())\n",
    "        elif kernel_text == \"LIN*SE\":\n",
    "            self.covar_module = gpytorch.kernels.LinearKernel() * gpytorch.kernels.RBFKernel()\n",
    "        elif kernel_text == \"LIN*PER\":\n",
    "            self.covar_module = gpytorch.kernels.LinearKernel() * gpytorch.kernels.PeriodicKernel()\n",
    "        elif kernel_text == \"SE+SE\":\n",
    "            self.covar_module =  gpytorch.kernels.RBFKernel() + gpytorch.kernels.RBFKernel()\n",
    "        elif kernel_text == \"RQ\":\n",
    "            self.covar_module = gpytorch.kernels.RQKernel()\n",
    "        elif kernel_text == \"PER\":\n",
    "            self.covar_module = gpytorch.kernels.PeriodicKernel()\n",
    "        #elif kernel_text == \"PER+SE\":\n",
    "        #    if weights is None:\n",
    "        #        self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.PeriodicKernel()) + gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel())\n",
    "        #    else:\n",
    "        #        self.covar_module = weights[0]*gpytorch.kernels.PeriodicKernel() + weights[1]*gpytorch.kernels.RBFKernel()\n",
    "        elif kernel_text == \"PER*SE\":\n",
    "            self.covar_module = gpytorch.kernels.PeriodicKernel() * gpytorch.kernels.RBFKernel()\n",
    "        #elif kernel_text == \"PER*LIN\":\n",
    "        #    self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.PeriodicKernel()) * gpytorch.kernels.ScaleKernel(gpytorch.kernels.LinearKernel())\n",
    "        elif kernel_text == \"MAT32\":\n",
    "            self.covar_module = gpytorch.kernels.MaternKernel(nu=1.5)\n",
    "        elif kernel_text == \"MAT32+MAT52\":\n",
    "            self.covar_module =  gpytorch.kernels.MaternKernel(nu=1.5) + gpytorch.kernels.MaternKernel()\n",
    "        elif kernel_text == \"MAT32*PER\":\n",
    "            self.covar_module =  gpytorch.kernels.MaternKernel(nu=1.5) * gpytorch.kernels.PeriodicKernel()\n",
    "        elif kernel_text == \"MAT32+PER\":\n",
    "            self.covar_module =  gpytorch.kernels.MaternKernel(nu=1.5) + gpytorch.kernels.PeriodicKernel()\n",
    "        elif kernel_text == \"MAT32*SE\":\n",
    "            self.covar_module =  gpytorch.kernels.MaternKernel(nu=1.5) * gpytorch.kernels.RBFKernel()\n",
    "        elif kernel_text == \"MAT32+SE\":\n",
    "            self.covar_module =  gpytorch.kernels.MaternKernel(nu=1.5) + gpytorch.kernels.RBFKernel()\n",
    "        elif kernel_text == \"MAT52\":\n",
    "            self.covar_module = gpytorch.kernels.MaternKernel()\n",
    "        elif kernel_text == \"MAT52*PER\":\n",
    "            self.covar_module =  gpytorch.kernels.MaternKernel() * gpytorch.kernels.PeriodicKernel()\n",
    "        elif kernel_text == \"MAT52+SE\":\n",
    "            self.covar_module =  gpytorch.kernels.MaternKernel() + gpytorch.kernels.RBFKernel()\n",
    "        elif kernel_text == \"SE*SE\":\n",
    "            self.covar_module =  gpytorch.kernels.RBFKernel() * gpytorch.kernels.RBFKernel()\n",
    "        elif kernel_text == \"(SE+RQ)*PER\":\n",
    "            self.covar_module =  (gpytorch.kernels.RBFKernel() + gpytorch.kernels.RQKernel()) * gpytorch.kernels.PeriodicKernel()\n",
    "        elif kernel_text == \"SE+SE+SE\":\n",
    "            self.covar_module =  gpytorch.kernels.RBFKernel() + gpytorch.kernels.RBFKernel() + gpytorch.kernels.RBFKernel()\n",
    "        elif kernel_text == \"MAT32+(MAT52*PER)\":\n",
    "            self.covar_module =  gpytorch.kernels.MaternKernel(nu=1.5) + (gpytorch.kernels.MaternKernel() * gpytorch.kernels.PeriodicKernel())\n",
    "\n",
    "        #elif kernel_text == \"RQ*PER\":\n",
    "        #    self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RQKernel()) * gpytorch.kernels.ScaleKernel(gpytorch.kernels.PeriodicKernel())\n",
    "        #elif kernel_text == \"RQ*MAT32\":\n",
    "        #    self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RQKernel()) * gpytorch.kernels.ScaleKernel(gpytorch.kernels.MaternKernel(nu=1.5))\n",
    "        #elif kernel_text == \"RQ*SE\":\n",
    "        #    self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RQKernel()) * gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel())\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "\n",
    "\n",
    "\n",
    "class ExactMIGPModel(gpytorch.models.ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood, kernel_text=\"SE\", weights=None):\n",
    "        super(ExactMIGPModel, self).__init__(train_x, train_y, likelihood)\n",
    "        self.mean_module = gpytorch.means.ZeroMean()\n",
    "        if kernel_text == \"[SE; SE]\":\n",
    "            self.covar_module = gpytorch.kernels.AdditiveStructureKernel(gpytorch.kernels.RBFKernel(active_dims=0) + gpytorch.kernels.RBFKernel(active_dims=1), num_dims=2)\n",
    "        elif kernel_text == \"[SE+SE; 1]2\":\n",
    "            self.covar_module = gpytorch.kernels.RBFKernel(active_dims=0) + gpytorch.kernels.RBFKernel(active_dims=0)\n",
    "        elif kernel_text == \"[SE+SE; 1]\":\n",
    "            self.covar_module = gpytorch.kernels.AdditiveStructureKernel(gpytorch.kernels.RBFKernel(active_dims=0) + gpytorch.kernels.RBFKernel(active_dims=0), num_dims=2)\n",
    "        elif kernel_text == \"[SE;* SE]\":\n",
    "            #TODO\n",
    "            self.covar_module = gpytorch.kernels.AdditiveStructureKernel(gpytorch.kernels.RBFKernel(active_dims=0) + gpytorch.kernels.RBFKernel(active_dims=1), num_dims=2)\n",
    "        elif kernel_text == \"[SE; LIN]\":\n",
    "            self.covar_module = gpytorch.kernels.AdditiveStructureKernel(gpytorch.kernels.RBFKernel(active_dims=0) + gpytorch.kernels.LinearKernel(active_dims=1), num_dims=2)\n",
    "        elif kernel_text == \"[LIN; SE]\":\n",
    "            self.covar_module = gpytorch.kernels.AdditiveStructureKernel(gpytorch.kernels.LinearKernel(active_dims=0) + gpytorch.kernels.RBFKernel(active_dims=1), num_dims=2)\n",
    "        elif kernel_text == \"[LIN; LIN]\":\n",
    "            self.covar_module = gpytorch.kernels.AdditiveStructureKernel(gpytorch.kernels.LinearKernel(active_dims=0) + gpytorch.kernels.LinearKernel(active_dims=1), num_dims=2)\n",
    "        elif kernel_text == \"[LIN;* LIN]\":\n",
    "            self.covar_module = gpytorch.kernels.LinearKernel(active_dims=0) * gpytorch.kernels.LinearKernel(active_dims=1)\n",
    "        elif kernel_text == \"[SE;* 1]\":\n",
    "            self.covar_module = gpytorch.kernels.RBFKernel(active_dims=0)\n",
    "        elif kernel_text == \"[PER;* 1]\":\n",
    "            self.covar_module = gpytorch.kernels.PeriodicKernel(active_dims=0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Various 1D and 2D datasets "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1D data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "possible_kenels = [\n",
    "    \"SE\",\n",
    "    \"C*SE\",\n",
    "    \"C*C*SE\",\n",
    "    \"LIN\",\n",
    "    \"C*LIN\",\n",
    "    \"LIN*SE\",\n",
    "    \"LIN*PER\",\n",
    "    \"SE+SE\",\n",
    "    \"RQ\",\n",
    "    \"PER\",\n",
    "    \"PER*SE\",\n",
    "    \"MAT32\",\n",
    "    \"MAT32+MAT52\",\n",
    "    \"MAT32*PER\",\n",
    "    \"MAT32+PER\",\n",
    "    \"MAT32*SE\",\n",
    "    \"MAT32+SE\",\n",
    "    \"MAT52\",\n",
    "    \"MAT52*PER\",\n",
    "    \"MAT52+SE\",\n",
    "    \"SE*SE\",\n",
    "    \"(SE+RQ)*PER\",\n",
    "    \"SE+SE+SE\",\n",
    "    \"MAT32+(MAT52*PER)\"]\n",
    "\n",
    "possible_datasets = [\n",
    "   \"alternating\",\n",
    "   \"broaden\",\n",
    "   \"large gap\",\n",
    "   \"linear outlier\",\n",
    "   \"parabola\",\n",
    "   \"periodic linear\",\n",
    "   \"periodic\",\n",
    "    \"periodic_1D flattened\",\n",
    "   \"v_lines\"\n",
    "]\n",
    "\n",
    "kernel_name = \"SE+SE\"\n",
    "dataset_name = \"periodic_1D flattened\"\n",
    "dataset_addendum = \"\"#\"51 points\"\n",
    "logarithmic_reinit=True\n",
    "data_normalization = True\n",
    "data_norm_y = False \n",
    "levels = [1e+4, 1e+5]\n",
    "uninformed = True\n",
    "\n",
    "log_path = f\"logs/{'x-normalized' if data_normalization else ''}_{dataset_name}_{dataset_addendum}/{kernel_name}\"\n",
    "if not os.path.exists(log_path):\n",
    "    os.makedirs(log_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "if dataset_name in datasets_1d:\n",
    "    df = pd.read_csv(datasets_1d[dataset_name].path, header=None)\n",
    "    train_x = torch.tensor(df[0], dtype=torch.float32)\n",
    "    train_y = torch.tensor(df[1], dtype=torch.float32)\n",
    "\n",
    "elif dataset_name == \"linear\":\n",
    "    train_x = torch.linspace(0, int(dataset_addendum.split(\" \")[0])-1, int(dataset_addendum.split(\" \")[0]))\n",
    "    train_y = train_x.clone()\n",
    "    #train_y[-1] = 0 \n",
    "\n",
    "\n",
    "# Z score normalization\n",
    "if data_normalization:\n",
    "    if not dataset_name == \"periodic_1D flattened\":\n",
    "        train_x = (train_x - train_x.mean()) / train_x.std()\n",
    "    if data_norm_y:\n",
    "        train_y = (train_y - train_y.mean()) / train_y.std()\n",
    "\n",
    "#train_x = torch.tensor([0.0, 1.0])\n",
    "#train_y = torch.tensor([0.0, 1.0])\n",
    "\n",
    "\n",
    "fig, ax = plot_data(train_x, train_y, title_add=dataset_name, return_figure=True)\n",
    "fig.savefig(f\"{log_path}/data.png\", bbox_inches='tight')\n",
    "#fig.savefig(f\"{log_path}/data.pgf\", bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_dim_data = (train_x, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(min(train_y))\n",
    "print(max(train_y))\n",
    "print(min(train_x))\n",
    "print(max(train_x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define the GP model\n",
    "likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "model = ExactGPModel(train_x, train_y, likelihood, kernel_text=kernel_name)\n",
    "likelihood_MAP = gpytorch.likelihoods.GaussianLikelihood()\n",
    "model_MAP = ExactGPModel(train_x, train_y, likelihood_MAP, kernel_text=kernel_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_names = [l[0] for l in list(model.named_parameters())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_restarts = 4\n",
    "\n",
    "## Train the GPs\n",
    "model.train()\n",
    "likelihood.train()\n",
    "neg_scaled_mll_loss, model, likelihood, mll_train_log = optimize_hyperparameters(model, likelihood, X=train_x, Y=train_y, MAP=False, uninformed=uninformed, random_restarts=random_restarts)\n",
    "mll_opt_params = [p.item() for p in model.parameters() if p.requires_grad]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Give me the index of the best training run\n",
    "best_mll_training_run = np.argmin([min(run) for run in mll_train_log.neg_loss])\n",
    "best_mll_training_run"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# Likelihood values\n",
    "mll_train_log.neg_loss\n",
    "# Parameters\n",
    "mll_train_log.x\n",
    "# Hessians\n",
    "mll_train_log.hessians"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the optimization path of the trainings, each in an individual plot, given the parameter of interest\n",
    "# The parameter progression is plotted as arrows pointing to the next parameter position\n",
    "# The arrows are colored according to the lateness of their progression\n",
    "# The colorbar shows the progression of the parameter\n",
    "\n",
    "\n",
    "\n",
    "f, axs = plt.subplots(1, (random_restarts), figsize=(4*(random_restarts), 4), sharey=True, layout=\"constrained\")\n",
    "if random_restarts == 1:\n",
    "    axs = [axs]\n",
    "\n",
    "xdim = 0\n",
    "ydim = 1\n",
    "# extract all the parameters\n",
    "## Likelihood values\n",
    "\n",
    "#mll_train_log.neg_loss\n",
    "## Parameters\n",
    "#mll_train_log.x\n",
    "parameter_paths = [[log for log in mll_train_log.x[i]] for i in range(len(mll_train_log.x))]\n",
    "parameter_values = [[log for log in mll_train_log.neg_loss[i]] for i in range(len(mll_train_log.neg_loss))]\n",
    "\n",
    "for i, ax in enumerate(axs):\n",
    "    # train log consists of a list of tuples, where the first element is the parameter vector and the second element is the loss\n",
    "    plot_parameter_progression(parameter_paths[i], losses=parameter_values[i], xlabel=param_names[xdim], ylabel=param_names[ydim], xdim=xdim, ydim=ydim, fig=f, ax=ax, display_figure=False, title_add=f\"restart {i}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_MAP.train()\n",
    "likelihood_MAP.train()\n",
    "neg_scaled_map_loss, model_MAP, likelihood_MAP, map_train_log = optimize_hyperparameters(model_MAP, likelihood_MAP, X=train_x, Y=train_y, MAP=True, uninformed=uninformed, random_restarts=random_restarts)\n",
    "map_opt_params = [p.item() for p in model_MAP.parameters() if p.requires_grad]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print([torch.nn.functional.softplus(p) for p in model_MAP.parameters() if p.requires_grad]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Give me the index of the best training run\n",
    "best_map_training_run = np.argmin([min(run) for run in map_train_log.neg_loss])\n",
    "best_map_training_run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring the PyGRANSO Hessian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_train_log.hessians[best_map_training_run]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.linalg.det(map_train_log.hessians[best_map_training_run][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mll_train_log.hessians[best_mll_training_run][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "f, axs = plt.subplots(1, (random_restarts), figsize=(4*(random_restarts), 4), sharey=True, layout=\"constrained\")\n",
    "if random_restarts == 1:\n",
    "    axs = [axs]\n",
    "\n",
    "# extract all the parameters\n",
    "parameter_paths = [[log for log in map_train_log.x[i]] for i in range(len(map_train_log.x))]\n",
    "parameter_values = [[log for log in map_train_log.neg_loss[i]] for i in range(len(map_train_log.neg_loss))]\n",
    "\n",
    "xdim = 0\n",
    "ydim = 1\n",
    "\n",
    "for i, ax in enumerate(axs):\n",
    "    plot_parameter_progression(parameter_paths[i], losses=parameter_values[i], xlabel=param_names[xdim], ylabel=param_names[ydim], xdim=xdim, ydim=ydim, fig=f, ax=ax, display_figure=False, title_add=f\"restart {i}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_dim_train_log = map_train_log "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One dimensional case:\n",
    "#Final best parameters: [('likelihood.noise_covar.raw_noise', Parameter containing:\n",
    "#tensor([-11.6140], requires_grad=True)), ('covar_module.kernels.0.raw_lengthscale', Parameter containing:\n",
    "#tensor([[-0.3335]], requires_grad=True)), ('covar_module.kernels.1.raw_lengthscale', Parameter containing:\n",
    "#tensor([[-0.3335]], requires_grad=True))] w. loss: 0.6454170942306519 (smaller=better)\n",
    "# Two dimensional case:\n",
    "#Final best parameters: [('likelihood.noise_covar.raw_noise', Parameter containing:\n",
    "#tensor([-11.7261], requires_grad=True)), ('covar_module.base_kernel.kernels.0.raw_lengthscale', Parameter containing:\n",
    "#tensor([[-0.3326]], requires_grad=True)), ('covar_module.base_kernel.kernels.1.raw_lengthscale', Parameter containing:\n",
    "#tensor([[-0.3326]], requires_grad=True))] w. loss: 0.6454619765281677 (smaller=better)\n",
    "\n",
    "#fixed_reinit(model_MAP, torch.tensor([-11.7261, -0.3326, -0.3326]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_dim_mean_vec = model_MAP(train_x).mean\n",
    "one_dim_cov_matr = model_MAP(train_x).covariance_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "likelihood.eval()\n",
    "model_MAP.eval()\n",
    "likelihood_MAP.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_dim_post_mean_vec = model_MAP(train_x).mean\n",
    "one_dim_post_cov_matr = model_MAP(train_x).covariance_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "likelihood.eval()\n",
    "model_MAP.eval()\n",
    "likelihood_MAP.eval()\n",
    "fig, axs = plt.subplots(1, 2, figsize=(16, 6), sharey=True)\n",
    "ax = axs[0]\n",
    "ax_MAP = axs[1]\n",
    "plot_model(model, likelihood, train_x, train_y, return_figure=False, figure=fig, ax=ax, loss_val=neg_scaled_mll_loss.item(), loss_type=\"mll\", display_figure=False)\n",
    "plot_model(model_MAP, likelihood_MAP, train_x, train_y, return_figure=False, figure=fig, ax=ax_MAP, loss_val=neg_scaled_map_loss.item(), loss_type=\"map\")\n",
    "fig.savefig(f\"{log_path}/posterior.png\", bbox_inches='tight')\n",
    "#fig.savefig(f\"{log_path}/posterior.pgf\", bbox_inches='tight')\n",
    "model.train()\n",
    "likelihood.train()\n",
    "model_MAP.train()\n",
    "likelihood_MAP.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_scaled_map_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()\n",
    "likelihood.train()\n",
    "model_MAP.train()\n",
    "likelihood_MAP.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_unscaled_MLL = -neg_scaled_mll_loss*len(*model.train_inputs)\n",
    "pos_unscaled_MAP = -neg_scaled_map_loss*len(*model.train_inputs)\n",
    "num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "bool_use_finite_difference_hessian = True\n",
    "\n",
    "## Calculate the metrics\n",
    "AIC_val, AIC_logs = AIC(pos_unscaled_MLL, num_params)\n",
    "BIC_val, BIC_logs = BIC(pos_unscaled_MLL, num_params, torch.tensor(len(train_x)))\n",
    "AIC_MAP_val, AIC_MAP_logs = AIC(pos_unscaled_MAP, num_params)\n",
    "BIC_MAP_val, BIC_MAP_logs = BIC(pos_unscaled_MAP, num_params, torch.tensor(len(train_x)))\n",
    "Lap0_val, Lap0_logs =     Laplace(model_MAP, pos_unscaled_MAP.clone(), uninformed=uninformed, use_finite_difference_hessian=bool_use_finite_difference_hessian, param_punish_term=0)\n",
    "LapAIC_val, LapAIC_logs = Laplace(model_MAP, pos_unscaled_MAP.clone(), uninformed=uninformed, use_finite_difference_hessian=bool_use_finite_difference_hessian, param_punish_term=-1)\n",
    "LapBIC_val, LapBIC_logs = Laplace(model_MAP, pos_unscaled_MAP.clone(), uninformed=uninformed, use_finite_difference_hessian=bool_use_finite_difference_hessian, param_punish_term=\"BIC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Lap0_logs[\"original symmetrized Hessian\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_step = 5e-1\n",
    "h_i_step = 0.5*h_step \n",
    "h_j_step = 0.5*h_step\n",
    "\n",
    "hessian_finite_differences_neg_unscaled_map = np.zeros((num_params, num_params))\n",
    "for h_step in [1e-1, 5e-1, 1e-2, 5e-2, 1e-3, 5e-3]:\n",
    "    print(h_step)\n",
    "    h_i_step = 0.5*h_step \n",
    "    h_j_step = 0.5*h_step\n",
    "    for i, j in itertools.product(range(num_params), range(num_params)):\n",
    "        h_i_vec = np.zeros(num_params)\n",
    "        h_j_vec = np.zeros(num_params)\n",
    "        h_i_vec[i] = 1.0\n",
    "        h_j_vec[j] = 1.0\n",
    "        hessian_finite_differences_neg_unscaled_map[i][j] = finite_difference_second_derivative_GP_neg_unscaled_map(model_MAP, likelihood_MAP, train_x, train_y, uninformed=uninformed, h_i_step=h_i_step, h_j_step=h_j_step, h_i_vec=h_i_vec, h_j_vec=h_j_vec)\n",
    "    print(hessian_finite_differences_neg_unscaled_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the hessian of the unscaled MLL\n",
    "params_list = [p for p in model.parameters()]\n",
    "neg_unscaled_map = -pos_unscaled_MAP\n",
    "try:\n",
    "    jacobian_neg_unscaled_map = torch.autograd.grad(neg_unscaled_map, params_list, retain_graph=True, create_graph=True, allow_unused=True)\n",
    "except Exception as E:\n",
    "    print(E)\n",
    "    import pdb\n",
    "    pdb.set_trace()\n",
    "    print(f\"E:{E}\")\n",
    "hessian_neg_unscaled_map = []\n",
    "# Calcuate -\\nabla\\nabla log(f(\\theta)) (i.e. Hessian of negative log marginal likelihood)\n",
    "for i in range(len(jacobian_neg_unscaled_mll)):\n",
    "    hessian_neg_unscaled_map.append(torch.autograd.grad(jacobian_neg_unscaled_map[i], params_list, retain_graph=True, allow_unused=True)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hessian_neg_unscaled_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Lap0_logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LapAIC_logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LapBIC_logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_evidences = list()\n",
    "model_evidence_logs = list()\n",
    "uninformed = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxcall = 1e+1\n",
    "if maxcall in levels:\n",
    "    print(maxcall)\n",
    "    nested_pos_model_evidence_e1, Nested_logs_e1 = Nested(model, store_full=True, pickle_directory=os.path.join(log_path, \"Nested_log\"), maxcall=maxcall)\n",
    "    model_evidences.append(nested_pos_model_evidence_e1)\n",
    "    model_evidence_logs.append(Nested_logs_e1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxcall = 1e+2\n",
    "if maxcall in levels:\n",
    "    print(maxcall)\n",
    "    nested_pos_model_evidence_e2, Nested_logs_e2 = Nested(model, store_full=True, pickle_directory=os.path.join(log_path, \"Nested_log\"), maxcall=maxcall)\n",
    "    model_evidences.append(nested_pos_model_evidence_e2)\n",
    "    model_evidence_logs.append(Nested_logs_e2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxcall = 1e+3\n",
    "if maxcall in levels:\n",
    "    print(maxcall)\n",
    "    nested_pos_model_evidence_e3, Nested_logs_e3 = Nested(model, store_full=True, pickle_directory=os.path.join(log_path, \"Nested_log\"), maxcall=maxcall, uninformed=uninformed)\n",
    "    model_evidences.append(nested_pos_model_evidence_e3)\n",
    "    model_evidence_logs.append(Nested_logs_e3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxcall = 1e+4\n",
    "if maxcall in levels:\n",
    "    print(maxcall)\n",
    "    nested_pos_model_evidence_e4, Nested_logs_e4 = Nested(model, store_full=True, pickle_directory=os.path.join(log_path, \"Nested_log\"), maxcall=maxcall, uninformed=uninformed)\n",
    "    model_evidences.append(nested_pos_model_evidence_e4)\n",
    "    model_evidence_logs.append(Nested_logs_e4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxcall = 1e+5\n",
    "if maxcall in levels:\n",
    "    print(maxcall)\n",
    "    nested_pos_model_evidence_e5, Nested_logs_e5 = Nested(model, store_full=True, pickle_directory=os.path.join(log_path, \"Nested_log\"), maxcall=maxcall, uninformed=uninformed)\n",
    "    model_evidences.append(nested_pos_model_evidence_e5)\n",
    "    model_evidence_logs.append(Nested_logs_e5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for mod_log in model_evidence_logs:\n",
    "    with open(f\"{mod_log['res file']}\", \"rb\") as f:\n",
    "        res = dill.load(f)\n",
    "    print(f\"Num calls: {sum(res.ncall)}\")\n",
    "    print(f\"logz Error: {res[\"logzerr\"][-1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plot_mll_opt = True\n",
    "std_filter = None # float as 1e-3 or None\n",
    "show_last_num = 0.5 # integer, float or None\n",
    "filter_type = \"none\" # (\"mean\"), \"max\", \"none\"\n",
    "xdim = 1\n",
    "ydim = 2\n",
    "\n",
    "param_names = [l[0] for l in list(model.named_parameters())]\n",
    "\n",
    "f, axs = plt.subplots(1, len(levels), figsize=(4*len(levels), 4), sharey=True, layout=\"constrained\")\n",
    "if len(levels) == 1:\n",
    "    axs = [axs]\n",
    "for ax, level, model_evidence_log in zip(axs, levels, model_evidence_logs):\n",
    "    if ax == axs[-1]:\n",
    "        nested_sampling_plot(model, model_evidence_log, xdim = xdim, ydim = ydim, filter_type=filter_type, std_filter=std_filter, show_last_num=show_last_num, return_figure=False, title_add=\"\", fig=f, ax=ax, display_figure=False, plot_mll_opt=plot_mll_opt, mll_opt_params=mll_opt_params, plot_lap=True, Lap0_logs=Lap0_logs, LapAIC_logs=LapAIC_logs, LapBIC_logs=LapBIC_logs, lap_colors = [\"r\", \"pink\", \"white\"], Lap_hess=Lap0_logs[\"original symmetrized Hessian\"])\n",
    "    else:\n",
    "        nested_sampling_plot(model, model_evidence_log, xdim = xdim, ydim = ydim, filter_type=filter_type, std_filter=std_filter, show_last_num=show_last_num, return_figure=False, title_add=\"\", fig=f, ax=ax, display_figure=False, plot_mll_opt=plot_mll_opt, mll_opt_params=mll_opt_params, plot_lap=True, Lap0_logs=Lap0_logs, LapAIC_logs=LapAIC_logs, LapBIC_logs=LapBIC_logs, lap_colors = [\"r\", \"pink\", \"white\"], Lap_hess=Lap0_logs[\"original symmetrized Hessian\"])\n",
    "    if ax == axs[0] or len(levels)==1:\n",
    "        ax.set_ylabel(param_names[ydim])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "map_opt_params = [p.item() for p in model_MAP.parameters() if p.requires_grad]\n",
    "plot_map_opt = False\n",
    "std_filter = None # float as 1e-3 or None\n",
    "show_last_num = 0.4 # integer, float or None\n",
    "filter_type = \"none\" # (\"mean\"), \"max\", \"none\"\n",
    "xdim = 0\n",
    "ydim = 1\n",
    "\n",
    "param_names = [l[0] for l in list(model.named_parameters())]\n",
    "\n",
    "f, axs = plt.subplots(1, len(levels), figsize=(4*len(levels), 4), sharey=True, layout=\"constrained\")\n",
    "if len(levels) == 1:\n",
    "    axs = [axs]\n",
    "for ax, level, model_evidence_log in zip(axs, levels, model_evidence_logs):\n",
    "    if ax == axs[-1]:\n",
    "        posterior_surface_plot(model, model_evidence_log, xdim = xdim, ydim = ydim, filter_type=filter_type, std_filter=std_filter, show_last_num=show_last_num, return_figure=False, title_add=\"\", fig=f, ax=ax, display_figure=False, plot_mll_opt=plot_map_opt, mll_opt_params=map_opt_params, plot_lap=True, Lap0_logs=Lap0_logs, LapAIC_logs=LapAIC_logs, LapBIC_logs=LapBIC_logs, lap_colors = [\"r\", \"pink\", \"white\"], uninformed=uninformed, Lap_hess=Lap0_logs[\"original symmetrized Hessian\"])\n",
    "    else:\n",
    "        posterior_surface_plot(model, model_evidence_log, xdim = xdim, ydim = ydim, filter_type=filter_type, std_filter=std_filter, show_last_num=show_last_num, return_figure=False, title_add=\"\", fig=f, ax=ax, display_figure=False, plot_mll_opt=plot_map_opt, mll_opt_params=map_opt_params, plot_lap=True, Lap0_logs=Lap0_logs, LapAIC_logs=LapAIC_logs, LapBIC_logs=LapBIC_logs, lap_colors = [\"r\", \"pink\", \"white\"], uninformed=uninformed, Lap_hess=Lap0_logs[\"original symmetrized Hessian\"])\n",
    "    if ax == axs[0] or len(levels)==1:\n",
    "        ax.set_ylabel(param_names[ydim])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the best value and the corresponding hyperparameters\n",
    "best_idx = np.argmax(res.logl)\n",
    "best_hyperparameters = res.samples[best_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimal GP according to Nested sampling\n",
    "fixed_reinit(model, torch.tensor(best_hyperparameters))\n",
    "model.eval()\n",
    "likelihood.eval()\n",
    "# mll of best parameterization according to Nested sampling:\n",
    "mll_fkt = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "unscaled_pos_nested_top_mll = (mll_fkt(model(train_x), train_y))*len(train_x)\n",
    "unscaled_pos_nested_top_map = unscaled_pos_nested_top_mll+log_normalized_prior(model)*len(train_x)\n",
    "fig, ax = plot_model(model, likelihood, train_x, train_y, return_figure=True, loss_val=res.logl[best_idx], loss_type=\"Nested logl\")\n",
    "ax.title.set_text(f\"{ax.title.get_text()} ; MLL: {unscaled_pos_nested_top_mll.item():.2f}; MAP: {unscaled_pos_nested_top_map.item():.2f}\\n MLL (u) {-unscaled_pos_nested_top_mll/len(train_x):.2f}; MAP (u) {-unscaled_pos_nested_top_map/len(train_x):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect metric names and values into a list of tuples\n",
    "metrics_data = [\n",
    "    (\"MLL\", f\"{pos_unscaled_MLL.item():.3f}\"),\n",
    "    (\"MLL (loss)\", f\"{neg_scaled_mll_loss.item():.3f}\"),\n",
    "    (\"AIC\", f\"{AIC_val.item():.3f}\"),\n",
    "    (\"AIC (s)\", f\"{(AIC_val.item()*(-0.5)):.3f}\"),\n",
    "    (\"BIC\", f\"{BIC_val.item():.3f}\"),\n",
    "    (\"BIC (s)\", f\"{(BIC_val.item()*(-0.5)):.3f}\"),\n",
    "    \n",
    "\n",
    "]\n",
    "labels = [m[0] for m in metrics_data]\n",
    "values = [m[1] for m in metrics_data]\n",
    "# Print in transposed form\n",
    "print(\"|        | \" + \" | \".join(labels) + \" |\")\n",
    "print(\"|--------|\" + \"|\".join([\"-----------\" for _ in labels]) + \"|\")\n",
    "print(\"| Value  | \" + \" | \".join(values) + \" |\")\n",
    "print()\n",
    "\n",
    "metrics_data = [(\"MAP\", f\"{pos_unscaled_MAP.item():.3f}\"),\n",
    "    (\"MAP (loss)\", f\"{neg_scaled_map_loss.item():.3f}\"),\n",
    "    (\"AIC_M\", f\"{AIC_MAP_val.item():.3f}\"),\n",
    "    (\"AIC_M (s)\", f\"{(AIC_MAP_val.item()*(-0.5)):.3f}\"),\n",
    "    (\"BIC_M\", f\"{BIC_MAP_val.item():.3f}\"),\n",
    "    (\"BIC_M (s)\", f\"{(BIC_MAP_val.item()*(-0.5)):.3f}\"),\n",
    "]\n",
    "\n",
    "labels = [m[0] for m in metrics_data]\n",
    "values = [m[1] for m in metrics_data]\n",
    "# Print in transposed form\n",
    "print(\"|        | \" + \" | \".join(labels) + \" |\")\n",
    "print(\"|--------|\" + \"|\".join([\"-----------\" for _ in labels]) + \"|\")\n",
    "print(\"| Value  | \" + \" | \".join(values) + \" |\")\n",
    "print()\n",
    "\n",
    "\n",
    "metrics_data=[    \n",
    "    (\"Lap\", f\"{Lap0_logs[\"laplace without replacement\"].item():.3f}\"),\n",
    "    (\"Lap0\", f\"{Lap0_val.item():.3f}\"),\n",
    "    (\"LapAIC\", f\"{LapAIC_val.item():.3f}\"),\n",
    "    (\"LapBIC\", f\"{LapBIC_val.item():.3f}\"),\n",
    "    ]\n",
    "\n",
    "\n",
    "labels = [m[0] for m in metrics_data]\n",
    "values = [m[1] for m in metrics_data]\n",
    "# Print in transposed form\n",
    "print(\"|        | \" + \" | \".join(labels) + \" |\")\n",
    "print(\"|--------|\" + \"|\".join([\"-----------\" for _ in labels]) + \"|\")\n",
    "print(\"| Value  | \" + \" | \".join(values) + \" |\")\n",
    "print()\n",
    "metrics_data = [    \n",
    "    #(\"Nested (1)\", f\"{Nested_val_e1:.3f}\"),\n",
    "    #(\"Nested (3)\", f\"{Nested_val_e3:.3f}\"),\n",
    "    (\"Nested (4)\", f\"{nested_pos_model_evidence_e4:.3f}\"),\n",
    "    (\"Nested (5)\", f\"{nested_pos_model_evidence_e5:.3f}\")\n",
    "    ]\n",
    "# Transpose the table: one row for the label, another for the value\n",
    "labels = [m[0] for m in metrics_data]\n",
    "values = [m[1] for m in metrics_data]\n",
    "\n",
    "# Print in transposed form\n",
    "print(\"|        | \" + \" | \".join(labels) + \" |\")\n",
    "print(\"|--------|\" + \"|\".join([\"-----------\" for _ in labels]) + \"|\")\n",
    "print(\"| Value  | \" + \" | \".join(values) + \" |\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fixed_reinit(model_MAP, torch.tensor(best_hyperparameters))\n",
    "mll_fkt = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood_MAP, model_MAP)\n",
    "neg_scaled_map_loss = -mll_fkt(model_MAP(train_x), train_y)-log_normalized_prior(model_MAP)\n",
    "pos_unscaled_MAP = -neg_scaled_map_loss*len(*model.train_inputs)\n",
    "num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "Nested_Lap0_val, Nested_Lap0_logs = Laplace(model_MAP, pos_unscaled_MAP, param_punish_term=0)\n",
    "\n",
    "print(f\"Lap/nested = {Lap0_logs[\"laplace without replacement\"].item()/nested_pos_model_evidence_e5:.3f}\"),\n",
    "print(f\"Lap0/nested = {Lap0_val/nested_pos_model_evidence_e5:.3f}\")\n",
    "print(f\"AIC (s)/nested = {AIC_val*(-0.5)/nested_pos_model_evidence_e5:.3f}\") \n",
    "print(f\"nested Lap0/nested = {Nested_Lap0_val/nested_pos_model_evidence_e5:.3f}\\n\")\n",
    "print(\"```python\")\n",
    "print(Nested_Lap0_val)\n",
    "\n",
    "print(Nested_Lap0_logs)\n",
    "print(\"```\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2D data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "possible_datasets = {\n",
    "    \"periodic_1D\" : periodic_1D,\n",
    "    \"periodic_2D\" : periodic_2D,\n",
    "    \"parabola_1D\" : parabola_1D,\n",
    "    \"parabola_2D\" : parabola_2D,\n",
    "    \"product\" : product,\n",
    "    \"periodic_sum\": periodic_sum,\n",
    "    \"periodic_sincos\": periodic_sincos,\n",
    "    \"linear_1D\": linear_1D,\n",
    "    \"linear_2D\": linear_2D,\n",
    "}\n",
    "\n",
    "possible_X_patterns = [\n",
    "    \"away\",\n",
    "    \"bullseye\",\n",
    "    \"circle\",\n",
    "    \"dots\",\n",
    "    \"dots minimal\",\n",
    "    \"dots minimal no shift\",\n",
    "    \"dots minimal shifted\",\n",
    "    \"dots minimal shifted around zero\",\n",
    "    \"dots minimal shifted to zero\",\n",
    "    \"dots favouring lower values\",\n",
    "    \"line\",\n",
    "    \"line X\",\n",
    "    \"line 05X1Y\",\n",
    "    \"h_lines\",\n",
    "    \"high lines\",\n",
    "    \"slant down\",\n",
    "    \"slant up\",\n",
    "    \"star\",\n",
    "    \"v_lines\",\n",
    "    \"wide lines\",\n",
    "    \"x_shape\",\n",
    "]\n",
    "\n",
    "possible_kenels = [\n",
    "        \"[SE; SE]\", #0\n",
    "        \"[SE; LIN]\",\n",
    "        \"[LIN; SE]\",\n",
    "        \"[LIN; LIN]\", #3\n",
    "        \"[LIN;* LIN]\",\n",
    "        \"[SE;* 1]\", #5\n",
    "        \"[PER;* 1]\", #6\n",
    "        \"[SE+SE; 1]\", #7 #BOO. Fuck AdditiveStructureKernels. All my Homies hate AdditiveStructureKernels\n",
    "        \"[SE+SE; 1]2\" #8\n",
    "    ]\n",
    "kernel_name = possible_kenels[1]\n",
    "dataset_name = \"periodic_1D\"\n",
    "pattern = \"line X\"\n",
    "\n",
    "dataset_addendum = \"\"#\"51 points\"\n",
    "logarithmic_reinit=True\n",
    "data_normalization = True\n",
    "data_norm_y = True \n",
    "X1_noise = 0.0\n",
    "X2_noise = 0.0\n",
    "levels = [1e+4, 1e+5]\n",
    "uninformed = True\n",
    "\n",
    "\n",
    "log_path = f\"logs/2D/{'x-normalized' if data_normalization else ''}_{dataset_name}-{pattern}_{dataset_addendum}/{kernel_name}\"\n",
    "if not os.path.exists(log_path):\n",
    "    os.makedirs(log_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load the dataset\n",
    "if pattern in datasets_2d:\n",
    "    df = pd.read_csv(datasets_2d[pattern].path, header=None)\n",
    "    x1 = torch.tensor(df[0], dtype=torch.float32) + X1_noise*torch.randn_like(torch.tensor(df[0], dtype=torch.float32))\n",
    "    x2 = torch.tensor(df[1], dtype=torch.float32) + X2_noise*torch.randn_like(torch.tensor(df[1], dtype=torch.float32))\n",
    "    train_x = torch.stack([x1, x2], dim=-1)\n",
    "    print(train_x)\n",
    "\n",
    "# Generate the z axis\n",
    "train_y = possible_datasets[dataset_name](train_x)\n",
    "# Z score normalization\n",
    "if data_normalization:\n",
    "    print(f\"X-scale: {train_x.std()}\")\n",
    "    print(f\"Y-scale: {train_y.std()}\")\n",
    "    train_x = (train_x - train_x.mean()) / train_x.std()\n",
    "    if data_norm_y:\n",
    "        train_y = (train_y - train_y.mean()) / train_y.std()\n",
    "\n",
    "    print(f\"New distribution\\n X: {train_x.mean()} {train_x.std()}\\n Y: {train_y.mean()} {train_y.std()}\")\n",
    "\n",
    "#train_x = torch.tensor([[0.0, 0.0], [1.0, 0.0]])\n",
    "#train_y = torch.tensor([0.0, 1.0])\n",
    "\n",
    "fig, ax = plot_3d_data(train_y, train_x[:,0], train_x[:,1], title_add=f\"{dataset_name}-{pattern}\", return_figure=True)\n",
    "fig.savefig(f\"{log_path}/data.png\", bbox_inches='tight')\n",
    "#fig.savefig(f\"{log_path}/data.pgf\", bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "two_dim_data = (train_x, train_y)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# Transpose the data set on the X/Y plane and store it as \"wide_lines.csv\"\n",
    "df = pandas.DataFrame({\"x1\": train_x[:,1], \"x2\": train_x[:,0], \"y\": train_y})\n",
    "df.to_csv(\"datasets/2D/wide_lines.csv\", header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define the GP model\n",
    "likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "model = ExactMIGPModel(train_x, train_y, likelihood, kernel_text=kernel_name)\n",
    "likelihood_MAP = gpytorch.likelihoods.GaussianLikelihood()\n",
    "model_MAP = ExactMIGPModel(train_x, train_y, likelihood_MAP, kernel_text=kernel_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_names = [l[0] for l in list(model.named_parameters())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_restarts = 4\n",
    "## Train the GPs\n",
    "model.train()\n",
    "likelihood.train()\n",
    "neg_scaled_mll_loss, model, likelihood, mll_train_log = optimize_hyperparameters(model, likelihood, X=train_x, Y=train_y, MAP=False, uninformed=uninformed, random_restarts=random_restarts, logarithmic_reinit=logarithmic_reinit)\n",
    "mll_opt_params = [p.item() for p in model.parameters() if p.requires_grad]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the optimization path of the trainings, each in an individual plot, given the parameter of interest\n",
    "# The parameter progression is plotted as arrows pointing to the next parameter position\n",
    "# The arrows are colored according to the lateness of their progression\n",
    "# The colorbar shows the progression of the parameter\n",
    "\n",
    "\n",
    "\n",
    "f, axs = plt.subplots(1, (random_restarts), figsize=(4*(random_restarts), 4), sharey=True, layout=\"constrained\")\n",
    "if random_restarts == 1:\n",
    "    axs = [axs]\n",
    "\n",
    "xdim = 0\n",
    "ydim = 1\n",
    "# extract all the parameters\n",
    "parameter_paths = [[log for log in mll_train_log.x[i]] for i in range(len(mll_train_log.x))]\n",
    "parameter_values = [[log for log in mll_train_log.neg_loss[i]] for i in range(len(mll_train_log.neg_loss))]\n",
    "\n",
    "for i, ax in enumerate(axs):\n",
    "    if len(parameter_values[i]) == 1:\n",
    "        continue\n",
    "    # train log consists of a list of tuples, where the first element is the parameter vector and the second element is the loss\n",
    "    plot_parameter_progression(parameter_paths[i], losses=parameter_values[i], xlabel=param_names[xdim], ylabel=param_names[ydim], xdim=xdim, ydim=ydim, fig=f, ax=ax, display_figure=False, title_add=f\"restart {i}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_MAP.train()\n",
    "likelihood_MAP.train()\n",
    "neg_scaled_map_loss, model_MAP, likelihood_MAP, map_train_log = optimize_hyperparameters(model_MAP, likelihood_MAP, X=train_x, Y=train_y, MAP=True, uninformed=uninformed, random_restarts=random_restarts, logarithmic_reinit=logarithmic_reinit)\n",
    "map_opt_params = [p.item() for p in model_MAP.parameters() if p.requires_grad]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Give me the index of the best training run\n",
    "best_map_training_run = np.argmin([min(run) for run in map_train_log.neg_loss])\n",
    "best_map_training_run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_train_log.hessians[best_map_training_run]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(model_MAP.parameters()))\n",
    "neg_scaled_map_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print([torch.nn.functional.softplus(p) for p in model_MAP.parameters() if p.requires_grad]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "f, axs = plt.subplots(1, (random_restarts), figsize=(4*(random_restarts), 4), sharey=True, layout=\"constrained\")\n",
    "if random_restarts == 1:\n",
    "    axs = [axs]\n",
    "\n",
    "# extract all the parameters\n",
    "parameter_paths = [[log for log in map_train_log.x[i]] for i in range(len(map_train_log.x))]\n",
    "parameter_values = [[log for log in map_train_log.neg_loss[i]] for i in range(len(map_train_log.neg_loss))]\n",
    "\n",
    "xdim = 0\n",
    "ydim = 1\n",
    "\n",
    "for i, ax in enumerate(axs):\n",
    "    if len(parameter_values[i]) == 1:\n",
    "        continue\n",
    "    plot_parameter_progression(parameter_paths[i], losses=parameter_values[i], xlabel=param_names[xdim], ylabel=param_names[ydim], xdim=xdim, ydim=ydim, fig=f, ax=ax, display_figure=False, title_add=f\"restart {i}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "two_dim_train_log = map_train_log "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "two_dim_mean_vec = model_MAP(train_x).mean\n",
    "two_dim_cov_matr = model_MAP(train_x).covariance_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "likelihood.eval()\n",
    "model_MAP.eval()\n",
    "likelihood_MAP.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "two_dim_post_mean_vec = model_MAP(train_x).mean\n",
    "two_dim_post_cov_matr = model_MAP(train_x).covariance_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "likelihood.eval()\n",
    "model_MAP.eval()\n",
    "likelihood_MAP.eval()\n",
    "fig = plt.figure(figsize=(16, 9))\n",
    "ax = fig.add_subplot(1, 2, 1, projection='3d')\n",
    "ax_MAP = fig.add_subplot(1, 2, 2, projection='3d')\n",
    "#ax = axs[0]\n",
    "#ax_MAP = axs[1]\n",
    "x_min=min(train_x[:,0])\n",
    "x_max=max(train_x[:,0])\n",
    "y_min=min(train_x[:,1])\n",
    "y_max=max(train_x[:,1])\n",
    "\n",
    "title_add = \"\"\n",
    "if y_min == y_max:\n",
    "    y_min = y_min-0.1\n",
    "    y_max = y_max+0.1\n",
    "    title_add += \" Synthetic y stretch\"\n",
    "if x_min == x_max:\n",
    "    x_min = x_min-0.1\n",
    "    x_max = x_max+0.1\n",
    "    title_add += \" Synthetic x stretch\"\n",
    "\n",
    "plot_3d_gp(model, likelihood, data=torch.stack([train_x[:,0], train_x[:,1], train_y], -1), x_min=x_min, x_max=x_max, y_min=y_min, y_max=y_max, return_figure=False, fig=fig, ax=ax, loss_val=np.round(neg_scaled_mll_loss.item(), 2), loss_type=\"mll\", display_figure=False, shadow=False, title_add=title_add)\n",
    "plot_3d_gp(model_MAP, likelihood_MAP, data=torch.stack([train_x[:,0], train_x[:,1], train_y], -1), x_min=x_min, x_max=x_max, y_min=y_min, y_max=y_max,return_figure=False, fig=fig, ax=ax_MAP, loss_val=np.round(neg_scaled_map_loss.item(), 2), loss_type=\"map\", shadow=False, title_add=title_add)\n",
    "fig.savefig(f\"{log_path}/posterior.png\", bbox_inches='tight')\n",
    "#fig.savefig(f\"{log_path}/posterior.pgf\", bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()\n",
    "likelihood.train()\n",
    "model_MAP.train()\n",
    "likelihood_MAP.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_unscaled_MLL = -neg_scaled_mll_loss*len(*model.train_inputs)\n",
    "pos_unscaled_MAP = -neg_scaled_map_loss*len(*model.train_inputs)\n",
    "num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "bool_use_finite_difference_hessian = True\n",
    "\n",
    "## Calculate the metrics\n",
    "AIC_val, AIC_logs = AIC(pos_unscaled_MLL, num_params)\n",
    "BIC_val, BIC_logs = BIC(pos_unscaled_MLL, num_params, torch.tensor(len(train_x)))\n",
    "AIC_MAP_val, AIC_MAP_logs = AIC(pos_unscaled_MAP, num_params)\n",
    "BIC_MAP_val, BIC_MAP_logs = BIC(pos_unscaled_MAP, num_params, torch.tensor(len(train_x)))\n",
    "Lap0_val, Lap0_logs =     Laplace(model_MAP, pos_unscaled_MAP.clone(), uninformed=uninformed, use_finite_difference_hessian=bool_use_finite_difference_hessian, param_punish_term=0)\n",
    "LapAIC_val, LapAIC_logs = Laplace(model_MAP, pos_unscaled_MAP.clone(), uninformed=uninformed, use_finite_difference_hessian=bool_use_finite_difference_hessian, param_punish_term=-1)\n",
    "LapBIC_val, LapBIC_logs = Laplace(model_MAP, pos_unscaled_MAP.clone(), uninformed=uninformed, use_finite_difference_hessian=bool_use_finite_difference_hessian, param_punish_term=\"BIC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Lap0_logs[\"original symmetrized Hessian\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Check all the individual likelihood evaluations and see how they perform. Is the amount of change plausible compared to the optimum?\n",
    "hessian_finite_differences_neg_unscaled_map = np.zeros((num_params, num_params))\n",
    "for i, j in itertools.product(range(num_params), range(num_params)):\n",
    "    h_i_vec = np.zeros(num_params)\n",
    "    h_j_vec = np.zeros(num_params)\n",
    "    h_i_vec[i] = 1.0\n",
    "    h_j_vec[j] = 1.0\n",
    "    hessian_finite_differences_neg_unscaled_map[i][j] = finite_difference_second_derivative_GP_neg_unscaled_map(model, likelihood, train_x, train_y, uninformed=uninformed, h_i_step=h_i_step, h_j_step=h_j_step, h_i_vec=h_i_vec, h_j_vec=h_j_vec)\n",
    "hessian_finite_differences_neg_unscaled_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finite_difference_hessian(model_MAP, likelihood_MAP, num_params, train_x, train_y, uninformed=uninformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.covar_module(torch.tensor([train_x[0][0], train_x[1][0]])).evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if np.isnan(Lap0_logs[\"laplace without replacement\"].item()):\n",
    "    # Load stop.ascii file and print it\n",
    "    with open(f\"stop.ascii\", \"r\") as f:\n",
    "        print(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Lap0_logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.linalg.eig(Lap0_logs[\"original symmetrized Hessian\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.linalg.eigh(Lap0_logs[\"original symmetrized Hessian\"]))\n",
    "print(\"---\")\n",
    "print(torch.linalg.det(Lap0_logs[\"original symmetrized Hessian\"]))\n",
    "print(torch.linalg.det(Lap0_logs[\"corrected Hessian\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LapAIC_logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LapBIC_logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_evidences = list()\n",
    "model_evidence_logs = list()\n",
    "uninformed = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxcall = 1e+1\n",
    "if maxcall in levels:\n",
    "    print(maxcall)\n",
    "    nested_pos_model_evidence_e1, Nested_logs_e1 = Nested(model, store_full=True, pickle_directory=os.path.join(log_path, \"Nested_log\"), maxcall=maxcall)\n",
    "    model_evidences.append(nested_pos_model_evidence_e1)\n",
    "    model_evidence_logs.append(Nested_logs_e1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxcall = 1e+2\n",
    "if maxcall in levels:\n",
    "    print(maxcall)\n",
    "    nested_pos_model_evidence_e2, Nested_logs_e2 = Nested(model, store_full=True, pickle_directory=os.path.join(log_path, \"Nested_log\"), maxcall=maxcall)\n",
    "    model_evidences.append(nested_pos_model_evidence_e2)\n",
    "    model_evidence_logs.append(Nested_logs_e2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxcall = 1e+3\n",
    "if maxcall in levels:\n",
    "    print(maxcall)\n",
    "    nested_pos_model_evidence_e3, Nested_logs_e3 = Nested(model, store_full=True, pickle_directory=os.path.join(log_path, \"Nested_log\"), maxcall=maxcall, uninformed=uninformed)\n",
    "    model_evidences.append(nested_pos_model_evidence_e3)\n",
    "    model_evidence_logs.append(Nested_logs_e3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxcall = 1e+4\n",
    "if maxcall in levels:\n",
    "    print(maxcall)\n",
    "    nested_pos_model_evidence_e4, Nested_logs_e4 = Nested(model, store_full=True, pickle_directory=os.path.join(log_path, \"Nested_log\"), maxcall=maxcall, uninformed=uninformed)\n",
    "    model_evidences.append(nested_pos_model_evidence_e4)\n",
    "    model_evidence_logs.append(Nested_logs_e4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxcall = 1e+5\n",
    "if maxcall in levels:\n",
    "    print(maxcall)\n",
    "    nested_pos_model_evidence_e5, Nested_logs_e5 = Nested(model, store_full=True, pickle_directory=os.path.join(log_path, \"Nested_log\"), maxcall=maxcall, uninformed=uninformed)\n",
    "    model_evidences.append(nested_pos_model_evidence_e5)\n",
    "    model_evidence_logs.append(Nested_logs_e5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for mod_log in model_evidence_logs:\n",
    "    with open(f\"{mod_log['res file']}\", \"rb\") as f:\n",
    "        res = dill.load(f)\n",
    "    print(f\"Num calls: {sum(res.ncall)}\")\n",
    "    print(f\"logz Error: {res[\"logzerr\"][-1]}\")\n",
    "    print(f\"Accepted samples: {len(res.samples)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plot_mll_opt = True\n",
    "std_filter = None # float as 1e-3 or None\n",
    "show_last_num = 0.1 # integer, float or None\n",
    "filter_type = \"none\" # (\"mean\"), \"max\", \"none\"\n",
    "xdim = 1\n",
    "ydim = 2\n",
    "\n",
    "param_names = [l[0] for l in list(model.named_parameters())]\n",
    "\n",
    "f, axs = plt.subplots(1, len(levels), figsize=(4*len(levels), 4), sharey=True, layout=\"constrained\")\n",
    "if len(levels) == 1:\n",
    "    axs = [axs]\n",
    "for ax, level, model_evidence_log in zip(axs, levels, model_evidence_logs):\n",
    "    if ax == axs[-1]:\n",
    "        nested_sampling_plot(model, model_evidence_log, xdim = xdim, ydim = ydim, filter_type=filter_type, std_filter=std_filter, show_last_num=show_last_num, return_figure=False, title_add=\"\", fig=f, ax=ax, display_figure=False, plot_mll_opt=plot_mll_opt, mll_opt_params=mll_opt_params, plot_lap=True, Lap0_logs=Lap0_logs, LapAIC_logs=LapAIC_logs, LapBIC_logs=LapBIC_logs, lap_colors = [\"r\", \"pink\", \"white\"], Lap_hess=Lap0_logs[\"original symmetrized Hessian\"])\n",
    "    else:\n",
    "        nested_sampling_plot(model, model_evidence_log, xdim = xdim, ydim = ydim, filter_type=filter_type, std_filter=std_filter, show_last_num=show_last_num, return_figure=False, title_add=\"\", fig=f, ax=ax, display_figure=False, plot_mll_opt=plot_mll_opt, mll_opt_params=mll_opt_params, plot_lap=True, Lap0_logs=Lap0_logs, LapAIC_logs=LapAIC_logs, LapBIC_logs=LapBIC_logs, lap_colors = [\"r\", \"pink\", \"white\"], Lap_hess=Lap0_logs[\"original symmetrized Hessian\"])\n",
    "    if ax == axs[0] or len(levels)==1:\n",
    "        ax.set_ylabel(param_names[ydim])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_opt_params = [p.item() for p in model_MAP.parameters() if p.requires_grad]\n",
    "\n",
    "plot_map_opt = False\n",
    "std_filter = None # float as 1e-3 or None\n",
    "show_last_num = 0.1 # integer, float or None\n",
    "filter_type = \"none\" # (\"mean\"), \"max\", \"none\"\n",
    "xdim = 1\n",
    "ydim = 2\n",
    "\n",
    "param_names = [l[0] for l in list(model.named_parameters())]\n",
    "\n",
    "f, axs = plt.subplots(1, len(levels), figsize=(4*len(levels), 4), sharey=True, layout=\"constrained\")\n",
    "if len(levels) == 1:\n",
    "    axs = [axs]\n",
    "for ax, level, model_evidence_log in zip(axs, levels, model_evidence_logs):\n",
    "    if ax == axs[-1]:\n",
    "        posterior_surface_plot(model, model_evidence_log, xdim = xdim, ydim = ydim, filter_type=filter_type, std_filter=std_filter, show_last_num=show_last_num, return_figure=False, title_add=\"\", fig=f, ax=ax, display_figure=False, plot_mll_opt=plot_map_opt, mll_opt_params=map_opt_params, plot_lap=True, Lap0_logs=Lap0_logs, LapAIC_logs=LapAIC_logs, LapBIC_logs=LapBIC_logs, lap_colors = [\"r\", \"pink\", \"white\"], uninformed=uninformed, Lap_hess=Lap0_logs[\"original symmetrized Hessian\"])\n",
    "    else:\n",
    "        posterior_surface_plot(model, model_evidence_log, xdim = xdim, ydim = ydim, filter_type=filter_type, std_filter=std_filter, show_last_num=show_last_num, return_figure=False, title_add=\"\", fig=f, ax=ax, display_figure=False, plot_mll_opt=plot_map_opt, mll_opt_params=map_opt_params, plot_lap=True, Lap0_logs=Lap0_logs, LapAIC_logs=LapAIC_logs, LapBIC_logs=LapBIC_logs, lap_colors = [\"r\", \"pink\", \"white\"], uninformed=uninformed, Lap_hess=Lap0_logs[\"original symmetrized Hessian\"])\n",
    "    if ax == axs[0] or len(levels)==1:\n",
    "        ax.set_ylabel(param_names[ydim])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the best value and the corresponding hyperparameters\n",
    "best_idx = np.argmax(res.logl)\n",
    "best_hyperparameters = res.samples[best_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimal GP according to Nested sampling\n",
    "fixed_reinit(model, torch.tensor(best_hyperparameters))\n",
    "model.eval()\n",
    "likelihood.eval()\n",
    "# mll of best parameterization according to Nested sampling:\n",
    "mll_fkt = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "pos_scaled_nested_top_mll = (mll_fkt(model(train_x), train_y))*len(train_x)\n",
    "pos_scaled_nested_top_map = pos_scaled_nested_top_mll+log_normalized_prior(model)*len(train_x)\n",
    "\n",
    "fig = plt.figure(figsize=(16, 6))\n",
    "ax = fig.add_subplot(1, 2, 1, projection='3d')\n",
    "\n",
    "x_min=min(train_x[:,0])\n",
    "x_max=max(train_x[:,0])\n",
    "y_min=min(train_x[:,1])\n",
    "y_max=max(train_x[:,1])\n",
    "\n",
    "\n",
    "title_add = \"\"\n",
    "if y_min == y_max:\n",
    "    y_min = y_min-0.1\n",
    "    y_max = y_max+0.1\n",
    "    title_add += \" Synthetic y stretch\"\n",
    "if x_min == x_max:\n",
    "    x_min = x_min-0.1\n",
    "    x_max = x_max+0.1\n",
    "    title_add += \" Synthetic x stretch\"\n",
    "\n",
    "\n",
    "fig, ax = plot_3d_gp(model, likelihood, data=torch.stack([train_x[:,0], train_x[:,1], train_y], -1), x_min=x_min, x_max=x_max, y_min=y_min, y_max=y_max, return_figure=True, fig=fig, ax=ax, display_figure=False, shadow=False)\n",
    "#fig, ax = plot_model(model, likelihood, train_x, train_y, return_figure=True, loss_val=res.logl[best_idx], loss_type=\"Nested logl\")\n",
    "ax.title.set_text(f\"{ax.title.get_text()} ; MLL: {pos_scaled_nested_top_mll.item():.2f}; MAP: {pos_scaled_nested_top_map.item():.2f}\\n MLL (u) {-pos_scaled_nested_top_mll/len(train_x):.2f}; MAP (u) {-pos_scaled_nested_top_map/len(train_x):.2f}\\n{title_add}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect metric names and values into a list of tuples\n",
    "metrics_data = [\n",
    "    (\"MLL\", f\"{pos_unscaled_MLL.item():.3f}\"),\n",
    "    (\"MLL (loss)\", f\"{neg_scaled_mll_loss.item():.3f}\"),\n",
    "    (\"AIC\", f\"{AIC_val.item():.3f}\"),\n",
    "    (\"AIC (s)\", f\"{(AIC_val.item()*(-0.5)):.3f}\"),\n",
    "    (\"BIC\", f\"{BIC_val.item():.3f}\"),\n",
    "    (\"BIC (s)\", f\"{(BIC_val.item()*(-0.5)):.3f}\"),\n",
    "    \n",
    "\n",
    "]\n",
    "labels = [m[0] for m in metrics_data]\n",
    "values = [m[1] for m in metrics_data]\n",
    "# Print in transposed form\n",
    "print(\"|        | \" + \" | \".join(labels) + \" |\")\n",
    "print(\"|--------|\" + \"|\".join([\"-----------\" for _ in labels]) + \"|\")\n",
    "print(\"| Value  | \" + \" | \".join(values) + \" |\")\n",
    "print()\n",
    "\n",
    "metrics_data = [(\"MAP\", f\"{pos_unscaled_MAP.item():.3f}\"),\n",
    "    (\"MAP (loss)\", f\"{neg_scaled_map_loss.item():.3f}\"),\n",
    "    (\"AIC_M\", f\"{AIC_MAP_val.item():.3f}\"),\n",
    "    (\"AIC_M (s)\", f\"{(AIC_MAP_val.item()*(-0.5)):.3f}\"),\n",
    "    (\"BIC_M\", f\"{BIC_MAP_val.item():.3f}\"),\n",
    "    (\"BIC_M (s)\", f\"{(BIC_MAP_val.item()*(-0.5)):.3f}\"),\n",
    "]\n",
    "\n",
    "labels = [m[0] for m in metrics_data]\n",
    "values = [m[1] for m in metrics_data]\n",
    "# Print in transposed form\n",
    "print(\"|        | \" + \" | \".join(labels) + \" |\")\n",
    "print(\"|--------|\" + \"|\".join([\"-----------\" for _ in labels]) + \"|\")\n",
    "print(\"| Value  | \" + \" | \".join(values) + \" |\")\n",
    "print()\n",
    "\n",
    "\n",
    "metrics_data=[    \n",
    "    (\"Lap\", f\"{Lap0_logs[\"laplace without replacement\"].item():.3f}\"),\n",
    "    (\"Lap0\", f\"{Lap0_val.item():.3f}\"),\n",
    "    (\"LapAIC\", f\"{LapAIC_val.item():.3f}\"),\n",
    "    (\"LapBIC\", f\"{LapBIC_val.item():.3f}\"),\n",
    "    ]\n",
    "\n",
    "\n",
    "labels = [m[0] for m in metrics_data]\n",
    "values = [m[1] for m in metrics_data]\n",
    "# Print in transposed form\n",
    "print(\"|        | \" + \" | \".join(labels) + \" |\")\n",
    "print(\"|--------|\" + \"|\".join([\"-----------\" for _ in labels]) + \"|\")\n",
    "print(\"| Value  | \" + \" | \".join(values) + \" |\")\n",
    "print()\n",
    "\n",
    "metrics_data = [(f\"Nested ({level:.1e})\", f\"{Nested_val:.3f}\") \n",
    "                for level, Nested_val in zip(levels, model_evidences)]\n",
    "\n",
    "# Transpose the table: one row for the label, another for the value\n",
    "labels = [m[0] for m in metrics_data]\n",
    "values = [m[1] for m in metrics_data]\n",
    "\n",
    "# Print in transposed form\n",
    "print(\"|        | \" + \" | \".join(labels) + \" |\")\n",
    "print(\"|--------|\" + \"|\".join([\"-----------\" for _ in labels]) + \"|\")\n",
    "print(\"| Value  | \" + \" | \".join(values) + \" |\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{dataset_name} - {pattern} {X1_noise};{X2_noise} - {kernel_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fixed_reinit(model_MAP, torch.tensor(best_hyperparameters))\n",
    "mll_fkt = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood_MAP, model_MAP)\n",
    "neg_scaled_map_loss = -mll_fkt(model_MAP(train_x), train_y)-log_normalized_prior(model_MAP)\n",
    "pos_unscaled_MAP = -neg_scaled_map_loss*len(*model.train_inputs)\n",
    "num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "Nested_Lap0_val, Nested_Lap0_logs = Laplace(model_MAP, pos_unscaled_MAP, param_punish_term=0)\n",
    "\n",
    "print(f\"Lap/nested = {Lap0_logs[\"laplace without replacement\"].item()/nested_pos_model_evidence_e5:.3f}\"),\n",
    "print(f\"Lap0/nested = {Lap0_val/nested_pos_model_evidence_e5:.3f}\")\n",
    "print(f\"AIC (s)/nested = {AIC_val*(-0.5)/nested_pos_model_evidence_e5:.3f}\") \n",
    "print(f\"nested Lap0/nested = {Nested_Lap0_val/nested_pos_model_evidence_e5:.3f}\\n\")\n",
    "print(\"```python\")\n",
    "print(Nested_Lap0_val)\n",
    "\n",
    "print(Nested_Lap0_logs)\n",
    "print(\"```\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Temp area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_dim_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "two_dim_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(one_dim_data[0] - two_dim_data[0][:,0])\n",
    "one_dim_data[1] - two_dim_data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_dim_mean_vec - two_dim_mean_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.min(np.array((one_dim_cov_matr - two_dim_cov_matr).detach().numpy()))\n",
    "# The value is negative (-0.001...), hence the two_dim_cov_matr is larger\n",
    "# This can happen when either the noise is larger in the two-dim case or the lengthscale is larger\n",
    "# Here, the noise is insignificantly smaller in the 2D case and the lengthscale is just slightly larger\n",
    "\n",
    "# HOWEVER, once I set the parameter values to be equal to the trained parameters of the 2D case, the difference between the covariance matrices is reduced to -4e-5, which could be argued is just numerical error.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Torch softplus call\n",
    "print(torch.nn.functional.softplus(torch.tensor(-11.6140)) - torch.nn.functional.softplus(torch.tensor(-11.7261)))\n",
    "print(torch.nn.functional.softplus(torch.tensor(-0.3335)) - torch.nn.functional.softplus(torch.tensor(-0.3326)))\n",
    "print(torch.nn.functional.softplus(torch.tensor(-0.3335)))\n",
    "print(torch.nn.functional.softplus(torch.tensor(-0.3326)))\n",
    "# Given the insignificant noise difference I would conclude the issue is with the lengthscale values"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# One dimensional case:\n",
    "Final best parameters: [('likelihood.noise_covar.raw_noise', Parameter containing:\n",
    "tensor([-11.6140], requires_grad=True)), ('covar_module.kernels.0.raw_lengthscale', Parameter containing:\n",
    "tensor([[-0.3335]], requires_grad=True)), ('covar_module.kernels.1.raw_lengthscale', Parameter containing:\n",
    "tensor([[-0.3335]], requires_grad=True))] w. loss: 0.6454170942306519 (smaller=better)\n",
    "# Two dimensional case:\n",
    "Final best parameters: [('likelihood.noise_covar.raw_noise', Parameter containing:\n",
    "tensor([-11.7261], requires_grad=True)), ('covar_module.base_kernel.kernels.0.raw_lengthscale', Parameter containing:\n",
    "tensor([[-0.3326]], requires_grad=True)), ('covar_module.base_kernel.kernels.1.raw_lengthscale', Parameter containing:\n",
    "tensor([[-0.3326]], requires_grad=True))] w. loss: 0.6454619765281677 (smaller=better)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(one_dim_post_mean_vec)\n",
    "print(one_dim_post_cov_matr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(two_dim_post_mean_vec)\n",
    "print(two_dim_post_cov_matr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the differences\n",
    "print(one_dim_post_mean_vec - two_dim_post_mean_vec)\n",
    "print(one_dim_post_cov_matr - two_dim_post_cov_matr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.max(np.array((one_dim_post_mean_vec - two_dim_post_mean_vec).detach().numpy())))\n",
    "np.max(np.array((one_dim_post_cov_matr - two_dim_post_cov_matr).detach().numpy()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for one_d, two_d in zip((one_dim_train_log.neg_loss[0], one_dim_train_log.x[0]), (two_dim_train_log.neg_loss[0], two_dim_train_log.x[0])):\n",
    "    #print(f\"{one_d[0]} - {two_d[0]}\")\n",
    "    print(one_d[0] - two_d[0])\n",
    "    print(one_d[1] - two_d[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"1D\", len(one_dim_train_log.neg_loss[0]))\n",
    "print(\"2D\", len(two_dim_train_log.neg_loss[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(max((len(one_dim_train_log.neg_loss[0]), len(two_dim_train_log.neg_loss[0])))):\n",
    "    print(f\"Step {i}\")\n",
    "    try:\n",
    "        one_dim_params = one_dim_train_log.x[0][i]\n",
    "        one_dim_MAP = one_dim_train_log.neg_loss[0][i]\n",
    "    except:\n",
    "        one_dim_params = None\n",
    "        one_dim_MAP = None\n",
    "    try:\n",
    "        two_dim_params = two_dim_train_log.x[0][i]\n",
    "        two_dim_MAP = two_dim_train_log.neg_loss[0][i]\n",
    "    except:\n",
    "        two_dim_params = None\n",
    "        two_dim_MAP = None\n",
    "\n",
    "    if one_dim_params is not None and two_dim_params is not None:\n",
    "        for one_d, two_d in zip(one_dim_params, two_dim_params):\n",
    "            print(f\"{one_d} - {two_d}\")\n",
    "        print(f\"MAPs {one_dim_MAP} - {two_dim_MAP}\")\n",
    "        print(f\"MAP diff {one_dim_MAP - two_dim_MAP}\")\n",
    "    elif one_dim_params is not None:\n",
    "        print(f\"One dim params {one_dim_params} - {one_dim_MAP}\")\n",
    "    elif two_dim_params is not None:\n",
    "        print(f\"Two dim params {two_dim_params} - {two_dim_MAP}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1D Block that performs a grid search over parameters and stores the MLL/MAP values (+ first and second derivative?)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tqdm"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def derivs_calc(params_list, loss):\n",
    "    # calculate the hessian of the unscaled loss\n",
    "    try:\n",
    "        env_grads = torch.autograd.grad(loss, params_list, retain_graph=True, create_graph=True, allow_unused=True)\n",
    "    except Exception as E:\n",
    "        print(E)\n",
    "        import pdb\n",
    "        pdb.set_trace()\n",
    "        print(f\"E:{E}\")\n",
    "    hess_params = []\n",
    "    # Calcuate -\\nabla\\nabla log(f(\\theta)) (i.e. Hessian of negative log marginal likelihood)\n",
    "    for i in range(len(env_grads)):\n",
    "        hess_params.append(torch.autograd.grad(env_grads[i], params_list, retain_graph=True, allow_unused=True)) \n",
    "\n",
    "    return env_grads, hess_params\n",
    "\n",
    "import itertools\n",
    "import tqdm\n",
    "log_noise_range = (-20, 10)\n",
    "log_lengthscale_range = (-20, 20)\n",
    "noise_steps = 75 \n",
    "lengthscale_steps = 100\n",
    "mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "params_list = [p for p in model.parameters()]\n",
    "for noise, l1, l2 in tqdm.tqdm(itertools.product(torch.linspace(log_noise_range[0], log_noise_range[1], noise_steps),\n",
    "                         torch.linspace(log_lengthscale_range[0], log_lengthscale_range[1], lengthscale_steps),\n",
    "                         torch.linspace(log_lengthscale_range[0], log_lengthscale_range[1], lengthscale_steps))):\n",
    "    fixed_reinit(model, torch.tensor([noise, l1, l2]))\n",
    "    # POSITIVE MLL\n",
    "    mll_val = mll(model(train_x), train_y)\n",
    "    # POSITIVE parameter prior likelihood\n",
    "    prior_val = log_normalized_prior(model=model, uninformed=uninformed)\n",
    "    map_val = mll_val + prior_val\n",
    "\n",
    "    Jacobian_mll, Hessian_mll = derivs_calc(params_list, mll_val)\n",
    "    Jacobian_map, Hessian_map = derivs_calc(params_list, map_val)\n",
    "\n",
    "    # store at the end of a csv file\n",
    "    # Header is \"log noise, log lengthscale 1, log lengthscale 2, mll, prior, map, d/dx mll, d2/dx2 mll, d/dx map, d2/dx2 map\"\n",
    "    df = pd.DataFrame({\n",
    "        \"log noise\": [noise.item()],\n",
    "        \"log lengthscale 1\": [l1.item()],\n",
    "        \"log lengthscale 2\": [l2.item()],\n",
    "        \"mll\": [mll_val.item()],\n",
    "        \"prior\": [prior_val.item()],\n",
    "        \"map\": [map_val.item()],\n",
    "        \"d/dx mll\": [Jacobian_mll],\n",
    "        \"d2/dx2 mll\": [Hessian_mll],\n",
    "        \"d/dx map\": [Jacobian_map],\n",
    "        \"d2/dx2 map\": [Hessian_map]\n",
    "    })\n",
    "    df.to_csv(\"likelihood_exploration_grid.csv\", mode=\"a\", sep=\";\", index=False, header=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "likelihood_exploration_df = pd.read_csv(\"likelihood_exploration_grid.csv\", sep=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "like_explor_positions = likelihood_exploration_df[[\"log noise\", \"log lengthscale 1\", \"log lengthscale 2\"]].to_numpy()\n",
    "like_explor_pos_scaled_mlls = likelihood_exploration_df[[\"mll\"]].to_numpy().flatten()\n",
    "like_explor_pos_scaled_maps = likelihood_exploration_df[[\"map\"]].to_numpy().flatten()\n",
    "like_explor_pos_scaled_maps_hess = likelihood_exploration_df[\"d2/dx2 map\"].apply(lambda x: torch.tensor([float(f) for f in re.findall(r\"[-+]?\\d*\\.\\d*\", x)]).reshape(3,3).numpy())\n",
    "like_explor_pos_scaled_maps_hess_dets = likelihood_exploration_df[\"d2/dx2 map\"].apply(lambda x: torch.linalg.det(torch.tensor([float(f) for f in re.findall(r\"[-+]?\\d*\\.\\d*\", x)]).reshape(3,3)).numpy())\n",
    "#like_explor_mlls_hess = likelihood_exploration_df[\"d2/dx2 mll\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Optimal parameters\")\n",
    "print(\"MAP: \", like_explor_positions[np.argmax(like_explor_pos_scaled_maps)], \":\", max(like_explor_pos_scaled_maps))\n",
    "print(\"MLL: \", like_explor_positions[np.argmax(like_explor_pos_scaled_mlls)], \":\", max(like_explor_pos_scaled_mlls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diag_neg_truth_map = [h[1][1]<0 and h[2][2]<0 for h in like_explor_pos_scaled_maps_hess]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(diag_neg_truth_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum([d < 0 for d in like_explor_pos_scaled_maps_hess_dets])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Count instances where the Hessian is not symmetric\n",
    "non_sym_count = sum([torch.allclose(h, h.T, atol=1e-5) for h in torch.tensor(like_explor_pos_scaled_maps_hess)])\n",
    "print(f\"Number of non-symmetric Hessians: {len(like_explor_pos_scaled_maps_hess) - non_sym_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "like_explor_pos_scaled_maps_hess[305353]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max(like_explor_pos_scaled_maps_hess_dets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "like_explor_pos_scaled_maps_hess[np.argmax(like_explor_pos_scaled_maps_hess_dets)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "xdim = 0\n",
    "ydim = 2\n",
    "\n",
    "ax_names = [[\"Log Noise\", \"Log Lengthscale 1\", \"Log Lengthscale 2\"][xdim], [\"Log Noise\", \"Log Lengthscale 1\", \"Log Lengthscale 2\"][ydim]]\n",
    "\n",
    "\n",
    "scatter_function_visualization(like_explor_positions, like_explor_pos_scaled_maps, axis_names=ax_names, show_best=True, filter_mode=\"max\", std_filter=10, display_figure=True, xdim=xdim, ydim=ydim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "xdim = 0\n",
    "ydim = 2\n",
    "\n",
    "ax_names = [[\"Log Noise\", \"Log Lengthscale 1\", \"Log Lengthscale 2\"][xdim], [\"Log Noise\", \"Log Lengthscale 1\", \"Log Lengthscale 2\"][ydim]]\n",
    "\n",
    "\n",
    "scatter_function_visualization(like_explor_positions, like_explor_pos_scaled_maps_hess_dets, axis_names=ax_names, show_best=True, filter_mode=\"max\", std_filter=10, display_figure=True, xdim=xdim, ydim=ydim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2D Block that performs a grid search over parameters and stores the MLL/MAP values (+ first and second derivative?)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tqdm"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def derivs_calc(params_list, loss):\n",
    "    # calculate the hessian of the unscaled loss\n",
    "    try:\n",
    "        env_grads = torch.autograd.grad(loss, params_list, retain_graph=True, create_graph=True, allow_unused=True)\n",
    "    except Exception as E:\n",
    "        print(E)\n",
    "        import pdb\n",
    "        pdb.set_trace()\n",
    "        print(f\"E:{E}\")\n",
    "    hess_params = []\n",
    "    # Calcuate -\\nabla\\nabla log(f(\\theta)) (i.e. Hessian of negative log marginal likelihood)\n",
    "    for i in range(len(env_grads)):\n",
    "        hess_params.append(torch.autograd.grad(env_grads[i], params_list, retain_graph=True, allow_unused=True)) \n",
    "\n",
    "    return env_grads, hess_params\n",
    "\n",
    "import itertools\n",
    "import tqdm\n",
    "log_noise_range = (-20, 10)\n",
    "log_lengthscale_range = (-20, 20)\n",
    "noise_steps = 75 \n",
    "lengthscale_steps = 100\n",
    "mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "params_list = [p for p in model.parameters()]\n",
    "for noise, l1, l2 in tqdm.tqdm(itertools.product(torch.linspace(log_noise_range[0], log_noise_range[1], noise_steps),\n",
    "                         torch.linspace(log_lengthscale_range[0], log_lengthscale_range[1], lengthscale_steps),\n",
    "                         torch.linspace(log_lengthscale_range[0], log_lengthscale_range[1], lengthscale_steps))):\n",
    "    fixed_reinit(model, torch.tensor([noise, l1, l2]))\n",
    "    # POSITIVE MLL\n",
    "    mll_val = mll(model(train_x), train_y)\n",
    "    # POSITIVE parameter prior likelihood\n",
    "    prior_val = log_normalized_prior(model=model, uninformed=uninformed)\n",
    "    map_val = mll_val + prior_val\n",
    "\n",
    "    Jacobian_mll, Hessian_mll = derivs_calc(params_list, mll_val)\n",
    "    Jacobian_map, Hessian_map = derivs_calc(params_list, map_val)\n",
    "\n",
    "    # store at the end of a csv file\n",
    "    # Header is \"log noise, log lengthscale 1, log lengthscale 2, mll, prior, map, d/dx mll, d2/dx2 mll, d/dx map, d2/dx2 map\"\n",
    "    df = pd.DataFrame({\n",
    "        \"log noise\": [noise.item()],\n",
    "        \"log lengthscale 1\": [l1.item()],\n",
    "        \"log lengthscale 2\": [l2.item()],\n",
    "        \"mll\": [mll_val.item()],\n",
    "        \"prior\": [prior_val.item()],\n",
    "        \"map\": [map_val.item()],\n",
    "        \"d/dx mll\": [Jacobian_mll],\n",
    "        \"d2/dx2 mll\": [Hessian_mll],\n",
    "        \"d/dx map\": [Jacobian_map],\n",
    "        \"d2/dx2 map\": [Hessian_map]\n",
    "    })\n",
    "    df.to_csv(\"likelihood_exploration_grid.csv\", mode=\"a\", sep=\";\", index=False, header=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "likelihood_exploration_df_2D = pd.read_csv(\"likelihood_exploration_grid_2D.csv\", sep=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "like_explor_positions_2D = likelihood_exploration_df_2D[[\"log noise\", \"log lengthscale 1\", \"log lengthscale 2\"]].to_numpy()\n",
    "like_explor_pos_scaled_mlls_2D = likelihood_exploration_df_2D[[\"mll\"]].to_numpy().flatten()\n",
    "like_explor_pos_scaled_maps_2D = likelihood_exploration_df_2D[[\"map\"]].to_numpy().flatten()\n",
    "like_explor_pos_scaled_maps_hess_2D = likelihood_exploration_df_2D[\"d2/dx2 map\"].apply(lambda x: torch.tensor([float(f) for f in re.findall(r\"[-+]?\\d*\\.\\d*\", x)]).reshape(3,3).numpy())\n",
    "like_explor_pos_scaled_maps_hess_dets_2D = likelihood_exploration_df_2D[\"d2/dx2 map\"].apply(lambda x: torch.linalg.det(torch.tensor([float(f) for f in re.findall(r\"[-+]?\\d*\\.\\d*\", x)]).reshape(3,3)).numpy())\n",
    "#like_explor_mlls_hess = likelihood_exploration_df[\"d2/dx2 mll\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Optimal parameters\")\n",
    "print(\"MAP: \", like_explor_positions_2D[np.argmax(like_explor_pos_scaled_maps_2D)], \":\", max(like_explor_pos_scaled_maps_2D))\n",
    "print(\"MLL: \", like_explor_positions_2D[np.argmax(like_explor_pos_scaled_mlls_2D)], \":\", max(like_explor_pos_scaled_mlls_2D))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diag_neg_truth_map_2D = [h[1][1]<0 and h[2][2]<0 for h in like_explor_pos_scaled_maps_hess_2D]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(diag_neg_truth_map_2D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum([d < 0 for d in like_explor_pos_scaled_maps_hess_dets_2D])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count instances where the Hessian is not symmetric\n",
    "non_sym_count_2D = sum([torch.allclose(h, h.T, atol=1e-5) for h in torch.tensor(like_explor_pos_scaled_maps_hess_2D)])\n",
    "print(f\"Number of non-symmetric Hessians: {len(like_explor_pos_scaled_maps_hess) - non_sym_count_2D}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max(like_explor_pos_scaled_maps_hess_dets_2D)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "like_explor_pos_scaled_maps_hess_2D[np.argmax(like_explor_pos_scaled_maps_hess_dets_2D)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "xdim = 0\n",
    "ydim = 2\n",
    "\n",
    "ax_names = [[\"Log Noise\", \"Log Lengthscale 1\", \"Log Lengthscale 2\"][xdim], [\"Log Noise\", \"Log Lengthscale 1\", \"Log Lengthscale 2\"][ydim]]\n",
    "\n",
    "\n",
    "scatter_function_visualization(like_explor_positions_2D, like_explor_pos_scaled_maps_2D, axis_names=ax_names, show_best=True, filter_mode=\"max\", std_filter=10, display_figure=True, xdim=xdim, ydim=ydim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "xdim = 0\n",
    "ydim = 2\n",
    "\n",
    "ax_names = [[\"Log Noise\", \"Log Lengthscale 1\", \"Log Lengthscale 2\"][xdim], [\"Log Noise\", \"Log Lengthscale 1\", \"Log Lengthscale 2\"][ydim]]\n",
    "\n",
    "\n",
    "scatter_function_visualization(like_explor_positions_2D, like_explor_pos_scaled_maps_hess_dets_2D, axis_names=ax_names, show_best=True, filter_mode=\"max\", std_filter=10, display_figure=True, xdim=xdim, ydim=ydim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"1D det argmax\", np.argmax(like_explor_pos_scaled_maps_hess_dets))\n",
    "print(\"2D det argmax\", np.argmax(like_explor_pos_scaled_maps_hess_dets_2D))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(like_explor_pos_scaled_maps_hess_dets[np.argmax(like_explor_pos_scaled_maps_hess_dets)])\n",
    "like_explor_pos_scaled_maps_hess_dets_2D[np.argmax(like_explor_pos_scaled_maps_hess_dets)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(like_explor_pos_scaled_maps_hess_dets_2D[np.argmax(like_explor_pos_scaled_maps_hess_dets_2D)])\n",
    "like_explor_pos_scaled_maps_hess_dets[np.argmax(like_explor_pos_scaled_maps_hess_dets_2D)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "index = random.randint(0, len(like_explor_pos_scaled_maps_hess_2D)-1)\n",
    "index = 204949\n",
    "print(index)\n",
    "print(like_explor_positions[index])\n",
    "print(like_explor_positions_2D[index])\n",
    "print(like_explor_pos_scaled_maps_hess[index])\n",
    "print(like_explor_pos_scaled_maps_hess_2D[index])\n",
    "print(like_explor_pos_scaled_maps_hess[index] - like_explor_pos_scaled_maps_hess_2D[index])\n",
    "#print(torch.linalg.det(like_explor_maps_hess[index] - like_explor_maps_hess_2D[index]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_minus = 204948\n",
    "f = 204949\n",
    "f_plus = 204950\n",
    "\n",
    "print((like_explor_pos_scaled_maps[f_minus] - 2*like_explor_pos_scaled_maps[f] + like_explor_pos_scaled_maps[f_plus]) / ((-0.404)**2))\n",
    "print((like_explor_pos_scaled_maps_2D[f_minus] - 2*like_explor_pos_scaled_maps_2D[f] + like_explor_pos_scaled_maps_2D[f_plus]) / ((-0.404)**2))\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The MAP and MLL optima are at the same location in both GPs, at least\n",
    "index = np.argmax(like_explor_pos_scaled_maps)\n",
    "print(index)\n",
    "print(like_explor_positions[index])\n",
    "print(like_explor_positions_2D[index])\n",
    "print(like_explor_pos_scaled_maps_hess[index])\n",
    "print(like_explor_pos_scaled_maps_hess_2D[index])\n",
    "print(like_explor_pos_scaled_maps_hess[index] - like_explor_pos_scaled_maps_hess_2D[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "like_explor_maps_hess_difference_dets = np.array([np.linalg.det(p) for p in (like_explor_pos_scaled_maps_hess - like_explor_pos_scaled_maps_hess_2D).to_numpy()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "xdim = 0\n",
    "ydim = 2\n",
    "\n",
    "ax_names = [[\"Log Noise\", \"Log Lengthscale 1\", \"Log Lengthscale 2\"][xdim], [\"Log Noise\", \"Log Lengthscale 1\", \"Log Lengthscale 2\"][ydim]]\n",
    "\n",
    "\n",
    "scatter_function_visualization(like_explor_positions_2D, like_explor_maps_hess_difference_dets, axis_names=ax_names, show_best=True, filter_mode=\"none\", std_filter=100, display_figure=True, xdim=xdim, ydim=ydim)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sage3_129",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
