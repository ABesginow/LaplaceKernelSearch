{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "16826c04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: anytree in /home/besginow/anaconda3/envs/sage/lib/python3.10/site-packages (2.8.0)\n",
      "Requirement already satisfied: six>=1.9.0 in /home/besginow/anaconda3/envs/sage/lib/python3.10/site-packages (from anytree) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install anytree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c252e947",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "import os\n",
    "import pprint\n",
    "from anytree import Node, RenderTree\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dca837a3",
   "metadata": {},
   "source": [
    "For future iterations of this I might change the scheme to a deep directory based approach.    \n",
    "Then iterations would be named `1.pickle`, `2.pickle` and be stored in deep directories like:    \n",
    "`\"Experiment_name/config1/config2/config3/config4/1.pickle\"`    \n",
    "This way I could just split the name accross the folder separator and have all the setting values, then the only thing I need is a list that gives me the correct setting-value assignment, which sounds a lot easier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afa87acf",
   "metadata": {},
   "source": [
    "# Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6bb3e6f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpickle(filepath : str):\n",
    "    file_object = open(filepath, \"rb\")\n",
    "    results = pickle.load(file_object)\n",
    "    file_object.close()\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b9256daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_setting_from_name(name :str, naming_schema : list):\n",
    "    result = {}\n",
    "    # Main assumption for the naming schema: parameters are strictly separated by underscores '_'\n",
    "    for value, setting in zip(name.split(\"_\"), naming_schema):\n",
    "        result[setting] = value\n",
    "    return result\n",
    "    "
   ]
  },
  {
   "cell_type": "raw",
   "id": "02415768",
   "metadata": {},
   "source": [
    "class Tree:\n",
    "    def __init__(self, leaves : list, parent=None):\n",
    "        self.parent = parent\n",
    "        self.leaves = leaves\n",
    "\n",
    "        \n",
    "    def recursive_tree_print(self, root):\n",
    "        line = \"\"\n",
    "        for branch in root:\n",
    "            if type(branch) == Tree:\n",
    "                line += f\"{branch} -\"\n",
    "                line += self.recursive_tree_print(branch)\n",
    "            elif type(branch) == list:\n",
    "                for b in branch:\n",
    "                    line += b + \"\\t\"\n",
    "            else:\n",
    "                line += branch + \"\\t\"\n",
    "        return line\n",
    "        \n",
    "    def __str__(self):\n",
    "        return self.recursive_tree_print(self.leaves)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5ab46d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_results(directory, experiment_name, naming_schema=None):\n",
    "    path = Path(directory)\n",
    "\n",
    "    dirs = [e for e in path.iterdir() if e.is_dir() and not str(e) == '.ipynb_checkpoints']\n",
    "    subdirs = {str(path): [e for e in path.iterdir() if e.is_dir()] for path in dirs}\n",
    "    \n",
    "    relevant_subdirs = subdirs[os.path.join(directory, experiment_name)]\n",
    "    relevant_subdirs.sort()\n",
    "    #result_filename = '*.pickle'\n",
    "    pickle_dirs = list()\n",
    "    for subdir in relevant_subdirs:\n",
    "        pickle_dirs.extend(sorted(subdir.glob(\"*.pickle\")))\n",
    "    results = []\n",
    "    if not naming_schema is None:\n",
    "        all_attributes = [extract_setting_from_name(subdir.name, naming_schema) for subdir in relevant_subdirs]    \n",
    "        for attributes, pick in zip(all_attributes, pickle_dirs):\n",
    "            try:\n",
    "                unpickled_stuff = unpickle(pick)\n",
    "                results.append({'attributes': attributes, 'results': unpickled_stuff})\n",
    "            except:\n",
    "                # Sometimes there were unknown issues with the pickle files, in those instances we re-ran training\n",
    "                print(\"Catastrophic failure\")\n",
    "    else:\n",
    "        for pick in pickle_dirs:\n",
    "            try:\n",
    "                unpickled_stuff = unpickle(pick)\n",
    "                results.append({'results': unpickled_stuff})\n",
    "            except:\n",
    "                # Sometimes there were unknown issues with the pickle files, in those instances we re-ran training\n",
    "                print(\"Catastrophic failure\")\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ef7376c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def deep_dict_get(data : dict, path : str):\n",
    "    \"\"\"\n",
    "    path a slash ('/') separated path down to \n",
    "    \"\"\"\n",
    "    temp = data.copy()\n",
    "    for entry in path.split(\"/\"):\n",
    "        # Catches leading '/' in tree printing\n",
    "        if entry == '':\n",
    "            continue\n",
    "        temp = temp[entry]\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f41bc6e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_values(d1 : dict, d2 : dict, path : str):\n",
    "    v1 = deep_dict_get(d1, path)\n",
    "    v2 = deep_dict_get(d2, path)\n",
    "    # Sanity check 1\n",
    "    if not type(v1) == type(v2):\n",
    "        return False\n",
    "    else:\n",
    "        # This can potentially cause errors when comparing lists of lists (or Tensors/Arrays)\n",
    "        return v1 == v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4d833486",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_common_root_list(root : str, values : list):\n",
    "    return [f\"{root}/{val}\" for val in values]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ed03ddf",
   "metadata": {},
   "source": [
    "# Main functions    \n",
    "What features do I need?    \n",
    "- ~~Show a structure tree of the results (i.e. experiment settings and result values)~~~\n",
    "- ~~Filtering by setting, given a key~~\n",
    "- ~~Filtering by values, given a key~~\n",
    "- Create selected statistics (mean/med/std/quartiles/...) for certain values/keys\n",
    "- Apply a function to certain values/keys and return the results (e.g. Eigendecomposition/Normalization/...)\n",
    "- ~~Group all entries that share settings/values~~\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cb4593bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_tree_structure(data, parent=None):\n",
    "    # Solved through recursively going deeper into the data structure and then returning the leafs if at the end\n",
    "\n",
    "    if parent is None and not len(data.keys()) == 1:\n",
    "        parent = Node(\"root\")\n",
    "    elif parent is None:\n",
    "        parent = Node(list(data.keys())[0])\n",
    "        data = data[list(data.keys())[0]]\n",
    "    \n",
    "    # Recursion condition\n",
    "    # If there are any dictionaries inside then go deeper\n",
    "    if not any([type(data[entry]) == dict for entry in data]):\n",
    "        for entry in data:\n",
    "            Node(entry, parent=parent) \n",
    "    else:\n",
    "        for entry in data:\n",
    "            if type(data[entry]) == dict:\n",
    "                branch = Node(entry, parent=parent)\n",
    "                generate_tree_structure(data[entry], parent=branch)\n",
    "            else:\n",
    "                Node(entry, parent=parent)\n",
    "    return parent\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "da64394e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_by(list_of_data : list, path, value=None):\n",
    "    \"\"\"\n",
    "    value : If None, make subgroups of equal values. \n",
    "            Otherwise return a single group where value is matched\n",
    "    \"\"\"\n",
    "    grouped_data = {}\n",
    "\n",
    "    if not value is None:\n",
    "        grouped_data[f\"{path} = {value}\"] = [data for data in list_of_data if deep_dict_get(data, path) == value]\n",
    "    else:    \n",
    "        finished_values = list()\n",
    "        for data in list_of_data:\n",
    "            value = deep_dict_get(data, path)\n",
    "            grouped_data[f\"{path} = {value}\"].append(data)\n",
    "            #if not value in finished_values:\n",
    "            #    grouped_data[f\"{path} = value\"] = [data for data in list_of_data if deep_dict_get(data, path) == value]\n",
    "            #    finished_values.append(value)\n",
    "    return grouped_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e29e1281",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dict_compare(d1, d2):\n",
    "    # Checking for empty list in d2, i.e. initial value\n",
    "    if not type(d1) == type(d2):\n",
    "        return False\n",
    "    return all((d1.get(k) == v for k, v in d2.items()))\n",
    "        \n",
    "\n",
    "# Could contain an alternative head as (list_of_data : list, paths : [list, dict], values : None) \n",
    "# where paths-values would require a 1-to-1 correspondence. \n",
    "# But this could quickly become error prone on the user side...\n",
    "def group_by_multiple(list_of_data : list, paths):\n",
    "    grouped_data = {}\n",
    "    # Grouping without values\n",
    "    if type(paths) == list:\n",
    "        finished_values = list()\n",
    "        for data in list_of_data:\n",
    "            paths_vals = {path : deep_dict_get(data, path) for path in paths}\n",
    "            # not any X <=> all not X\n",
    "            # i.e. only succeeds when this combination didn't exist before\n",
    "            if not any([dict_comapare(paths_vals, fin_val) for fin_val in finished_values]):\n",
    "                grouped_data[\" ; \".join([f\"{path} = {paths_vals[path]}\" for path in paths_vals])] = [data]\n",
    "            else:\n",
    "                grouped_data[\" ; \".join([f\"{path} = {paths_vals[path]}\" for path in paths_vals])].append(data)\n",
    "\n",
    "    # Grouping by path-value combinations\n",
    "    # Only returns the group where all those pairs are true\n",
    "    elif type(paths) == dict:\n",
    "        # The keys will contain the paths\n",
    "        # The values will be the corresponding expected values\n",
    "        # Yes, this could be a one-liner with a very neat nested list creation, \n",
    "        #  but I chose readability with temporary variables over it.\n",
    "        good_data = list()\n",
    "        for data in list_of_data:\n",
    "            if all([deep_dict_get(data, path) == paths[path] for path in paths]):\n",
    "                good_data.append(data)\n",
    "        grouped_data[\" ; \".join([f\"{path} = {paths[path]}\" for path in paths])] = good_data\n",
    "    return grouped_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1e065d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_by(list_of_data : list, path : str, value):\n",
    "    filtered_data = list()\n",
    "    for data in list_of_data:\n",
    "        if deep_dict_get(data, path) == value:\n",
    "            filtered_data.append(data)\n",
    "    return filtered_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "174730a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_value(list_of_data : list, path : str):\n",
    "    \"\"\"\n",
    "        returns a list of the target value from each data dict\n",
    "    \"\"\"\n",
    "    list_of_values = list()\n",
    "    for data in list_of_data:\n",
    "        list_of_values.append(deep_dict_get(data, path))\n",
    "    return list_of_values"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d505fea3",
   "metadata": {},
   "source": [
    "# Pseudocode blocks\n",
    "\n",
    "\n",
    "def calc_mean(data):\n",
    "    return mean(data)\n",
    "\n",
    "def calc_std(data):\n",
    "    return std(data)\n",
    "\n",
    "\n",
    "def calc_statistics(data, statistics):\n",
    "    if type(statistics) == list:\n",
    "        result dictionary\n",
    "        for statistic in statistics:\n",
    "            result_dict[statistic] = call statistic\n",
    "        return result_dict\n",
    "    else:\n",
    "        reutrn call statistic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b944501",
   "metadata": {},
   "source": [
    "# Load results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b730f878",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Node('/results/attributes'),\n",
      " Node('/results/attributes/eval_START'),\n",
      " Node('/results/attributes/eval_END'),\n",
      " Node('/results/attributes/eval_COUNT'),\n",
      " Node('/results/attributes/optimizer'),\n",
      " Node('/results/attributes/train_iters'),\n",
      " Node('/results/attributes/LR'),\n",
      " Node('/results/attributes/BFGS'),\n",
      " Node('/results/attributes/data_gen'),\n",
      " Node('/results/MLL'),\n",
      " Node('/results/MLL/SIN*RBF'),\n",
      " Node('/results/MLL/SIN*RBF/loss'),\n",
      " Node('/results/MLL/SIN*RBF/model parameters'),\n",
      " Node('/results/MLL/C*C*RBF'),\n",
      " Node('/results/MLL/C*C*RBF/loss'),\n",
      " Node('/results/MLL/C*C*RBF/model parameters'),\n",
      " Node('/results/MLL/C*RBF'),\n",
      " Node('/results/MLL/C*RBF/loss'),\n",
      " Node('/results/MLL/C*RBF/model parameters'),\n",
      " Node('/results/MLL/4C*SIN'),\n",
      " Node('/results/MLL/4C*SIN/loss'),\n",
      " Node('/results/MLL/4C*SIN/model parameters'),\n",
      " Node('/results/MLL/C*SIN + C*SIN + C*SIN'),\n",
      " Node('/results/MLL/C*SIN + C*SIN + C*SIN/loss'),\n",
      " Node('/results/MLL/C*SIN + C*SIN + C*SIN/model parameters'),\n",
      " Node('/results/MLL/C*SIN + C*SIN'),\n",
      " Node('/results/MLL/C*SIN + C*SIN/loss'),\n",
      " Node('/results/MLL/C*SIN + C*SIN/model parameters'),\n",
      " Node('/results/MLL/C*SIN'),\n",
      " Node('/results/MLL/C*SIN/loss'),\n",
      " Node('/results/MLL/C*SIN/model parameters'),\n",
      " Node('/results/MC'),\n",
      " Node('/results/MC/SIN*RBF'),\n",
      " Node('/results/MC/SIN*RBF/loss'),\n",
      " Node('/results/MC/SIN*RBF/num_draws'),\n",
      " Node('/results/MC/SIN*RBF/details'),\n",
      " Node('/results/MC/SIN*RBF/details/Kernel code'),\n",
      " Node('/results/MC/SIN*RBF/details/seed'),\n",
      " Node('/results/MC/SIN*RBF/details/Likelihood time'),\n",
      " Node('/results/MC/SIN*RBF/details/Model compile time'),\n",
      " Node('/results/MC/SIN*RBF/details/Sampling time'),\n",
      " Node('/results/MC/SIN*RBF/details/Total time'),\n",
      " Node('/results/MC/SIN*RBF/details/Bad entries'),\n",
      " Node('/results/MC/SIN*RBF/details/likelihood approximation'),\n",
      " Node('/results/MC/C*C*RBF'),\n",
      " Node('/results/MC/C*C*RBF/loss'),\n",
      " Node('/results/MC/C*C*RBF/num_draws'),\n",
      " Node('/results/MC/C*C*RBF/details'),\n",
      " Node('/results/MC/C*C*RBF/details/Kernel code'),\n",
      " Node('/results/MC/C*C*RBF/details/seed'),\n",
      " Node('/results/MC/C*C*RBF/details/Likelihood time'),\n",
      " Node('/results/MC/C*C*RBF/details/Model compile time'),\n",
      " Node('/results/MC/C*C*RBF/details/Sampling time'),\n",
      " Node('/results/MC/C*C*RBF/details/Total time'),\n",
      " Node('/results/MC/C*C*RBF/details/Bad entries'),\n",
      " Node('/results/MC/C*C*RBF/details/likelihood approximation'),\n",
      " Node('/results/MC/C*RBF'),\n",
      " Node('/results/MC/C*RBF/loss'),\n",
      " Node('/results/MC/C*RBF/num_draws'),\n",
      " Node('/results/MC/C*RBF/details'),\n",
      " Node('/results/MC/C*RBF/details/Kernel code'),\n",
      " Node('/results/MC/C*RBF/details/seed'),\n",
      " Node('/results/MC/C*RBF/details/Likelihood time'),\n",
      " Node('/results/MC/C*RBF/details/Model compile time'),\n",
      " Node('/results/MC/C*RBF/details/Sampling time'),\n",
      " Node('/results/MC/C*RBF/details/Total time'),\n",
      " Node('/results/MC/C*RBF/details/Bad entries'),\n",
      " Node('/results/MC/C*RBF/details/likelihood approximation'),\n",
      " Node('/results/MC/4C*SIN'),\n",
      " Node('/results/MC/4C*SIN/loss'),\n",
      " Node('/results/MC/4C*SIN/num_draws'),\n",
      " Node('/results/MC/4C*SIN/details'),\n",
      " Node('/results/MC/4C*SIN/details/Kernel code'),\n",
      " Node('/results/MC/4C*SIN/details/seed'),\n",
      " Node('/results/MC/4C*SIN/details/Likelihood time'),\n",
      " Node('/results/MC/4C*SIN/details/Model compile time'),\n",
      " Node('/results/MC/4C*SIN/details/Sampling time'),\n",
      " Node('/results/MC/4C*SIN/details/Total time'),\n",
      " Node('/results/MC/4C*SIN/details/Bad entries'),\n",
      " Node('/results/MC/4C*SIN/details/likelihood approximation'),\n",
      " Node('/results/MC/C*SIN + C*SIN + C*SIN'),\n",
      " Node('/results/MC/C*SIN + C*SIN + C*SIN/loss'),\n",
      " Node('/results/MC/C*SIN + C*SIN + C*SIN/num_draws'),\n",
      " Node('/results/MC/C*SIN + C*SIN + C*SIN/details'),\n",
      " Node('/results/MC/C*SIN + C*SIN + C*SIN/details/Kernel code'),\n",
      " Node('/results/MC/C*SIN + C*SIN + C*SIN/details/seed'),\n",
      " Node('/results/MC/C*SIN + C*SIN + C*SIN/details/Likelihood time'),\n",
      " Node('/results/MC/C*SIN + C*SIN + C*SIN/details/Model compile time'),\n",
      " Node('/results/MC/C*SIN + C*SIN + C*SIN/details/Sampling time'),\n",
      " Node('/results/MC/C*SIN + C*SIN + C*SIN/details/Total time'),\n",
      " Node('/results/MC/C*SIN + C*SIN + C*SIN/details/Bad entries'),\n",
      " Node('/results/MC/C*SIN + C*SIN + C*SIN/details/likelihood approximation'),\n",
      " Node('/results/MC/C*SIN + C*SIN'),\n",
      " Node('/results/MC/C*SIN + C*SIN/loss'),\n",
      " Node('/results/MC/C*SIN + C*SIN/num_draws'),\n",
      " Node('/results/MC/C*SIN + C*SIN/details'),\n",
      " Node('/results/MC/C*SIN + C*SIN/details/Kernel code'),\n",
      " Node('/results/MC/C*SIN + C*SIN/details/seed'),\n",
      " Node('/results/MC/C*SIN + C*SIN/details/Likelihood time'),\n",
      " Node('/results/MC/C*SIN + C*SIN/details/Model compile time'),\n",
      " Node('/results/MC/C*SIN + C*SIN/details/Sampling time'),\n",
      " Node('/results/MC/C*SIN + C*SIN/details/Total time'),\n",
      " Node('/results/MC/C*SIN + C*SIN/details/Bad entries'),\n",
      " Node('/results/MC/C*SIN + C*SIN/details/likelihood approximation'),\n",
      " Node('/results/MC/C*SIN'),\n",
      " Node('/results/MC/C*SIN/loss'),\n",
      " Node('/results/MC/C*SIN/num_draws'),\n",
      " Node('/results/MC/C*SIN/details'),\n",
      " Node('/results/MC/C*SIN/details/Kernel code'),\n",
      " Node('/results/MC/C*SIN/details/seed'),\n",
      " Node('/results/MC/C*SIN/details/Likelihood time'),\n",
      " Node('/results/MC/C*SIN/details/Model compile time'),\n",
      " Node('/results/MC/C*SIN/details/Sampling time'),\n",
      " Node('/results/MC/C*SIN/details/Total time'),\n",
      " Node('/results/MC/C*SIN/details/Bad entries'),\n",
      " Node('/results/MC/C*SIN/details/likelihood approximation'))\n",
      "############################\n",
      "RBF_PER\n",
      "\n",
      "----\n",
      "4C*SIN:\n",
      "\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'Laplace'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_5844/877221558.py\u001b[0m in \u001b[0;36m<cell line: 12>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{model_kernel}:\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmetric\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"Laplace\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"MC\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"MLL\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"AIC\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{metric}\\t - \\t{main_dict['results'][metric][model_kernel]['loss']}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m: 'Laplace'"
     ]
    }
   ],
   "source": [
    "#naming_schema = [\"Metric\", \"Kernel_search\", \"train_data_ratio\", \"Data_kernel\", \"weights\", \"Variance_list\", \"eval_START\", \"eval_END\", \"eval_COUNT\", \"optimizer\", \"train_iterations\", \"LR\", \"Noise\", \"Data_scaling\", \"BFGS\"]\n",
    "all_results = load_results('results', \"hardcoded\")\n",
    "#pprint.pprint(all_results)\n",
    "result_tree = generate_tree_structure(all_results[0]).descendants\n",
    "pprint.pprint(result_tree)\n",
    "\n",
    "\n",
    "# data - kernel assignment : RBF_PER = SIN*RBF; 4PER = 4C*SIN; PER = C*SIN\n",
    "# Perform for each main dictionary: Iterate over kernels -> Look for the lowest(highest?) loss, \n",
    "\n",
    "\n",
    "for main_dict in all_results:\n",
    "    print(\"############################\")\n",
    "    print(main_dict[\"results\"][\"attributes\"][\"data_gen\"])\n",
    "    for model_kernel in ['4C*SIN','C*C*RBF','C*RBF','C*SIN','C*SIN + C*SIN','C*SIN + C*SIN + C*SIN','SIN*RBF']:\n",
    "        print(\"\\n----\")\n",
    "        print(f\"{model_kernel}:\\n\")\n",
    "        for metric in [\"Laplace\", \"MC\", \"MLL\", \"AIC\"]:\n",
    "            print(f\"{metric}\\t - \\t{main_dict['results'][metric][model_kernel]['loss']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e6c15cba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': tensor(156.2486, requires_grad=True), 'model parameters': [('likelihood.noise_covar.raw_noise', Parameter containing:\n",
      "tensor([-2.8824], requires_grad=True)), ('covar_module.kernels.0.raw_outputscale', Parameter containing:\n",
      "tensor(-0.3144, requires_grad=True)), ('covar_module.kernels.0.base_kernel.raw_lengthscale', Parameter containing:\n",
      "tensor([[0.3339]], requires_grad=True)), ('covar_module.kernels.0.base_kernel.raw_period_length', Parameter containing:\n",
      "tensor([[-0.1196]], requires_grad=True)), ('covar_module.kernels.1.raw_outputscale', Parameter containing:\n",
      "tensor(-2.3296, requires_grad=True)), ('covar_module.kernels.1.base_kernel.raw_lengthscale', Parameter containing:\n",
      "tensor([[-0.0876]], requires_grad=True)), ('covar_module.kernels.1.base_kernel.raw_period_length', Parameter containing:\n",
      "tensor([[-2.7317]], requires_grad=True)), ('covar_module.kernels.2.raw_outputscale', Parameter containing:\n",
      "tensor(0.6926, requires_grad=True)), ('covar_module.kernels.2.base_kernel.raw_lengthscale', Parameter containing:\n",
      "tensor([[2.5635]], requires_grad=True)), ('covar_module.kernels.2.base_kernel.raw_period_length', Parameter containing:\n",
      "tensor([[-0.8736]], requires_grad=True)), ('covar_module.kernels.3.raw_outputscale', Parameter containing:\n",
      "tensor(-0.2040, requires_grad=True)), ('covar_module.kernels.3.base_kernel.raw_lengthscale', Parameter containing:\n",
      "tensor([[2.9504]], requires_grad=True)), ('covar_module.kernels.3.base_kernel.raw_period_length', Parameter containing:\n",
      "tensor([[-0.9243]], requires_grad=True))]}\n",
      "{'loss': tensor(120.4090, requires_grad=True), 'model parameters': [('likelihood.noise_covar.raw_noise', Parameter containing:\n",
      "tensor([-2.9503], requires_grad=True)), ('covar_module.kernels.0.raw_lengthscale', Parameter containing:\n",
      "tensor([[-1.6816]], requires_grad=True)), ('covar_module.kernels.0.raw_period_length', Parameter containing:\n",
      "tensor([[0.5514]], requires_grad=True)), ('covar_module.kernels.1.raw_lengthscale', Parameter containing:\n",
      "tensor([[1.2599]], requires_grad=True))]}\n",
      "tensor(0.0793)\n",
      "tensor(0.6449)\n",
      "tensor(0.1906)\n",
      "tensor(2.4320)\n"
     ]
    }
   ],
   "source": [
    "print(deep_dict_get(all_results[0], '/results/MLL/4C*SIN'))\n",
    "print(deep_dict_get(all_results[0], '/results/MLL/SIN*RBF'))\n",
    "print(torch.nn.functional.softplus(torch.tensor(-2.494)))\n",
    "print(torch.nn.functional.softplus(torch.tensor(-0.099)))\n",
    "print(torch.nn.functional.softplus(torch.tensor(-1.561)))\n",
    "print(torch.nn.functional.softplus(torch.tensor(2.340)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d3ad0f5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.6449)\n",
      "tensor(0.0793)\n",
      "tensor(0.1906)\n",
      "tensor(2.4320)\n"
     ]
    }
   ],
   "source": [
    "deep_dict_get(all_results[0], '/results/MLL/4C*SIN')\n",
    "\"\"\"\n",
    "  ('covar_module.kernels.0.raw_outputscale',\n",
    "   Parameter containing:\n",
    "   tensor(-2.4943, requires_grad=True)),\n",
    "  \n",
    "  ('covar_module.kernels.1.raw_outputscale',\n",
    "   Parameter containing:\n",
    "   tensor(-0.0990, requires_grad=True)),\n",
    "  \n",
    "  ('covar_module.kernels.2.raw_outputscale',\n",
    "   Parameter containing:\n",
    "   tensor(-1.5614, requires_grad=True)),\n",
    "  \n",
    "  ('covar_module.kernels.3.raw_outputscale',\n",
    "   Parameter containing:\n",
    "   tensor(2.3405, requires_grad=True)),\n",
    "\"\"\"\n",
    "\n",
    "print(torch.nn.functional.softplus(torch.tensor(-0.099)))\n",
    "print(torch.nn.functional.softplus(torch.tensor(-2.494)))\n",
    "print(torch.nn.functional.softplus(torch.tensor(-1.561)))\n",
    "print(torch.nn.functional.softplus(torch.tensor(2.340)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6660c3d1",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Laplace'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_5844/1832475971.py\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#all_results[0][\"results\"][\"attributes\"][\"data_gen\"]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mpprint\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_results\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"results\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Laplace\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"4C*SIN\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"details\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mMLL\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mall_results\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"results\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Laplace\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"4C*SIN\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"details\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"MLL\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mH\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mall_results\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"results\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Laplace\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"4C*SIN\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"details\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"corrected Hessian\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_results\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"results\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Laplace\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"4C*SIN\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"details\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"diag(prior var)\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Laplace'"
     ]
    }
   ],
   "source": [
    "#all_results[0][\"results\"][\"attributes\"][\"data_gen\"]\n",
    "pprint.pprint(all_results[0][\"results\"][\"Laplace\"][\"4C*SIN\"][\"details\"])\n",
    "MLL = all_results[0][\"results\"][\"Laplace\"][\"4C*SIN\"][\"details\"][\"MLL\"]\n",
    "H = all_results[0][\"results\"][\"Laplace\"][\"4C*SIN\"][\"details\"][\"corrected Hessian\"]\n",
    "S = torch.diag(all_results[0][\"results\"][\"Laplace\"][\"4C*SIN\"][\"details\"][\"diag(prior var)\"])\n",
    "t_mu = all_results[0][\"results\"][\"Laplace\"][\"4C*SIN\"][\"details\"][\"prior mean\"]\n",
    "t_0 = all_results[0][\"results\"][\"Laplace\"][\"4C*SIN\"][\"details\"][\"parameter values\"]\n",
    "\n",
    "diff = t_mu - t_0\n",
    "mid_term = (S.inverse() - H).inverse()\n",
    "matmuls = diff.t() @ S.inverse() @ mid_term @ H @ diff\n",
    "print(matmuls)\n",
    "#print(torch.log(S.det()))\n",
    "print(MLL - 0.5*torch.log(S.det()) - 0.5*torch.log(mid_term.det()) + 0.5* matmuls)\n",
    "#pprint.pprint(list(zip(all_results[0][\"results\"][\"Laplace\"][\"4C*SIN\"][\"details\"]['parameter list'], all_results[0][\"results\"][\"Laplace\"][\"4C*SIN\"][\"details\"]['parameter values'])))\n",
    "\n",
    "#pprint.pprint(torch.linalg.eig(all_results[0][\"results\"][\"Laplace\"][\"4C*SIN\"][\"details\"][\"corrected Hessian\"]))\n",
    "#pprint.pprint(torch.linalg.eig(all_results[0][\"results\"][\"Laplace\"][\"4C*SIN\"][\"details\"][\"original symmetrized Hessian\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bd71d8a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38\n",
      "(Node('/results/results'),\n",
      " Node('/results/results/Kernel search time'),\n",
      " Node('/results/results/details'),\n",
      " Node('/results/results/model history'),\n",
      " Node('/results/results/performance history'),\n",
      " Node('/results/results/loss history'),\n",
      " Node('/results/results/final model'),\n",
      " Node('/results/results/parameters'),\n",
      " Node('/results/results/parameters/likelihood.noise_covar.raw_noise'),\n",
      " Node('/results/results/parameters/covar_module.raw_outputscale'),\n",
      " Node('/results/results/parameters/covar_module.base_kernel.raw_lengthscale'),\n",
      " Node('/results/attributes'),\n",
      " Node('/results/attributes/Kernel_search'),\n",
      " Node('/results/attributes/train_data_ratio'),\n",
      " Node('/results/attributes/Data_kernel'),\n",
      " Node('/results/attributes/weights'),\n",
      " Node('/results/attributes/Variance_list'),\n",
      " Node('/results/attributes/eval_START'),\n",
      " Node('/results/attributes/eval_END'),\n",
      " Node('/results/attributes/eval_COUNT'),\n",
      " Node('/results/attributes/optimizer'),\n",
      " Node('/results/attributes/train_iterations'),\n",
      " Node('/results/attributes/LR'),\n",
      " Node('/results/attributes/Noise'),\n",
      " Node('/results/attributes/Data_scaling'),\n",
      " Node('/results/attributes/BFGS'),\n",
      " Node('/results/attributes/Metric'),\n",
      " Node('/results/attributes/parameter_punishment'),\n",
      " Node('/results/attributes/experiment name'))\n",
      "['(c * SE)', '((c * PER) * (c * SE))', '(c * SE)', '(c * LIN)', '((c * SE) * (c * PER))', '(c * SE)', '((c * SE) + (c * PER))', '(c * PER)', '(c * LIN)', '(c * PER)', '((c * PER) * (c * PER))', '(c * PER)', '((c * PER) + (c * PER))', '(((c * PER) * (c * SE)) * (c * PER))', '((c * PER) + (c * SE))', '(c * PER)', '(((c * SE) + (c * SE)) * (c * PER))', '((c * SE) * (c * PER))', '(c * PER)', '(c * SE)', '(c * SE)', '(c * SE)', '(c * SE)', '(c * LIN)', '(c * SE)', '(((c * LIN) + (c * SE)) * (c * PER))', '((c * PER) * (c * PER))', '(((c * SE) * (c * PER)) * (c * PER))', '(c * SE)', '(c * LIN)', '((c * SE) * ((c * SE) * (c * PER)))', '((c * SE) * ((c * PER) * (c * SE)))', '(c * SE)', '(c * SE)', '(c * SE)', '(c * SE)', '((c * LIN) + (c * PER))', '(c * SE)']\n",
      "\n",
      "########## Laplace ##########\n",
      "\n",
      "Percentage of exactly correct model: 13.157894736842104\n",
      "Percentage of correct component: 20\n",
      "\n",
      "##############\n",
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not str",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_362024/2613457015.py\u001b[0m in \u001b[0;36m<cell line: 20>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Percentage of correct component: {np.sum(['PER' in m for m in all_final_models])}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n##############\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mLaplace_runtimes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfilter_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_results\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'/results/results/details/Total time'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;31m#training_runtimes = filter_value(all_results, '/results/results/Training time')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;31m#KS_runtimes = filter_value(all_results, '/results/results/Kernel search time')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_362024/3131956802.py\u001b[0m in \u001b[0;36mfilter_value\u001b[0;34m(list_of_data, path)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mlist_of_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlist_of_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0mlist_of_values\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdeep_dict_get\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mlist_of_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_362024/2957879044.py\u001b[0m in \u001b[0;36mdeep_dict_get\u001b[0;34m(data, path)\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mentry\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m             \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mtemp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtemp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mentry\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtemp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: list indices must be integers or slices, not str"
     ]
    }
   ],
   "source": [
    "#naming_schema = [\"Metric\", \"Kernel_search\", \"train_data_ratio\", \"Data_kernel\", \"weights\", \"Variance_list\", \"eval_START\", \"eval_END\", \"eval_COUNT\", \"optimizer\", \"train_iterations\", \"LR\", \"Noise\", \"Data_scaling\", \"BFGS\"]\n",
    "all_results = load_results('results', \"Laplace\")\n",
    "print(len(all_results))\n",
    "result_tree = generate_tree_structure(all_results[0]).descendants\n",
    "pprint.pprint(result_tree)\n",
    "\n",
    "print(filter_value(all_results, '/results/results/final model'))\n",
    "\n",
    "#########\n",
    "\"\"\"\n",
    "    Looking at the final models that were found and the ratio of exactly the correct one\n",
    "\"\"\"\n",
    "#########\n",
    "print(\"\\n########## Laplace ##########\\n\")\n",
    "all_final_models = filter_value(all_results, '/results/results/final model')\n",
    "winners = group_by(all_results, 'results/results/final model', value='(c * PER)')['results/results/final model = (c * PER)']\n",
    "print(f\"Percentage of exactly correct model: {len(winners)/len(all_final_models)*100}\")\n",
    "print(f\"Percentage of correct component: {np.sum(['PER' in m for m in all_final_models])}\")\n",
    "print(\"\\n##############\\n\")\n",
    "Laplace_runtimes = filter_value(all_results, '/results/results/details/Total time')\n",
    "#training_runtimes = filter_value(all_results, '/results/results/Training time')\n",
    "#KS_runtimes = filter_value(all_results, '/results/results/Kernel search time')\n",
    "print(f\"Average Laplace runtime: {np.mean(Laplace_runtimes)}\")\n",
    "#print(f\"Average Laplace runtime: {np.mean(KS_runtimes)}\")\n",
    "\n",
    "print(\"\\n#############################\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2d2000be",
   "metadata": {},
   "source": [
    "#naming_schema = [\"Metric\", \"Kernel_search\", \"train_data_ratio\", \"Data_kernel\", \"weights\", \"Variance_list\", \"eval_START\", \"eval_END\", \"eval_COUNT\", \"optimizer\", \"train_iterations\", \"LR\", \"Noise\", \"Data_scaling\", \"BFGS\"]\n",
    "all_results = load_results('results', \"MC\")\n",
    "result_tree = generate_tree_structure(all_results[0]).descendants\n",
    "pprint.pprint(result_tree)\n",
    "\n",
    "\n",
    "#########\n",
    "\"\"\"\n",
    "    Looking at the final models that were found and the ratio of exactly the correct one\n",
    "\"\"\"\n",
    "#########\n",
    "print(\"\\n########## MCMC ##########\\n\")\n",
    "all_final_models = filter_value(all_results, '/results/results/final model')\n",
    "winners = group_by(all_results, 'results/results/final model', value='(c * PER)')['results/results/final model = (c * PER)']\n",
    "print(f\"Percentage of exactly correct model: {len(winners)/len(all_final_models)*100}\")\n",
    "print(f\"Percentage of correct component: {np.sum(['PER' in m for m in all_final_models])}\")\n",
    "print(\"\\n#############################\")\n",
    "\n",
    "filter_value(all_results, '/results/results/details/Sampling time')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "931e5497",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "379ead81",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "771c1a34",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
