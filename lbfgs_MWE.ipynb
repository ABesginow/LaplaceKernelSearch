{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import sys\n",
    "sys.path.insert(1, \"..\")\n",
    "import pprint\n",
    "import gpytorch\n",
    "import torch\n",
    "import numpy as np\n",
    "import metrics\n",
    "import copy\n",
    "import configparser\n",
    "from experiment_functions import Experiment\n",
    "from GaussianProcess import ExactGPModel\n",
    "from globalParams import options, hyperparameter_limits\n",
    "import gpytorch\n",
    "from helpFunctions import get_string_representation_of_kernel as gsr\n",
    "from helpFunctions import clean_kernel_expression\n",
    "from helpFunctions import get_kernels_in_kernel_expression, get_full_kernels_in_kernel_expression\n",
    "from helpFunctions import amount_of_base_kernels\n",
    "from itertools import product\n",
    "import json\n",
    "from kernelSearch import *\n",
    "from matplotlib import pyplot as plt\n",
    "from metrics import *\n",
    "from multiprocessing import Pool\n",
    "import numpy as np\n",
    "import os\n",
    "import pdb\n",
    "import pickle\n",
    "import random\n",
    "import tikzplotlib\n",
    "import time\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from scipy.stats import multivariate_normal\n",
    "import matplotlib as mpl\n",
    "\n",
    "# To run STAN in a Jupyter notebook\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import gpytorch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the training data\n",
    "train_x = torch.linspace(0, 1, 10)\n",
    "train_y = torch.sin(train_x * (2 * 3.1416))\n",
    "\n",
    "# Define the Gaussian Process model\n",
    "class GPModel(gpytorch.models.ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood):\n",
    "        super(GPModel, self).__init__(train_x, train_y, likelihood)\n",
    "        self.mean_module = gpytorch.means.ZeroMean()\n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel())\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "\n",
    "# Initialize the likelihood and model\n",
    "likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "model = GPModel(train_x, train_y, likelihood)\n",
    "\n",
    "\n",
    "\n",
    "# Make predictions with the model\n",
    "model.eval()\n",
    "likelihood.eval()\n",
    "with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "    test_x = torch.linspace(0, 1, 51)\n",
    "    observed_pred = likelihood(model(test_x))\n",
    "\n",
    "# Plot the results\n",
    "with torch.no_grad():\n",
    "    mean = observed_pred.mean\n",
    "    lower, upper = observed_pred.confidence_region()\n",
    "plt.plot(train_x.numpy(), train_y.numpy(), 'k*')\n",
    "plt.plot(test_x.numpy(), mean.numpy(), 'b')\n",
    "plt.fill_between(test_x.numpy(), lower.numpy(), upper.numpy(), alpha=0.5)\n",
    "plt.ylim([-3, 3])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_prior(model, theta_mu=None, sigma=None):\n",
    "    # params -\n",
    "    # TODO de-spaghettize this once the priors are coded properly\n",
    "    prior_dict = {'SE': {'raw_lengthscale' : {\"mean\": -0.21221139138922668 , \"std\":1.8895426067756804}},\n",
    "                  'MAT52': {'raw_lengthscale' :{\"mean\": 0.7993038925994188, \"std\":2.145122566357853 } },\n",
    "                  'MAT32': {'raw_lengthscale' :{\"mean\": 1.5711054238673443, \"std\":2.4453761235991216 } },\n",
    "                  'RQ': {'raw_lengthscale' :{\"mean\": -0.049841950913676276, \"std\":1.9426354614713097 },\n",
    "                          'raw_alpha' :{\"mean\": 1.882148553921053, \"std\":3.096431944989054 } },\n",
    "                  'PER':{'raw_lengthscale':{\"mean\": 0.7778461197268618, \"std\":2.288946656544974 },\n",
    "                          'raw_period_length':{\"mean\": 0.6485334993738499, \"std\":0.9930632050553377 } },\n",
    "                  'LIN':{'raw_variance' :{\"mean\": -0.8017903983055685, \"std\":0.9966569921354465 } },\n",
    "                  'c':{'raw_outputscale':{\"mean\": -1.6253091096349706, \"std\":2.2570021716661923 } },\n",
    "                  'noise': {'raw_noise':{\"mean\": -3.51640656386717, \"std\":3.5831320474767407 }}}\n",
    "    #prior_dict = {\"SE\": {\"raw_lengthscale\": {\"mean\": 0.891, \"std\": 2.195}},\n",
    "    #              \"MAT\": {\"raw_lengthscale\": {\"mean\": 1.631, \"std\": 2.554}},\n",
    "    #              \"PER\": {\"raw_lengthscale\": {\"mean\": 0.338, \"std\": 2.636},\n",
    "    #                      \"raw_period_length\": {\"mean\": 0.284, \"std\": 0.902}},\n",
    "    #              \"LIN\": {\"raw_variance\": {\"mean\": -1.463, \"std\": 1.633}},\n",
    "    #              \"c\": {\"raw_outputscale\": {\"mean\": -2.163, \"std\": 2.448}},\n",
    "    #              \"noise\": {\"raw_noise\": {\"mean\": -1.792, \"std\": 3.266}}}\n",
    "\n",
    "    variances_list = list()\n",
    "    debug_param_name_list = list()\n",
    "    theta_mu = list()\n",
    "    params = list()\n",
    "    covar_string = gsr(model.covar_module)\n",
    "    covar_string = covar_string.replace(\"(\", \"\")\n",
    "    covar_string = covar_string.replace(\")\", \"\")\n",
    "    covar_string = covar_string.replace(\" \", \"\")\n",
    "    covar_string = covar_string.replace(\"PER\", \"PER+PER\")\n",
    "    covar_string_list = [s.split(\"*\") for s in covar_string.split(\"+\")]\n",
    "    covar_string_list.insert(0, [\"LIKELIHOOD\"])\n",
    "    covar_string_list = list(chain.from_iterable(covar_string_list))\n",
    "    both_PER_params = False\n",
    "    for (param_name, param), cov_str in zip(model.named_parameters(), covar_string_list):\n",
    "        params.append(param.item())\n",
    "        debug_param_name_list.append(param_name)\n",
    "        # First param is (always?) noise and is always with the likelihood\n",
    "        if \"likelihood\" in param_name:\n",
    "            theta_mu.append(prior_dict[\"noise\"][\"raw_noise\"][\"mean\"])\n",
    "            variances_list.append(prior_dict[\"noise\"][\"raw_noise\"][\"std\"])\n",
    "            continue\n",
    "        else:\n",
    "            if (cov_str == \"PER\" or cov_str == \"RQ\") and not both_PER_params:\n",
    "                theta_mu.append(prior_dict[cov_str][param_name.split(\".\")[-1]][\"mean\"])\n",
    "                variances_list.append(prior_dict[cov_str][param_name.split(\".\")[-1]][\"std\"])\n",
    "                both_PER_params = True\n",
    "            elif (cov_str == \"PER\" or cov_str == \"RQ\") and both_PER_params:\n",
    "                theta_mu.append(prior_dict[cov_str][param_name.split(\".\")[-1]][\"mean\"])\n",
    "                variances_list.append(prior_dict[cov_str][param_name.split(\".\")[-1]][\"std\"])\n",
    "                both_PER_params = False\n",
    "            else:\n",
    "                try:\n",
    "                    theta_mu.append(prior_dict[cov_str][param_name.split(\".\")[-1]][\"mean\"])\n",
    "                    variances_list.append(prior_dict[cov_str][param_name.split(\".\")[-1]][\"std\"])\n",
    "                except Exception as E:\n",
    "                    import pdb\n",
    "                    pdb.set_trace()\n",
    "                    prev_cov = cov_str\n",
    "    theta_mu = torch.tensor(theta_mu)\n",
    "    theta_mu = theta_mu.unsqueeze(0).t()\n",
    "    sigma = torch.diag(torch.Tensor(variances_list))\n",
    "    sigma = sigma@sigma\n",
    "    prior = torch.distributions.MultivariateNormal(theta_mu.t(), sigma)\n",
    "\n",
    "    # for convention reasons I'm diving by the number of datapoints\n",
    "    return prior.log_prob(torch.Tensor(params)).item() / len(*model.train_inputs)\n",
    "\n",
    "def optimize_hyperparameters(model, likelihood, train_iterations, X, Y, with_BFGS=False, MAP=False, prior=None, **kwargs):\n",
    "    \"\"\"\n",
    "    find optimal hyperparameters either by BO or by starting from random initial values multiple times, using an optimizer every time\n",
    "    and then returning the best result\n",
    "    \"\"\"\n",
    "    ## Setup\n",
    "    # Log the parameters found during training\n",
    "    log_param_path = kwargs.get(\"log_param_path\", False)\n",
    "    log_likelihood = kwargs.get(\"log_likelihood\", False)\n",
    "    random_restarts = kwargs.get(\"random_restarts\", options[\"training\"][\"restarts\"]+1)\n",
    "    best_loss = 1e400\n",
    "    optimal_parameters = dict()\n",
    "    limits = hyperparameter_limits\n",
    "    if log_param_path:\n",
    "        param_log_list = list()\n",
    "    if log_likelihood:\n",
    "        likelihood_logs = list()\n",
    "    # start runs\n",
    "    for iteration in range(random_restarts):\n",
    "    #for iteration in range(2):\n",
    "        if log_likelihood:\n",
    "            likelihood_log = list()\n",
    "        if log_param_path:\n",
    "            param_log_dict = {param_name[0] : list() for param_name in model.named_parameters()}\n",
    "        # optimize and determine loss\n",
    "        # Perform a training for AIC and Laplace\n",
    "        model.train()\n",
    "        likelihood.train()\n",
    "\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=0.1)\n",
    "\n",
    "        mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "\n",
    "        for i in range(train_iterations):\n",
    "            # Zero gradients from previous iteration\n",
    "            optimizer.zero_grad()\n",
    "            # Output from model\n",
    "            output = model(X)\n",
    "            # Calc loss and backprop gradients\n",
    "            loss = -mll(output, Y)\n",
    "            if MAP:\n",
    "                log_p = log_prior(model)\n",
    "                loss -= log_p\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if log_param_path:\n",
    "                for param_name in model.named_parameters():\n",
    "                    param_log_dict[param_name[0]].append(param_name[1].item())\n",
    "            if log_likelihood:\n",
    "                likelihood_log.append(loss.item())\n",
    "\n",
    "        if with_BFGS:\n",
    "            # Additional BFGS optimization to better ensure optimal parameters\n",
    "            # LBFGS_optimizer = torch.optim.LBFGS(model.parameters(), max_iter=50, line_search_fn='strong_wolfe')\n",
    "            LBFGS_optimizer = torch.optim.LBFGS(\n",
    "                model.parameters(), max_iter=500, max_eval=10000, tolerance_grad=1e-1, tolerance_change=1e-1,\n",
    "                line_search_fn='strong_wolfe')\n",
    "            # define closure\n",
    "            training_iterations=50\n",
    "            def closure():\n",
    "                LBFGS_optimizer.zero_grad()\n",
    "                output = model(X)\n",
    "                loss = -mll(output, Y)\n",
    "                if MAP:\n",
    "                    log_p = log_prior(model)\n",
    "                    loss -= log_p\n",
    "                loss.backward()\n",
    "                if log_param_path:\n",
    "                    for param_name in model.named_parameters():\n",
    "                        param_log_dict[param_name[0]].append(param_name[1].item())\n",
    "                if log_likelihood:\n",
    "                    likelihood_log.append(loss.item())\n",
    "                return loss\n",
    "            LBFGS_optimizer.step(closure)\n",
    "\n",
    "        # Zero gradients from previous iteration\n",
    "        optimizer.zero_grad()\n",
    "        # Output from model\n",
    "        output = model(X)\n",
    "        # Calc loss and backprop gradients\n",
    "        loss = -mll(output, Y)\n",
    "        if MAP:\n",
    "            log_p = log_prior(model)\n",
    "            loss -= log_p\n",
    "\n",
    "#        model.train_model(with_BFGS=with_BFGS)\n",
    "        current_loss = loss\n",
    "        # check if the current run is better than previous runs\n",
    "        if current_loss < best_loss:\n",
    "            # if it is the best, save all used parameters\n",
    "            best_loss = current_loss\n",
    "            for param_name, param in model.named_parameters():\n",
    "                optimal_parameters[param_name] = copy.deepcopy(param)\n",
    "\n",
    "        # set new random inital values\n",
    "        model.likelihood.noise_covar.noise = torch.rand(1) * (limits[\"Noise\"][1] - limits[\"Noise\"][0]) + limits[\"Noise\"][0]\n",
    "        #self.mean_module.constant = torch.rand(1) * (limits[\"Mean\"][1] - limits[\"Mean\"][0]) + limits[\"Mean\"][0]\n",
    "        for kernel in get_kernels_in_kernel_expression(model.covar_module):\n",
    "            hypers = limits[kernel._get_name()]\n",
    "            for hyperparameter in hypers:\n",
    "                new_value = torch.rand(1) * (hypers[hyperparameter][1] - hypers[hyperparameter][0]) + hypers[hyperparameter][0]\n",
    "                setattr(kernel, hyperparameter, new_value)\n",
    "\n",
    "        # print output if enabled\n",
    "        if options[\"training\"][\"print_optimizing_output\"]:\n",
    "            print(f\"HYPERPARAMETER OPTIMIZATION: Random Restart {iteration}: loss: {current_loss}, optimal loss: {best_loss}\")\n",
    "        if log_likelihood:\n",
    "            likelihood_logs.append(likelihood_log)\n",
    "        if log_param_path:\n",
    "            param_log_list.append(param_log_dict)\n",
    "\n",
    "    # finally, set the hyperparameters those in the optimal run\n",
    "    model.initialize(**optimal_parameters)\n",
    "    output = model(X)\n",
    "    mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "    loss = -mll(output, Y)\n",
    "    if MAP:\n",
    "        log_p = log_prior(model)\n",
    "        loss -= log_p\n",
    "    if not loss == best_loss:\n",
    "        import pdb\n",
    "        pdb.set_trace()\n",
    "        print(loss)\n",
    "        print(best_loss)\n",
    "    logables = dict()\n",
    "    if log_param_path:\n",
    "        logables[\"training_log\"] = param_log_dict\n",
    "    if log_likelihood:\n",
    "        logables[\"likelihood_log\"] = likelihood_log\n",
    "    if len(logables.keys()) > 0:\n",
    "        return loss, logables\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "num_train_iter = 1\n",
    "use_MAP = False \n",
    "use_LBFGS = True\n",
    "# Train the MAP for 100 Iterations of ADAM and then 50 more of L-BFGS\n",
    "loss, training_log = optimize_hyperparameters(model, likelihood, num_train_iter, train_x, train_y, use_LBFGS, MAP=use_MAP, log_param_path=True, random_restarts=1, log_likelihood=True)\n",
    "training_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.autograd.grad(loss, [p for p in model.parameters()], retain_graph=True, create_graph=True, allow_unused=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Make predictions with the model\n",
    "model.eval()\n",
    "likelihood.eval()\n",
    "with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "    test_x = torch.linspace(0, 1, 51)\n",
    "    observed_pred = likelihood(model(test_x))\n",
    "\n",
    "# Plot the results\n",
    "with torch.no_grad():\n",
    "    mean = observed_pred.mean\n",
    "    lower, upper = observed_pred.confidence_region()\n",
    "plt.plot(train_x.numpy(), train_y.numpy(), 'k*')\n",
    "plt.plot(test_x.numpy(), mean.numpy(), 'b')\n",
    "plt.fill_between(test_x.numpy(), lower.numpy(), upper.numpy(), alpha=0.5)\n",
    "plt.ylim([-3, 3])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_prior(model, theta_mu=None, sigma=None):\n",
    "    # params -\n",
    "    # TODO de-spaghettize this once the priors are coded properly\n",
    "    prior_dict = {'SE': {'raw_lengthscale' : {\"mean\": -0.21221139138922668 , \"std\":1.8895426067756804}},\n",
    "                  'MAT52': {'raw_lengthscale' :{\"mean\": 0.7993038925994188, \"std\":2.145122566357853 } },\n",
    "                  'MAT32': {'raw_lengthscale' :{\"mean\": 1.5711054238673443, \"std\":2.4453761235991216 } },\n",
    "                  'RQ': {'raw_lengthscale' :{\"mean\": -0.049841950913676276, \"std\":1.9426354614713097 },\n",
    "                          'raw_alpha' :{\"mean\": 1.882148553921053, \"std\":3.096431944989054 } },\n",
    "                  'PER':{'raw_lengthscale':{\"mean\": 0.7778461197268618, \"std\":2.288946656544974 },\n",
    "                          'raw_period_length':{\"mean\": 0.6485334993738499, \"std\":0.9930632050553377 } },\n",
    "                  'LIN':{'raw_variance' :{\"mean\": -0.8017903983055685, \"std\":0.9966569921354465 } },\n",
    "                  'c':{'raw_outputscale':{\"mean\": -1.6253091096349706, \"std\":2.2570021716661923 } },\n",
    "                  'noise': {'raw_noise':{\"mean\": -3.51640656386717, \"std\":3.5831320474767407 }}}\n",
    "    #prior_dict = {\"SE\": {\"raw_lengthscale\": {\"mean\": 0.891, \"std\": 2.195}},\n",
    "    #              \"MAT\": {\"raw_lengthscale\": {\"mean\": 1.631, \"std\": 2.554}},\n",
    "    #              \"PER\": {\"raw_lengthscale\": {\"mean\": 0.338, \"std\": 2.636},\n",
    "    #                      \"raw_period_length\": {\"mean\": 0.284, \"std\": 0.902}},\n",
    "    #              \"LIN\": {\"raw_variance\": {\"mean\": -1.463, \"std\": 1.633}},\n",
    "    #              \"c\": {\"raw_outputscale\": {\"mean\": -2.163, \"std\": 2.448}},\n",
    "    #              \"noise\": {\"raw_noise\": {\"mean\": -1.792, \"std\": 3.266}}}\n",
    "\n",
    "    variances_list = list()\n",
    "    debug_param_name_list = list()\n",
    "    theta_mu = list()\n",
    "    params = list()\n",
    "    covar_string = gsr(model.covar_module)\n",
    "    covar_string = covar_string.replace(\"(\", \"\")\n",
    "    covar_string = covar_string.replace(\")\", \"\")\n",
    "    covar_string = covar_string.replace(\" \", \"\")\n",
    "    covar_string = covar_string.replace(\"PER\", \"PER+PER\")\n",
    "    covar_string_list = [s.split(\"*\") for s in covar_string.split(\"+\")]\n",
    "    covar_string_list.insert(0, [\"LIKELIHOOD\"])\n",
    "    covar_string_list = list(chain.from_iterable(covar_string_list))\n",
    "    both_PER_params = False\n",
    "    for (param_name, param), cov_str in zip(model.named_parameters(), covar_string_list):\n",
    "        params.append(param.item())\n",
    "        debug_param_name_list.append(param_name)\n",
    "        # First param is (always?) noise and is always with the likelihood\n",
    "        if \"likelihood\" in param_name:\n",
    "            theta_mu.append(prior_dict[\"noise\"][\"raw_noise\"][\"mean\"])\n",
    "            variances_list.append(prior_dict[\"noise\"][\"raw_noise\"][\"std\"])\n",
    "            continue\n",
    "        else:\n",
    "            if (cov_str == \"PER\" or cov_str == \"RQ\") and not both_PER_params:\n",
    "                theta_mu.append(prior_dict[cov_str][param_name.split(\".\")[-1]][\"mean\"])\n",
    "                variances_list.append(prior_dict[cov_str][param_name.split(\".\")[-1]][\"std\"])\n",
    "                both_PER_params = True\n",
    "            elif (cov_str == \"PER\" or cov_str == \"RQ\") and both_PER_params:\n",
    "                theta_mu.append(prior_dict[cov_str][param_name.split(\".\")[-1]][\"mean\"])\n",
    "                variances_list.append(prior_dict[cov_str][param_name.split(\".\")[-1]][\"std\"])\n",
    "                both_PER_params = False\n",
    "            else:\n",
    "                try:\n",
    "                    theta_mu.append(prior_dict[cov_str][param_name.split(\".\")[-1]][\"mean\"])\n",
    "                    variances_list.append(prior_dict[cov_str][param_name.split(\".\")[-1]][\"std\"])\n",
    "                except Exception as E:\n",
    "                    import pdb\n",
    "                    pdb.set_trace()\n",
    "                    prev_cov = cov_str\n",
    "    theta_mu = torch.tensor(theta_mu)\n",
    "    theta_mu = theta_mu.unsqueeze(0).t()\n",
    "    sigma = torch.diag(torch.Tensor(variances_list))\n",
    "    sigma = sigma@sigma\n",
    "    prior = torch.distributions.MultivariateNormal(theta_mu.t(), sigma)\n",
    "\n",
    "    # for convention reasons I'm diving by the number of datapoints\n",
    "    return prior.log_prob(torch.Tensor(params)).item() / len(*model.train_inputs)\n",
    "\n",
    "\n",
    "def random_reinit(model):\n",
    "    for i, (param, limit) in enumerate(zip(model.parameters(), [{\"Noise\": hyperparameter_limits[\"Noise\"]},*[hyperparameter_limits[kernel] for kernel in get_full_kernels_in_kernel_expression(model.covar_module)]])):\n",
    "        covar_text = gsr(model.covar_module)\n",
    "        param_name = list(limit.keys())[0]\n",
    "        new_param_value = torch.randn_like(param) * (limit[param_name][1] - limit[param_name][0]) + limit[param_name][0]\n",
    "        param.data = new_param_value\n",
    "\n",
    "def optimize_hyperparameters(model, likelihood, **kwargs):\n",
    "    \"\"\"\n",
    "    find optimal hyperparameters either by BO or by starting from random initial values multiple times, using an optimizer every time\n",
    "    and then returning the best result\n",
    "    \"\"\"\n",
    "    log_param_path = kwargs.get(\"log_param_path\", False)\n",
    "    log_likelihood = kwargs.get(\"log_likelihood\", False)\n",
    "    random_restarts = kwargs.get(\"random_restarts\", options[\"training\"][\"restarts\"]+1)\n",
    "    line_search = kwargs.get(\"line_search\", False)\n",
    "    BFGS_iter = kwargs.get(\"BFGS_iter\", 50)\n",
    "    train_iterations = kwargs.get(\"train_iterations\", 0)\n",
    "    X = kwargs.get(\"X\", model.train_inputs)\n",
    "    Y = kwargs.get(\"Y\", model.train_targets)\n",
    "    with_BFGS = kwargs.get(\"with_BFGS\", True)\n",
    "    MAP = kwargs.get(\"MAP\", True)\n",
    "    prior = kwargs.get(\"prior\", False)\n",
    "\n",
    "    if log_likelihood:\n",
    "        likelihood_log = list()\n",
    "    if log_param_path:\n",
    "        param_log_dict = {param_name[0] : list() for param_name in model.named_parameters()}\n",
    "\n",
    "    best_loss = float('inf')\n",
    "    best_model_state_dict = None\n",
    "    best_likelihood_state_dict = None\n",
    "\n",
    "    mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "\n",
    "    for _ in range(random_restarts):\n",
    "        try:\n",
    "            optimizer = torch.optim.Adam(model.parameters(), lr=0.1)\n",
    "            mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "            # Train the ADAM part\n",
    "            for _ in range(train_iterations):\n",
    "                optimizer.zero_grad()\n",
    "                output = model(train_x)\n",
    "                loss = -mll(output, train_y)\n",
    "                if MAP:\n",
    "                    log_p = log_prior(model)\n",
    "                    loss -= log_p\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                if log_param_path:\n",
    "                    for param_name in model.named_parameters():\n",
    "                        param_log_dict[param_name[0]].append(param_name[1].item())\n",
    "                if log_likelihood:\n",
    "                    likelihood_log.append(loss.item())\n",
    "            # Train the L-BFGS part\n",
    "            optimizer = torch.optim.LBFGS(model.parameters(), max_iter=BFGS_iter, line_search_fn=None if line_search else \"strong_wolfe\")\n",
    "            def closure():\n",
    "                optimizer.zero_grad()\n",
    "                output = model(train_x)\n",
    "                loss = -mll(output, train_y)\n",
    "                if MAP:\n",
    "                    log_p = log_prior(model)\n",
    "                    loss -= log_p\n",
    "                loss.backward()\n",
    "                if log_param_path:\n",
    "                    for param_name in model.named_parameters():\n",
    "                        param_log_dict[param_name[0]].append(param_name[1].item())\n",
    "                if log_likelihood:\n",
    "                    likelihood_log.append(loss.item())\n",
    "                    \n",
    "                return loss\n",
    "            loss = optimizer.step(closure)\n",
    "            if loss < best_loss:\n",
    "                best_loss = loss\n",
    "                best_model_state_dict = model.state_dict()\n",
    "                best_likelihood_state_dict = likelihood.state_dict()\n",
    "        except Exception as E:\n",
    "            pass \n",
    "        random_reinit(model)\n",
    "    model.load_state_dict(best_model_state_dict)\n",
    "    likelihood.load_state_dict(best_likelihood_state_dict)\n",
    "\n",
    "    # Zero gradients from previous iteration\n",
    "    optimizer.zero_grad()\n",
    "    # Output from model\n",
    "    output = model(train_x)\n",
    "    # Calc loss and backprop gradients\n",
    "    loss = -mll(output, train_y)\n",
    "    if MAP:\n",
    "        log_p = log_prior(model)\n",
    "        loss -= log_p\n",
    "    return loss, model, likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('likelihood.noise_covar.raw_noise',\n",
       "  Parameter containing:\n",
       "  tensor([-10.9333], requires_grad=True)),\n",
       " ('covar_module.raw_outputscale',\n",
       "  Parameter containing:\n",
       "  tensor(-0.4848, requires_grad=True)),\n",
       " ('covar_module.base_kernel.raw_lengthscale',\n",
       "  Parameter containing:\n",
       "  tensor([[-1.6634]], requires_grad=True))]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Train the model using Adam\n",
    "#model.train()\n",
    "#likelihood.train()\n",
    "#optimizer = torch.optim.Adam(model.parameters(), lr=0.1)\n",
    "#mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "#training_iterations = 50\n",
    "#for i in range(training_iterations):\n",
    "#    optimizer.zero_grad()\n",
    "#    output = model(train_x)\n",
    "#    loss = -mll(output, train_y)\n",
    "#    loss.backward()\n",
    "#    optimizer.step()\n",
    "\n",
    "# Train the model using LBFGS\n",
    "loss, model, likelihood = optimize_hyperparameters(model, likelihood, train_iterations=0, MAP=True, X=train_x, Y=train_y, random_restarts=1)\n",
    "\n",
    "list(model.named_parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('likelihood.noise_covar.raw_noise',\n",
       "  Parameter containing:\n",
       "  tensor([-5.2447], requires_grad=True)),\n",
       " ('covar_module.raw_outputscale',\n",
       "  Parameter containing:\n",
       "  tensor(-0.4889, requires_grad=True)),\n",
       " ('covar_module.base_kernel.raw_lengthscale',\n",
       "  Parameter containing:\n",
       "  tensor([[-1.6424]], requires_grad=True))]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.2568], grad_fn=<SoftplusBackwardBackward0>),\n",
       " tensor(-0.1955, grad_fn=<AddBackward0>),\n",
       " tensor([[-0.0332]], grad_fn=<AddBackward0>))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.autograd.grad(loss, [p for p in model.parameters()], retain_graph=True, create_graph=True, allow_unused=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_reinit(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([0.0510], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor(11.3643, requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[-0.2365]], requires_grad=True)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(model.parameters())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.optim import LBFGS, Adam\n",
    "\n",
    "def random_reinit(model):\n",
    "    for i, (param, limit) in enumerate(zip(model.parameters(), [{\"Noise\": hyperparameter_limits[\"Noise\"]},*[hyperparameter_limits[kernel] for kernel in get_full_kernels_in_kernel_expression(model.covar_module)]])):\n",
    "        covar_text = gsr(model.covar_module)\n",
    "        param_name = list(limit.keys())[0]\n",
    "        new_param_value = torch.randn_like(param) * (limit[param_name][1] - limit[param_name][0]) + limit[param_name][0]\n",
    "        param.data = new_param_value\n",
    "\n",
    "def train_model(model, likelihood, train_x, train_y, train_iterations, random_restart):\n",
    "    best_loss = float('inf')\n",
    "    best_model_state_dict = None\n",
    "    best_likelihood_state_dict = None\n",
    "    for _ in range(random_restart):\n",
    "        random_reinit(model)\n",
    "        optimizer = Adam(model.parameters(), lr=0.1)\n",
    "        mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "        for j in range(train_iterations):\n",
    "            optimizer.zero_grad()\n",
    "            output = model(train_x)\n",
    "            loss = -mll(output, train_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        optimizer = LBFGS(model.parameters(), line_search_fn=None)\n",
    "        def closure():\n",
    "            optimizer.zero_grad()\n",
    "            output = model(train_x)\n",
    "            loss = -mll(output, train_y)\n",
    "            loss.backward()\n",
    "            return loss\n",
    "        loss = optimizer.step(closure)\n",
    "        if loss < best_loss:\n",
    "            best_loss = loss\n",
    "            best_model_state_dict = model.state_dict()\n",
    "            best_likelihood_state_dict = likelihood.state_dict()\n",
    "    model.load_state_dict(best_model_state_dict)\n",
    "    likelihood.load_state_dict(best_likelihood_state_dict)\n",
    "    return model, likelihood\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/besginow/anaconda3/envs/sage/lib/python3.10/site-packages/linear_operator/utils/cholesky.py:40: NumericalWarning: A not p.d., added jitter of 1.0e-06 to the diagonal\n",
      "  warnings.warn(\n",
      "/home/besginow/anaconda3/envs/sage/lib/python3.10/site-packages/linear_operator/utils/cholesky.py:40: NumericalWarning: A not p.d., added jitter of 1.0e-05 to the diagonal\n",
      "  warnings.warn(\n",
      "/home/besginow/anaconda3/envs/sage/lib/python3.10/site-packages/linear_operator/utils/cholesky.py:40: NumericalWarning: A not p.d., added jitter of 1.0e-04 to the diagonal\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "NotPSDError",
     "evalue": "Matrix not positive definite after repeatedly adding jitter up to 1.0e-04.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotPSDError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_88145/365659495.py\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlikelihood\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_88145/3216648513.py\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, likelihood, train_x, train_y, train_iterations, random_restart)\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclosure\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mbest_loss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0mbest_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/sage/lib/python3.10/site-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m                     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m                     \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_optimizer_step_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/sage/lib/python3.10/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/sage/lib/python3.10/site-packages/torch/optim/lbfgs.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    436\u001b[0m                     \u001b[0;31m# no use to re-evaluate that function here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m                     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 438\u001b[0;31m                         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclosure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    439\u001b[0m                     \u001b[0mflat_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gather_flat_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m                     \u001b[0mopt_cond\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mflat_grad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mtolerance_grad\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/sage/lib/python3.10/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_88145/3216648513.py\u001b[0m in \u001b[0;36mclosure\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mclosure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mmll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/sage/lib/python3.10/site-packages/gpytorch/models/exact_gp.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    318\u001b[0m             \u001b[0;31m# Make the prediction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0msettings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcg_tolerance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval_cg_tolerance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 320\u001b[0;31m                 \u001b[0mpredictive_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictive_covar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprediction_strategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexact_prediction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfull_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfull_covar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    321\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m             \u001b[0;31m# Reshape predictive mean to match the appropriate event shape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/sage/lib/python3.10/site-packages/gpytorch/models/exact_prediction_strategies.py\u001b[0m in \u001b[0;36mexact_prediction\u001b[0;34m(self, joint_mean, joint_covar)\u001b[0m\n\u001b[1;32m    271\u001b[0m         return (\n\u001b[1;32m    272\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexact_predictive_mean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_train_covar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 273\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexact_predictive_covar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_test_covar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_train_covar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    274\u001b[0m         )\n\u001b[1;32m    275\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/sage/lib/python3.10/site-packages/gpytorch/models/exact_prediction_strategies.py\u001b[0m in \u001b[0;36mexact_predictive_covar\u001b[0;34m(self, test_test_covar, test_train_covar)\u001b[0m\n\u001b[1;32m    319\u001b[0m             \u001b[0mtest_train_covar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_dense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_train_covar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m             \u001b[0mtrain_test_covar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_train_covar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 321\u001b[0;31m             \u001b[0mcovar_correction_rhs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_train_covar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msolve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_test_covar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    322\u001b[0m             \u001b[0;31m# For efficiency\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_test_covar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/sage/lib/python3.10/site-packages/linear_operator/operators/_linear_operator.py\u001b[0m in \u001b[0;36msolve\u001b[0;34m(self, right_tensor, left_tensor)\u001b[0m\n\u001b[1;32m   2203\u001b[0m         \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSolve\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2204\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mleft_tensor\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2205\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepresentation_tree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mright_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepresentation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2206\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2207\u001b[0m             return func.apply(\n",
      "\u001b[0;32m~/anaconda3/envs/sage/lib/python3.10/site-packages/linear_operator/functions/_solve.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(ctx, representation_tree, has_left, *args)\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mleft_tensor\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m             \u001b[0msolves\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_solve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlinear_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mright_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msolves\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/sage/lib/python3.10/site-packages/linear_operator/functions/_solve.py\u001b[0m in \u001b[0;36m_solve\u001b[0;34m(linear_op, rhs)\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlinear_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msolve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrhs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msettings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfast_computations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msolves\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moff\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mlinear_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0msettings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_cholesky_size\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlinear_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcholesky\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cholesky_solve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrhs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/sage/lib/python3.10/site-packages/linear_operator/operators/_linear_operator.py\u001b[0m in \u001b[0;36mcholesky\u001b[0;34m(self, upper)\u001b[0m\n\u001b[1;32m   1227\u001b[0m         \u001b[0;34m:\u001b[0m\u001b[0;32mreturn\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mCholesky\u001b[0m \u001b[0mfactor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlower\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mupper\u001b[0m \u001b[0mtriangular\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1228\u001b[0m         \"\"\"\n\u001b[0;32m-> 1229\u001b[0;31m         \u001b[0mchol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cholesky\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mupper\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1230\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mupper\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1231\u001b[0m             \u001b[0mchol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transpose_nonbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/sage/lib/python3.10/site-packages/linear_operator/utils/memoize.py\u001b[0m in \u001b[0;36mg\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0mkwargs_pkl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_is_in_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs_pkl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs_pkl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0m_add_to_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs_pkl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs_pkl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_get_from_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs_pkl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs_pkl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/sage/lib/python3.10/site-packages/linear_operator/operators/_linear_operator.py\u001b[0m in \u001b[0;36m_cholesky\u001b[0;34m(self, upper)\u001b[0m\n\u001b[1;32m    488\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m         \u001b[0;31m# contiguous call is necessary here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 490\u001b[0;31m         \u001b[0mcholesky\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpsd_safe_cholesky\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluated_mat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupper\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mupper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    491\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mTriangularLinearOperator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcholesky\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupper\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mupper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/sage/lib/python3.10/site-packages/linear_operator/utils/cholesky.py\u001b[0m in \u001b[0;36mpsd_safe_cholesky\u001b[0;34m(A, upper, out, jitter, max_tries)\u001b[0m\n\u001b[1;32m     63\u001b[0m             \u001b[0mNumber\u001b[0m \u001b[0mof\u001b[0m \u001b[0mattempts\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mwith\u001b[0m \u001b[0msuccessively\u001b[0m \u001b[0mincreasing\u001b[0m \u001b[0mjitter\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0mto\u001b[0m \u001b[0mmake\u001b[0m \u001b[0mbefore\u001b[0m \u001b[0mraising\u001b[0m \u001b[0man\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m     \"\"\"\n\u001b[0;32m---> 65\u001b[0;31m     \u001b[0mL\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_psd_safe_cholesky\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjitter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mjitter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_tries\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_tries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mupper\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/sage/lib/python3.10/site-packages/linear_operator/utils/cholesky.py\u001b[0m in \u001b[0;36m_psd_safe_cholesky\u001b[0;34m(A, out, jitter, max_tries)\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mL\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mNotPSDError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Matrix not positive definite after repeatedly adding jitter up to {jitter_new:.1e}.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotPSDError\u001b[0m: Matrix not positive definite after repeatedly adding jitter up to 1.0e-04."
     ]
    }
   ],
   "source": [
    "train_model(model, likelihood, train_x, train_y, 50, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "bad operand type for unary -: 'MultivariateNormal'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_86918/3917229789.py\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlikelihood\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Make predictions with the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mlikelihood\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_86918/714638428.py\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, likelihood, train_x, train_y, train_iterations, random_restart)\u001b[0m\n\u001b[1;32m     31\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mlikelihood\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: bad operand type for unary -: 'MultivariateNormal'"
     ]
    }
   ],
   "source": [
    "\n",
    "# Make predictions with the model\n",
    "model.eval()\n",
    "likelihood.eval()\n",
    "with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "    test_x = torch.linspace(0, 1, 51)\n",
    "    observed_pred = likelihood(model(test_x))\n",
    "\n",
    "# Plot the results\n",
    "with torch.no_grad():\n",
    "    mean = observed_pred.mean\n",
    "    lower, upper = observed_pred.confidence_region()\n",
    "plt.plot(train_x.numpy(), train_y.numpy(), 'k*')\n",
    "plt.plot(test_x.numpy(), mean.numpy(), 'b')\n",
    "plt.fill_between(test_x.numpy(), lower.numpy(), upper.numpy(), alpha=0.5)\n",
    "plt.ylim([-3, 3])\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sage",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
