{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16826c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install anytree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c252e947",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy \n",
    "import itertools\n",
    "from pathlib import Path\n",
    "import os\n",
    "import pprint\n",
    "from anytree import Node, RenderTree\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy \n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "dca837a3",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "afa87acf",
   "metadata": {},
   "source": [
    "# Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb3e6f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpickle(filepath : str):\n",
    "    file_object = open(filepath, \"rb\")\n",
    "    results = pickle.load(file_object)\n",
    "    file_object.close()\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9256daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_setting_from_name(name :str, naming_schema : list):\n",
    "    result = {}\n",
    "    # Main assumption for the naming schema: parameters are strictly separated by underscores '_'\n",
    "    for value, setting in zip(name.split(\"_\"), naming_schema):\n",
    "        result[setting] = value\n",
    "    return result\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ab46d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_results(directory, experiment_name, naming_schema=None, regex_scheme=None):\n",
    "    path = Path(directory)\n",
    "\n",
    "    dirs = [e for e in path.iterdir() if e.is_dir() and not str(e) == '.ipynb_checkpoints']\n",
    "    subdirs = {str(path): [e for e in path.iterdir() if e.is_dir()] for path in dirs}\n",
    "    \n",
    "    relevant_subdirs = subdirs[os.path.join(directory, experiment_name)]\n",
    "    relevant_subdirs.sort()\n",
    "    #result_filename = '*.pickle'\n",
    "    pickle_dirs = list()\n",
    "    for subdir in relevant_subdirs:\n",
    "        if regex_scheme:\n",
    "            pickle_dirs.extend(sorted(subdir.glob(regex_scheme)))\n",
    "        else:\n",
    "            pickle_dirs.extend(sorted(subdir.glob(\"*.pickle\")))\n",
    "    results = []\n",
    "    if not naming_schema is None:\n",
    "        all_attributes = [extract_setting_from_name(subdir.name, naming_schema) for subdir in relevant_subdirs]    \n",
    "        for attributes, pick in zip(all_attributes, pickle_dirs):\n",
    "            try:\n",
    "                unpickled_stuff = unpickle(pick)\n",
    "                results.append({'attributes': attributes, 'results': unpickled_stuff})\n",
    "            except:\n",
    "                # Sometimes there were unknown issues with the pickle files, in those instances we re-ran training\n",
    "                print(\"Catastrophic failure\")\n",
    "    else:\n",
    "        for pick in pickle_dirs:\n",
    "            try:\n",
    "                unpickled_stuff = unpickle(pick)\n",
    "                results.append({'results': unpickled_stuff})\n",
    "            except:\n",
    "                # Sometimes there were unknown issues with the pickle files, in those instances we re-ran training\n",
    "                print(\"Catastrophic failure\")\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef7376c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def deep_dict_get(data : dict, path : str):\n",
    "    \"\"\"\n",
    "    path a slash ('/') separated path down to \n",
    "    \"\"\"\n",
    "    temp = data.copy()\n",
    "    for entry in path.split(\"/\"):\n",
    "        # Catches leading '/' in tree printing\n",
    "        if entry == '':\n",
    "            continue\n",
    "        temp = temp[entry]\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f41bc6e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_values(d1 : dict, d2 : dict, path : str):\n",
    "    v1 = deep_dict_get(d1, path)\n",
    "    v2 = deep_dict_get(d2, path)\n",
    "    # Sanity check 1\n",
    "    if not type(v1) == type(v2):\n",
    "        return False\n",
    "    else:\n",
    "        # This can potentially cause errors when comparing lists of lists (or Tensors/Arrays)\n",
    "        return v1 == v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d833486",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_common_root_list(root : str, values : list):\n",
    "    return [f\"{root}/{val}\" for val in values]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0ed03ddf",
   "metadata": {},
   "source": [
    "# Main functions    \n",
    "What features do I need?    \n",
    "- ~~Show a structure tree of the results (i.e. experiment settings and result values)~~~\n",
    "- ~~Filtering by setting, given a key~~\n",
    "- ~~Filtering by values, given a key~~\n",
    "- Create selected statistics (mean/med/std/quartiles/...) for certain values/keys\n",
    "- Apply a function to certain values/keys and return the results (e.g. Eigendecomposition/Normalization/...)\n",
    "- ~~Group all entries that share settings/values~~\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb4593bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_tree_structure(data, parent=None):\n",
    "    # Solved through recursively going deeper into the data structure and then returning the leafs if at the end\n",
    "\n",
    "    if parent is None and not len(data.keys()) == 1:\n",
    "        parent = Node(\"root\")\n",
    "    elif parent is None:\n",
    "        parent = Node(list(data.keys())[0])\n",
    "        data = data[list(data.keys())[0]]\n",
    "    \n",
    "    # Recursion condition\n",
    "    # If there are any dictionaries inside then go deeper\n",
    "    if not any([type(data[entry]) == dict for entry in data]):\n",
    "        for entry in data:\n",
    "            Node(entry, parent=parent) \n",
    "    else:\n",
    "        for entry in data:\n",
    "            if type(data[entry]) == dict:\n",
    "                branch = Node(entry, parent=parent)\n",
    "                generate_tree_structure(data[entry], parent=branch)\n",
    "            else:\n",
    "                Node(entry, parent=parent)\n",
    "    return parent\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da64394e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_by(list_of_data : list, path, value=None):\n",
    "    \"\"\"\n",
    "    value : If None, make subgroups of equal values. \n",
    "            Otherwise return a single group where value is matched\n",
    "    \"\"\"\n",
    "    grouped_data = {}\n",
    "\n",
    "    if not value is None:\n",
    "        grouped_data[f\"{path} = {value}\"] = [data for data in list_of_data if deep_dict_get(data, path) == value]\n",
    "    else:    \n",
    "        finished_values = list()\n",
    "        for data in list_of_data:\n",
    "            value = deep_dict_get(data, path)\n",
    "            if f\"{path} = {value}\" in grouped_data.keys():\n",
    "                grouped_data[f\"{path} = {value}\"].append(data)\n",
    "            else:\n",
    "                grouped_data[f\"{path} = {value}\"] = list()\n",
    "                grouped_data[f\"{path} = {value}\"].append(data)\n",
    "            #if not value in finished_values:\n",
    "            #    grouped_data[f\"{path} = value\"] = [data for data in list_of_data if deep_dict_get(data, path) == value]\n",
    "            #    finished_values.append(value)\n",
    "    return grouped_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e29e1281",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dict_compare(d1, d2):\n",
    "    # Checking for empty list in d2, i.e. initial value\n",
    "    if not type(d1) == type(d2):\n",
    "        return False\n",
    "    return all((d1.get(k) == v for k, v in d2.items()))\n",
    "        \n",
    "\n",
    "# Could contain an alternative head as (list_of_data : list, paths : [list, dict], values : None) \n",
    "# where paths-values would require a 1-to-1 correspondence. \n",
    "# But this could quickly become error prone on the user side...\n",
    "def group_by_multiple(list_of_data : list, paths):\n",
    "    grouped_data = {}\n",
    "    # Grouping without values\n",
    "    if type(paths) == list:\n",
    "        finished_values = list()\n",
    "        for data in list_of_data:\n",
    "            paths_vals = {path : deep_dict_get(data, path) for path in paths}\n",
    "            # not any X <=> all not X\n",
    "            # i.e. only succeeds when this combination didn't exist before\n",
    "            if not any([dict_comapare(paths_vals, fin_val) for fin_val in finished_values]):\n",
    "                grouped_data[\" ; \".join([f\"{path} = {paths_vals[path]}\" for path in paths_vals])] = [data]\n",
    "            else:\n",
    "                grouped_data[\" ; \".join([f\"{path} = {paths_vals[path]}\" for path in paths_vals])].append(data)\n",
    "\n",
    "    # Grouping by path-value combinations\n",
    "    # Only returns the group where all those pairs are true\n",
    "    elif type(paths) == dict:\n",
    "        # The keys will contain the paths\n",
    "        # The values will be the corresponding expected values\n",
    "        # Yes, this could be a one-liner with a very neat nested list creation, \n",
    "        #  but I chose readability with temporary variables over it.\n",
    "        good_data = list()\n",
    "        for data in list_of_data:\n",
    "            if all([deep_dict_get(data, path) == paths[path] for path in paths]):\n",
    "                good_data.append(data)\n",
    "        grouped_data[\" ; \".join([f\"{path} = {paths[path]}\" for path in paths])] = good_data\n",
    "    return grouped_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e065d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_by(list_of_data : list, path : str, value):\n",
    "    filtered_data = list()\n",
    "    for data in list_of_data:\n",
    "        if deep_dict_get(data, path) == value:\n",
    "            filtered_data.append(data)\n",
    "    return filtered_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "174730a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_value(list_of_data : list, path : str):\n",
    "    \"\"\"\n",
    "        returns a list of the target value from each data dict\n",
    "    \"\"\"\n",
    "    list_of_values = list()\n",
    "    for data in list_of_data:\n",
    "        list_of_values.append(deep_dict_get(data, path))\n",
    "    return list_of_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5643bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_attributes(data : dict):\n",
    "    return data[\"results\"][\"attributes\"]\n",
    "\n",
    "def get_results(data : dict):\n",
    "    return data[\"results\"][\"results\"]\n",
    "\n",
    "def find_common_attributes(att_1 : dict, att_2 : dict):\n",
    "    \"\"\"\n",
    "    Returns a dictionary of common attributes between two attribute dictionaries\n",
    "    \"\"\"\n",
    "    common_attributes = {}\n",
    "    for key in att_1.keys():\n",
    "        if key in att_2.keys() and att_1[key] == att_2[key]:\n",
    "            common_attributes[key] = att_1[key]\n",
    "    return common_attributes\n",
    "\n",
    "def find_differing_attributes(att_1 : dict, att_2 : dict):\n",
    "    \"\"\"\n",
    "    Returns a dictionary of differing attributes between two attribute dictionaries\n",
    "    \"\"\"\n",
    "    differing_attributes = {}\n",
    "    for key in att_1.keys():\n",
    "        if key in att_2.keys() and att_1[key] != att_2[key]:\n",
    "            differing_attributes[key] = (att_1[key], att_2[key])\n",
    "    return differing_attributes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6b70ce21",
   "metadata": {},
   "source": [
    "# Things I want to check\n",
    " - **Hardcoded setting**: Do the metrics have the same order as MCMC?\n",
    " - **CO2**: Also check the order\n",
    " - **CO2**: Check the prediction of future datapoints for each favourite metric \n",
    " - **Kernel search**: How do the kernels found by the metrics perform according to MCMC?\n",
    " - (Kernel search: How do the kernels found by the metrics predict the future datapoints?)\n",
    " - **LODE**: Does Laplace find the most appropriate Differential equation? I.e. the correct numbers of parameters?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4b944501",
   "metadata": {},
   "source": [
    "# Load results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f59bbcdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results = load_results('results', \"hardcoded\", regex_scheme=\"results.pickle\")\n",
    "\n",
    "postfix = \"\"\n",
    "result_tree = generate_tree_structure(all_results[0]).descendants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87631a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combining two different seeds\n",
    "# Look for the exact same set of attributes and append the corresponding runs to the first result list\n",
    "import copy\n",
    "def combine_results(main_results, other_results):\n",
    "    \"\"\"\n",
    "        main_results : If the result lists are of different size (in terms of the settings), this list is the one containing the larger set of settings\n",
    "        other_results: This is the list of results added to the other\n",
    "    \"\"\"\n",
    "    return_results = copy.deepcopy(main_results)\n",
    "    for main_dict in main_results:\n",
    "        # Check for these two \n",
    "        data_gen_kernel = main_dict[\"results\"][\"attributes\"][\"data_gen\"]\n",
    "        dataset_size = main_dict[\"results\"][\"attributes\"][\"eval_COUNT\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a6f014",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============\n",
    "# Preparing the results dict\n",
    "# =============\n",
    "\n",
    "# relevant keys: \"data_gen\" and \"eval_COUNT\"\n",
    "# Have a dictionary for each evaluation metric. It has dictionaries for each \"eval_COUNT\" that contains a dictionary per \"data_gen\" that has dictionaries for the different kernels.\n",
    "results_dict = {}\n",
    "\n",
    "metrics_keys = ('AIC', 'BIC', 'MLL', 'MAP', 'Nested', 'Lap', \"Lap0\", \"LapAIC\", \"LapBIC\")\n",
    "\n",
    "all_data_gens = sorted(set([get_attributes(result)[\"data_gen\"] for result in all_results]))\n",
    "all_data_sizes = sorted(set([get_attributes(result)[\"eval_COUNT\"] for result in all_results]))\n",
    "str_all_data_sizes = sorted(set([str(get_attributes(result)[\"eval_COUNT\"]) for result in all_results]))\n",
    "\n",
    "\n",
    "results_dict = {key: \n",
    "                {str(key1): \n",
    "                 {key2: \n",
    "                  {key3: [] for key3 in all_data_gens} # The TESTED kernel\n",
    "                    for key2 in all_data_gens} # The GENERATING kernel\n",
    "                      for key1 in all_data_sizes} # Then the dataset size\n",
    "                        for key in metrics_keys} # Far out: The different metrics\n",
    "\n",
    "for result in all_results:\n",
    "    attributes = get_attributes(result)\n",
    "    data_gen_kernel = attributes[\"data_gen\"]\n",
    "    dataset_size = str(attributes[\"eval_COUNT\"])\n",
    "\n",
    "    results = get_results(result)\n",
    "    for run in results:\n",
    "        for kernel in all_data_gens:\n",
    "            for metric in ('AIC', 'BIC', 'MLL', 'MAP', 'Nested'):\n",
    "                if metric in (\"AIC\", \"BIC\"):\n",
    "                    results_dict[metric][str(dataset_size)][data_gen_kernel][kernel].append(deep_dict_get(run, f\"{metric}/{kernel}/loss\").detach().numpy().item() * (-0.5))\n",
    "                else:\n",
    "                    results_dict[metric][str(dataset_size)][data_gen_kernel][kernel].append(deep_dict_get(run, f\"{metric}/{kernel}/loss\").detach().numpy().item() if type(deep_dict_get(run, f\"{metric}/{kernel}/loss\")) == torch.Tensor else deep_dict_get(run, f\"{metric}/{kernel}/loss\"))\n",
    "            for threshold in deep_dict_get(run, f\"Laplace/{kernel}\"):\n",
    "                if threshold == 0.0:\n",
    "                    results_dict['Lap'][str(dataset_size)][data_gen_kernel][kernel].append(deep_dict_get(run, f\"Laplace/{kernel}\")[threshold][\"details\"][\"laplace without replacement\"].detach().numpy().item())\n",
    "                    results_dict['Lap0'][str(dataset_size)][data_gen_kernel][kernel].append(deep_dict_get(run, f\"Laplace/{kernel}\")[threshold][\"loss\"].detach().numpy().item())\n",
    "                elif threshold == -1.0:\n",
    "                    results_dict['LapAIC'][str(dataset_size)][data_gen_kernel][kernel].append(deep_dict_get(run, f\"Laplace/{kernel}\")[threshold][\"loss\"].detach().numpy().item())\n",
    "                elif threshold == \"BIC\":\n",
    "                    results_dict['LapBIC'][str(dataset_size)][data_gen_kernel][kernel].append(deep_dict_get(run, f\"Laplace/{kernel}\")[threshold][\"loss\"].detach().numpy().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9363c507",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Draw a a matrix of heatmaps where each row is a dataset size is a metric.\n",
    "# Inside the heatmaps apply the \"numpy.mean\" function to the results of each kernel.\n",
    "\n",
    "fig, axs = plt.subplots(len(metrics_keys), len(all_data_sizes), figsize=(20, 40))\n",
    "for i, metric in enumerate(metrics_keys):\n",
    "    for j, dataset_size in enumerate(all_data_sizes):\n",
    "        dataset_size = str(dataset_size)  # Ensure dataset_size is a string for consistency in the dictionary keys\n",
    "        # Get the data for this metric and dataset size\n",
    "        data = np.array([[results_dict[metric][dataset_size][data_gen][kernel] for kernel in all_data_gens] for data_gen in all_data_gens])\n",
    "        \n",
    "        # Calculate the mean across kernels\n",
    "        # Rows show the GENERATING kernel, columns show the TESTED kernel\n",
    "        mean_data = np.mean(data, axis=2)\n",
    "\n",
    "        ## Translate each row into an ordering, i.e. the best is assigned a \"1\", the second best a \"2\", etc.\n",
    "        #for row in mean_data:\n",
    "        #    sorted_indices = np.argsort(row)\n",
    "        #    rank = np.empty_like(sorted_indices)\n",
    "        #    rank[sorted_indices] = np.arange(len(row)) + 1\n",
    "        #    row[:] = rank\n",
    "       \n",
    "        # Create a heatmap\n",
    "        im = axs[i, j].imshow(mean_data, aspect='auto', cmap='viridis_r')\n",
    "        \n",
    "        # Set the ticks and labels\n",
    "        axs[i, j].set_xticks(np.arange(len(all_data_gens)))\n",
    "        axs[i, j].set_yticks(np.arange(len(all_data_gens)))\n",
    "        axs[i, j].set_xticklabels(all_data_gens)\n",
    "        axs[i, j].set_yticklabels(all_data_gens)\n",
    "        \n",
    "        # Set title and labels\n",
    "        axs[i, j].set_title(f\"{metric} - {dataset_size}\")\n",
    "        # Rotate the x-tick labels for better readability\n",
    "        plt.setp(axs[i, j].xaxis.get_majorticklabels(), rotation=45, ha='right')\n",
    "        # Add a colorbar\n",
    "        fig.colorbar(im, ax=axs[i, j])\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b27260a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function that returns the kendalltau correlation of two sequences\n",
    "def kendalltau(list1, list2):\n",
    "    return scipy.stats.kendalltau(list1, list2)\n",
    "\n",
    "# Helper function that returns the spearman correlation of two sequences\n",
    "def spearman(list1, list2):\n",
    "    return scipy.stats.spearmanr(list1, list2)\n",
    "\n",
    "# Helper function that translates a list of values to a ranking\n",
    "def rank_values(values):\n",
    "    sorted_indices = np.argsort(values)\n",
    "    rank = np.empty_like(sorted_indices)\n",
    "    rank[sorted_indices] = np.arange(len(values)) + 1\n",
    "    return rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf4896a",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_kendall_taus = {metric: {str(size) : {gen: None for gen in all_data_gens} for size in all_data_sizes} for metric in metrics_keys}\n",
    "all_spearman_rs = {metric: {str(size) : {gen: None for gen in all_data_gens} for size in all_data_sizes} for metric in metrics_keys}\n",
    "all_rank_kendall_taus = {metric: {str(size) : {gen: None for gen in all_data_gens} for size in all_data_sizes} for metric in metrics_keys}\n",
    "all_rank_spearman_rs = {metric: {str(size) : {gen: None for gen in all_data_gens} for size in all_data_sizes} for metric in metrics_keys}\n",
    "\n",
    "for test_key, nested_key in itertools.product(results_dict.keys(), [\"Nested\"]):\n",
    "    for dataset_size in all_data_sizes:\n",
    "        dataset_size = str(dataset_size)  # Ensure dataset_size is a string for consistency in the dictionary keys\n",
    "        for data_gen_kernel in all_data_gens:\n",
    "            value_under_test = results_dict[test_key][dataset_size][data_gen_kernel]\n",
    "            ground_truth = results_dict[nested_key][dataset_size][data_gen_kernel]\n",
    "\n",
    "            tau, p_value = kendalltau(([np.nanmean(ground_truth[key]) for key in all_data_gens]), ([np.nanmean(value_under_test[key]) for key in all_data_gens]))\n",
    "            spearman_corr, spearman_p_value = spearman(([np.nanmean(ground_truth[key]) for key in all_data_gens]), ([np.nanmean(value_under_test[key]) for key in all_data_gens]))\n",
    "            \n",
    "            tau_rank, p_value_rank = kendalltau(rank_values([np.nanmean(ground_truth[key]) for key in all_data_gens]), rank_values([np.nanmean(value_under_test[key]) for key in all_data_gens]))\n",
    "            spearman_corr_rank, spearman_p_value_rank = spearman(rank_values([np.nanmean(ground_truth[key]) for key in all_data_gens]), rank_values([np.nanmean(value_under_test[key]) for key in all_data_gens]))\n",
    "\n",
    "            all_kendall_taus[test_key][dataset_size][data_gen_kernel] = tau\n",
    "            all_spearman_rs[test_key][dataset_size][data_gen_kernel] = spearman_corr\n",
    "            all_rank_kendall_taus[test_key][dataset_size][data_gen_kernel] = tau_rank\n",
    "            all_rank_spearman_rs[test_key][dataset_size][data_gen_kernel] = spearman_corr_rank\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a955bbec",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data_gens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d30bc66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_all_possible_dict_slicings(data_dict, path):\n",
    "    keys = path.split('/')\n",
    "    valid_key = \"\" \n",
    "    for key in keys:\n",
    "        if key == \":\":\n",
    "            all_possible_dict_values = deep_dict_get(data_dict, valid_key).keys()\n",
    "        else:\n",
    "            valid_key += f\"{key}/\"\n",
    "    return [path.replace(\":\", str(value)) for value in all_possible_dict_values]\n",
    "\n",
    "def dict_slice(data_dict, path):\n",
    "    all_possible_dict_values = []\n",
    "    all_sliced_paths = find_all_possible_dict_slicings(data_dict, path)\n",
    "    for sliced_path in all_sliced_paths:\n",
    "        all_possible_dict_values.append(deep_dict_get(data_dict, sliced_path))\n",
    "    return all_possible_dict_values \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e06a6dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "find_all_possible_dict_slicings(all_kendall_taus, \"Lap/:/SE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74fb6820",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_dict = all_rank_kendall_taus \n",
    "fig, axs = plt.subplots(len(str_all_data_sizes), 1,  figsize=(5, 15))\n",
    "for row, dataset_size in enumerate(sorted(all_data_sizes)):\n",
    "    axs[row].boxplot([dict_slice(target_dict, f\"{metric}/{dataset_size}/:\") for metric in target_dict.keys()], tick_labels=target_dict.keys())\n",
    "    axs[row].set_title(f\"{dataset_size}\")\n",
    "    axs[row].set_xlabel(\"Kernel\")\n",
    "    axs[row].set_ylabel(\"Kendall Tau Rank\")\n",
    "    # Rotate the x-tick labels for better readability\n",
    "    plt.setp(axs[row].xaxis.get_majorticklabels(), rotation=45, ha='right')\n",
    "    axs[row].grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "#plt.boxplot([dict_slice(target_dict, f\"{metric}/{dataset_size}/:\") for metric in target_dict.keys()], tick_labels=target_dict.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "134d9952",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6be4c4a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a56ebe79",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff9c06e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8831e41",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sage3_129",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
