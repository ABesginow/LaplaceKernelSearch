{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e9654ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "\n",
    "class TanhWarp(nn.Module):\n",
    "    \"\"\"\n",
    "    g(y) = y + sum_i a_i * tanh(b_i * (y + c_i))\n",
    "    with a_i, b_i > 0 to guarantee monotonicity: g'(y) = 1 + sum_i a_i*b_i*(1 - tanh(.)^2) > 0\n",
    "    \"\"\"\n",
    "    def __init__(self, M=3, b_transform=None, a_transform=None):\n",
    "        super().__init__()\n",
    "        self.raw_a = nn.Parameter(torch.zeros(M))  # -> a = softplus(raw_a)\n",
    "        self.raw_b = nn.Parameter(torch.zeros(M))  # -> b = softplus(raw_b)\n",
    "        # The transformation functions for a, b\n",
    "        # limit b to be in the interval (q, p) resp. for a in (q2, p2)\n",
    "        p, q = 1.0, 0.0\n",
    "        self.b_transform = b_transform if b_transform is not None else lambda x: (torch.sigmoid(x) * (p - q)) + q \n",
    "        p2, q2 = 5.0, 0.0\n",
    "        self.a_transform = a_transform if a_transform is not None else lambda x: (torch.sigmoid(x) * (p2 - q2)) + q2\n",
    "\n",
    "        self.c     = nn.Parameter(torch.zeros(M))\n",
    "        self.softplus = nn.Softplus()\n",
    "\n",
    "    def forward(self, y):\n",
    "        a = self.a_transform(self.raw_a)  # (M,)\n",
    "        b = self.b_transform(self.raw_b)  # (M,)\n",
    "        z = y.unsqueeze(-1) + self.c   # (..., M)\n",
    "        return y + torch.sum(a * torch.tanh(b * z), dim=-1)\n",
    "\n",
    "    def log_abs_det_jacobian(self, y):\n",
    "        a = self.a_transform(self.raw_a)\n",
    "        b = self.b_transform(self.raw_b)\n",
    "        z = y.unsqueeze(-1) + self.c\n",
    "        t = torch.tanh(b * z)\n",
    "        sech2 = 1 - t**2\n",
    "        deriv = 1.0 + torch.sum(a * b * sech2, dim=-1)\n",
    "        deriv = torch.clamp(deriv, min=1e-8)  # numerical safety\n",
    "        return torch.log(deriv)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def inverse(self, u, y_init=None, iters=40):\n",
    "        # Newton solve for y: g(y)=u ; monotone g ensures unique root\n",
    "        y = u.clone() if y_init is None else y_init.clone()\n",
    "        for _ in range(iters):\n",
    "            gy  = self.forward(y)\n",
    "            dgy = torch.exp(self.log_abs_det_jacobian(y))\n",
    "            y   = y - (gy - u)/dgy\n",
    "        return y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83cc5961",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gpytorch\n",
    "class Model(gpytorch.models.ExactGP):\n",
    "    def __init__(self, X, u, likelihood, **kwargs):\n",
    "        super().__init__(X, u, likelihood)\n",
    "        self.mean_module = gpytorch.means.ZeroMean()\n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel())\n",
    "        if \"warp\" in kwargs:\n",
    "            self.warp = kwargs.get(\"warp\", [])\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "243fa679",
   "metadata": {},
   "outputs": [],
   "source": [
    "#in 1d:\n",
    "## Daten von x=0..1\n",
    "#f:=x->2+exp(exp(3-100*x))-1/5*exp(-1/2*(x-0.1)^2/(1/5)^2)+0.05*x;\n",
    "#\n",
    "#Noise:\n",
    "#heteroscedatic!\n",
    "#the noise is Gaussian in log-space with a standard deviation of 5%\n",
    "#[x->exp(0.95*log(f(x))),f,x->exp(1.05*log(f(x)))]\n",
    "#\n",
    "## baue 2. Input \"a\" ein. a=0..1. Hat weniger einfluss\n",
    "#g:=x->f(x-0.1*sin(5*a/2))*(1+0.1*arctan(5*a-2.5))+0.1*cos(10*a/3);\n",
    "\n",
    "\n",
    "data_f = lambda x: 2 + torch.exp(torch.exp(3 - 100 * x)) - 1/5 * torch.exp(-1/2 * (x - 0.1)**2 / (1/5)**2) + 0.05 * x\n",
    "data_noise = lambda x: torch.exp(0.95 * torch.log(data_f(x)))\n",
    "data_noise_high = lambda x: torch.exp(1.05 * torch.log(data_f(x)))\n",
    "\n",
    "data_g = lambda x, a: data_f(x - 0.1 * torch.sin(5 * a / 2)) * (1 + 0.1 * torch.arctan(5 * a - 2.5)) + 0.1 * torch.cos(10 * a / 3)\n",
    "\n",
    "\n",
    "X = torch.linspace(0.1, 1, 1000)\n",
    "\n",
    "\n",
    "# TODO log(y) OHNE warping\n",
    "\n",
    "\n",
    "\n",
    "y = data_g(X, torch.tensor(0.5)) #torch.sin(2 * torch.pi * X) + 0.1 * torch.randn_like(X)  # example data\n",
    "y = y - y.mean()  # center the data\n",
    "y = y / y.std()  # normalize the data\n",
    "\n",
    "warp = TanhWarp(M=1)\n",
    "u = warp(y)  # transformed targets\n",
    "plt.plot(X.numpy(), y.numpy(), label='Original Data')\n",
    "plt.plot(X.numpy(), u.detach().numpy(), label='Warped Data')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee922cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "model = Model(X, u, likelihood, warp=warp)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.1)\n",
    "model.train(); likelihood.train()\n",
    "optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4997f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "for _ in range(200):\n",
    "    optimizer.zero_grad()\n",
    "    u = model.warp(y)  # y -> u\n",
    "    output = model(X)  # MultivariateNormal over f(x) (latent function in u-space)\n",
    "    base_term = mll(output, u)\n",
    "\n",
    "    jac_term = model.warp.log_abs_det_jacobian(y).sum()  # + Î£ log g'(y_n)\n",
    "    loss = -(base_term + jac_term)\n",
    "    if _ % 50 == 0:\n",
    "        print(loss.item())\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b67ab90b",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(model.warp.named_parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3aca37f",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(model.named_parameters())\n",
    "#list(likelihood.named_parameters())"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8cec2bfd",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "from helpers.training_functions import granso_optimization\n",
    "from helpers.util_functions import log_normalized_prior, get_full_kernels_in_kernel_expression, randomize_model_hyperparameters\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "kernel_parameter_priors = {\n",
    "    (\"RBFKernel\", \"lengthscale\"): {\"mean\": 0.0, \"std\": 10.0}, \n",
    "    (\"MaternKernel\", \"lengthscale\"): {\"mean\": 0.0, \"std\": 10.0},\n",
    "    (\"LinearKernel\", \"variance\"): {\"mean\": 0.0, \"std\": 10.0},\n",
    "    (\"AffineKernel\", \"variance\"): {\"mean\": 0.0, \"std\": 10.0},\n",
    "    (\"RQKernel\", \"lengthscale\"): {\"mean\": 0.0, \"std\": 10.0},\n",
    "    (\"RQKernel\", \"alpha\"): {\"mean\": 0.0, \"std\": 10.0},\n",
    "    (\"CosineKernel\", \"period_length\"): {\"mean\": 0.0, \"std\": 10.0},\n",
    "    (\"PeriodicKernel\", \"lengthscale\"): {\"mean\": 0.0, \"std\": 10.0},\n",
    "    (\"PeriodicKernel\", \"period_length\"): {\"mean\": 0.0, \"std\": 10.0},\n",
    "    (\"ScaleKernel\", \"outputscale\"): {\"mean\": 0.0, \"std\": 10.0},\n",
    "    (\"LODE_Kernel\", \"signal_variance_2_0\"): {\"mean\": 0.0, \"std\": 10.0},  # full match\n",
    "    (\"LODE_Kernel\", \"lengthscale\"): {\"mean\": 0.0, \"std\": 10.0},           # base fallback\n",
    "}\n",
    "\n",
    "\n",
    "parameter_priors = {\n",
    "    \"likelihood.raw_task_noises\": {\"mean\": 0.0, \"std\": 10.0},\n",
    "    \"likelihood.raw_noise\": {\"mean\": 0.0, \"std\": 10.0}\n",
    "}\n",
    "\n",
    "\n",
    "kernel_param_specs = {\n",
    "    (\"RBFKernel\", \"lengthscale\"): {\"bounds\": (1e-1, 5.0)}, # add ', \"type\": \"uniform\"},' # to use uniform distribution\n",
    "    (\"MaternKernel\", \"lengthscale\"): {\"bounds\": (1e-1, 1.0)},\n",
    "    (\"LinearKernel\", \"variance\"): {\"bounds\": (1e-1, 1.0)},\n",
    "    (\"AffineKernel\", \"variance\"): {\"bounds\": (1e-1, 1.0)},\n",
    "    (\"RQKernel\", \"lengthscale\"): {\"bounds\": (1e-1, 1.0)},\n",
    "    (\"RQKernel\", \"alpha\"): {\"bounds\": (1e-1, 1.0)},\n",
    "    (\"CosineKernel\", \"period_length\"): {\"bounds\": (1e-1, 10.0), \"type\": \"uniform\"},\n",
    "    (\"PeriodicKernel\", \"lengthscale\"): {\"bounds\": (1e-1, 5.0)},\n",
    "    (\"PeriodicKernel\", \"period_length\"): {\"bounds\": (1e-1, 10.0), \"type\": \"uniform\"},\n",
    "    (\"ScaleKernel\", \"outputscale\"): {\"bounds\": (1e-1, 10.0)},\n",
    "    #(\"LODE_Kernel\", \"signal_variance_2_0\"): {\"bounds\": (0.05, 0.5)},  # full match\n",
    "    (\"LODE_Kernel\", \"signal_variance\"): {\"bounds\": (1e-1, 10)},  # base\n",
    "    (\"LODE_Kernel\", \"lengthscale\"): {\"bounds\": (1e-1, 5.0)},           \n",
    "}\n",
    "\n",
    "\n",
    "param_specs = {\n",
    "    \"likelihood.raw_task_noises\": {\"bounds\": (1e-1, 1e-0)},\n",
    "    \"likelihood.raw_noise\": {\"bounds\": (1e-1, 1e-0)}\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "# Define the objective function\n",
    "def objective_function(var_in):\n",
    "    model = var_in['model']\n",
    "    train_x = var_in['train_x']\n",
    "    train_y = var_in['train_y']\n",
    "    likelihood = var_in.get(\"likelihood\", gpytorch.likelihoods.GaussianLikelihood())\n",
    "    mll_fkt = var_in.get(\"mll_fkt\", gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model))\n",
    "\n",
    "    MAP = var_in.get('MAP', False)\n",
    "    parameter_priors = var_in.get('parameter_priors', None)\n",
    "    kernel_parameter_priors = var_in.get('kernel_parameter_priors', None)\n",
    "    model_parameter_prior = var_in.get('model_parameter_prior', None)\n",
    "    output = model(train_x)\n",
    "    try:\n",
    "        # TODO PyGRANSO dying is a severe problem. as it literally exits the program instead of raising an error\n",
    "        # negative scaled MLL\n",
    "        u = model.warp(train_y)\n",
    "        loss = -mll_fkt(output, u) -model.warp.log_abs_det_jacobian(train_y).sum()\n",
    "    except Exception as E:\n",
    "        print(\"LOG ERROR: Severe PyGRANSO issue. Loss is inf+0\")\n",
    "        print(f\"LOG ERROR: {E}\")\n",
    "        loss = torch.tensor(np.finfo(np.float32).max, requires_grad=True) + torch.tensor(-10.0)\n",
    "    #print(f\"LOG: {loss}\")\n",
    "    return [loss, None, None]\n",
    "\n",
    "var_in = {\"model\" : model, \"train_x\" : X, \"train_y\" : y, \"likelihood\" : likelihood, \"MAP\" : False,\n",
    "          \"parameter_priors\" : parameter_priors, \"kernel_parameter_priors\" : kernel_parameter_priors, \"model_parameter_prior\" : None}\n",
    "\n",
    "comb_fn = lambda X_struct: objective_function(var_in)\n",
    "\n",
    "neg_scaled_mll, model_MLL, model_likelihood_MLL, training_log_MLL = granso_optimization(model, likelihood, X, u, random_restarts=5, maxit=1000, MAP=False, double_precision=False, verbose=False, objective_function=comb_fn)\n",
    "\n",
    "neg_scaled_mll -= model.warp.log_abs_det_jacobian(y).sum()\n",
    "\n",
    "model_parameters = [p for p in model.parameters() if p.requires_grad]\n",
    "\n",
    "\n",
    "jacobian_neg_unscaled_map = torch.autograd.grad(neg_scaled_mll, model_parameters, retain_graph=True, create_graph=True, allow_unused=True)\n",
    "hessian_neg_unscaled_map_raw = []\n",
    "# Calcuate -\\nabla\\nabla log(f(\\theta)) (i.e. Hessian of negative log posterior)\n",
    "for i in range(len(jacobian_neg_unscaled_map)):\n",
    "    hessian_neg_unscaled_map_raw.append(torch.autograd.grad(jacobian_neg_unscaled_map[i], model_parameters, retain_graph=True, allow_unused=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Xtest = torch.linspace(0.1, 1, 1000)\n",
    "model.eval(); likelihood.eval()\n",
    "with torch.no_grad():\n",
    "    mvn = likelihood(model(Xtest))            # dist over u*\n",
    "    mean_u = mvn.mean\n",
    "    var_u  = mvn.variance\n",
    "\n",
    "    # Point estimates in y-space\n",
    "    median_y = warp.inverse(mean_u)           # â predictive median\n",
    "\n",
    "    # Monte Carlo moments\n",
    "    S = 512\n",
    "    eps = torch.randn(S, *mean_u.shape, device=mean_u.device)\n",
    "    u_samps = mean_u.unsqueeze(0) + eps * var_u.sqrt().unsqueeze(0)\n",
    "    y_samps = warp.inverse(u_samps)\n",
    "\n",
    "    mean_y = y_samps.mean(dim=0)\n",
    "    # matplotlib plot the samples\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(Xtest.cpu().numpy(), median_y.cpu().numpy(), label='Predictive Median', color='blue')\n",
    "    #plt.fill_between(warp.inverse(mean_u -+ 2*torch.sqrt(var_u))))\n",
    "    plt.fill_between(Xtest.cpu().numpy(),\n",
    "                    warp.inverse(mean_u - 2 * torch.sqrt(var_u)).cpu().numpy(),\n",
    "                    warp.inverse(mean_u + 2 * torch.sqrt(var_u)).cpu().numpy(),\n",
    "                    color='blue', alpha=0.2, label='95% Credible Interval')\n",
    "    #plt.fill_between(Xtest.cpu().numpy(), \n",
    "    #                 y_samps.quantile(0.05, dim=0).cpu().numpy(), \n",
    "    #                 y_samps.quantile(0.95, dim=0).cpu().numpy(), \n",
    "    #                 color='blue', alpha=0.2, label='95% Credible Interval')\n",
    "    plt.scatter(X.cpu().numpy(), y.cpu().numpy(), s=10, color='red', label='Data')\n",
    "    plt.title('GP with Tanh Warp')\n",
    "    plt.xlabel('X')\n",
    "    plt.ylabel('y')\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d01a4e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Xtest = torch.linspace(0.1, 1, 1000)\n",
    "model.eval(); likelihood.eval()\n",
    "for a_val in list(range(0, 50, 5)):\n",
    "    model.warp.raw_a.data.fill_(a_val)\n",
    "    print(model.warp.raw_a.data)\n",
    "    with torch.no_grad():\n",
    "        mvn = likelihood(model(Xtest))            # dist over u*\n",
    "        mean_u = mvn.mean\n",
    "        var_u  = mvn.variance\n",
    "\n",
    "        # Point estimates in y-space\n",
    "        median_y = warp.inverse(mean_u)           # â predictive median\n",
    "\n",
    "        # Monte Carlo moments\n",
    "        S = 512\n",
    "        eps = torch.randn(S, *mean_u.shape, device=mean_u.device)\n",
    "        u_samps = mean_u.unsqueeze(0) + eps * var_u.sqrt().unsqueeze(0)\n",
    "        y_samps = warp.inverse(u_samps)\n",
    "\n",
    "        mean_y = y_samps.mean(dim=0)\n",
    "        # matplotlib plot the samples\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(Xtest.cpu().numpy(), median_y.cpu().numpy(), label='Predictive Median', color='blue')\n",
    "        #plt.fill_between(warp.inverse(mean_u -+ 2*torch.sqrt(var_u))))\n",
    "        plt.fill_between(Xtest.cpu().numpy(),\n",
    "                        warp.inverse(mean_u - 2 * torch.sqrt(var_u)).cpu().numpy(),\n",
    "                        warp.inverse(mean_u + 2 * torch.sqrt(var_u)).cpu().numpy(),\n",
    "                        color='blue', alpha=0.2, label='95% Credible Interval')\n",
    "        #plt.fill_between(Xtest.cpu().numpy(), \n",
    "        #                 y_samps.quantile(0.05, dim=0).cpu().numpy(), \n",
    "        #                 y_samps.quantile(0.95, dim=0).cpu().numpy(), \n",
    "        #                 color='blue', alpha=0.2, label='95% Credible Interval')\n",
    "        plt.scatter(X.cpu().numpy(), y.cpu().numpy(), s=10, color='red', label='Data')\n",
    "        plt.title('GP with Tanh Warp')\n",
    "        plt.xlabel('X')\n",
    "        plt.ylabel('y')\n",
    "        plt.legend()\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c273b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    mvn = likelihood(model(Xtest))            # dist over u*\n",
    "    mean_u = mvn.mean\n",
    "    var_u  = mvn.variance\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(Xtest.cpu().numpy(), mean_u.cpu().numpy(), label='Predictive Median', color='blue')\n",
    "    #plt.fill_between(warp.inverse(mean_u -+ 2*torch.sqrt(var_u))))\n",
    "    plt.fill_between(Xtest.cpu().numpy(),\n",
    "                     mean_u - 2 * torch.sqrt(var_u),\n",
    "                     mean_u + 2 * torch.sqrt(var_u),\n",
    "                     color='blue', alpha=0.2, label='95% Credible Interval')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c1a9bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(X, model.warp(y).detach().numpy())\n",
    "plt.plot(X, y)\n",
    "list(model.warp.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c65b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.linspace(-20, 20, 1000)\n",
    "b = 2.1\n",
    "plt.plot(x, x + 80*torch.tanh(b*x))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "laplace",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
