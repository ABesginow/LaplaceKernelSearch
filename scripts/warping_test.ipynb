{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c0d55bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers.plotting_functions import plot_3d_gp, plot_3d_data\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "import torch\n",
    "\n",
    "torch.set_default_dtype(torch.float64)\n",
    "#torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e9654ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "\n",
    "class TanhWarp(nn.Module):\n",
    "    \"\"\"\n",
    "    g(y) = y + sum_i a_i * tanh(b_i * (y + c_i))\n",
    "    with a_i, b_i > 0 to guarantee monotonicity: g'(y) = 1 + sum_i a_i*b_i*(1 - tanh(.)^2) > 0\n",
    "    \"\"\"\n",
    "    def __init__(self, M=3, b_transform=None, a_transform=None):\n",
    "        super().__init__()\n",
    "        self.raw_a = nn.Parameter(torch.zeros(M))  # -> a = softplus(raw_a)\n",
    "        self.raw_b = nn.Parameter(torch.zeros(M))  # -> b = softplus(raw_b)\n",
    "        # The transformation functions for a, b\n",
    "        # limit b to be in the interval (q, p) resp. for a in (q2, p2)\n",
    "        #p, q = 10000.0, 0.0\n",
    "        #self.b_transform = b_transform if b_transform is not None else lambda x: (torch.sigmoid(x) * (p - q)) + q \n",
    "        #p2, q2 = 20000.0, 0.0\n",
    "        #self.a_transform = a_transform if a_transform is not None else lambda x: (torch.sigmoid(x) * (p2 - q2)) + q2\n",
    "        self.a_transform = torch.nn.Softplus()\n",
    "        self.b_transform = torch.nn.Softplus()\n",
    "\n",
    "        self.c     = nn.Parameter(torch.zeros(M))\n",
    "\n",
    "    def forward(self, y):\n",
    "        a = self.a_transform(self.raw_a)  # (M,)\n",
    "        b = self.b_transform(self.raw_b)  # (M,)\n",
    "        z = y.unsqueeze(-1) + self.c   # (..., M)\n",
    "        return y + torch.sum(a * torch.tanh(b * z), dim=-1)\n",
    "\n",
    "    def log_abs_det_jacobian(self, y):\n",
    "        a = self.a_transform(self.raw_a)\n",
    "        b = self.b_transform(self.raw_b)\n",
    "        z = y.unsqueeze(-1) + self.c\n",
    "        t = torch.tanh(b * z)\n",
    "        sech2 = 1 - t**2\n",
    "        deriv = 1.0 + torch.sum(a * b * sech2, dim=-1)\n",
    "        deriv = torch.clamp(deriv, min=1e-8)  # numerical safety\n",
    "        return torch.log(deriv)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def inverse(self, u):\n",
    "        u_scaled = u/self.a_transform(self.raw_a) \n",
    "        y = 1/self.b_transform(self.raw_b) * torch.atanh(u_scaled) - self.c\n",
    "        if torch.any(torch.isnan(y)):\n",
    "            # replace the nans with the values in u\n",
    "            y[torch.isnan(y)] = u[torch.isnan(y)]\n",
    "            for _ in range(40):\n",
    "                gy  = self.forward(y)\n",
    "                dgy = torch.exp(self.log_abs_det_jacobian(y))\n",
    "                y   = y - (gy - u)/dgy\n",
    "        return y\n",
    "\n",
    "\n",
    "#    @torch.no_grad()\n",
    "#    def inverse(self, u, y_init=None, iters=40):\n",
    "#        # Newton solve for y: g(y)=u ; monotone g ensures unique root\n",
    "#        y = u.clone() if y_init is None else y_init.clone()\n",
    "#        for _ in range(iters):\n",
    "#            gy  = self.forward(y)\n",
    "#            dgy = torch.exp(self.log_abs_det_jacobian(y))\n",
    "#            y   = y - (gy - u)/dgy\n",
    "#        return y\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83cc5961",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gpytorch\n",
    "class Model(gpytorch.models.ExactGP):\n",
    "    def __init__(self, X, u, likelihood, **kwargs):\n",
    "        super().__init__(X, u, likelihood)\n",
    "        self.mean_module = gpytorch.means.ZeroMean()\n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel())\n",
    "        if \"warp\" in kwargs:\n",
    "            self.warp = kwargs.get(\"warp\", lambda x: None)\n",
    "            self.y = kwargs.get(\"y\", [])\n",
    "            self.X = X\n",
    "            self.old_params = self.parameters()\n",
    "            self.apply_warp()\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "\n",
    "    def apply_warp(self):\n",
    "        if self.old_params == self.parameters():\n",
    "            return\n",
    "        self.u = self.warp(self.y)\n",
    "        self.set_train_data(self.X, self.u)\n",
    "        self.old_params = self.parameters()\n",
    "\n",
    "\n",
    "\n",
    "class MIModel(gpytorch.models.ExactGP):\n",
    "    def __init__(self, X, u, likelihood, **kwargs):\n",
    "        super().__init__(X, u, likelihood)\n",
    "        self.mean_module = gpytorch.means.ZeroMean()\n",
    "        k0 = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel(active_dims=0))\n",
    "        k1 = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel(active_dims=1))\n",
    "        self.covar_module = k0 * k1\n",
    "        if \"warp\" in kwargs:\n",
    "            self.warp = kwargs.get(\"warp\", lambda x: None)\n",
    "            self.y = kwargs.get(\"y\", [])\n",
    "            self.X = X\n",
    "            self.old_params = self.parameters()\n",
    "            self.apply_warp()\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "\n",
    "\n",
    "    def apply_warp(self):\n",
    "        if self.old_params == self.parameters():\n",
    "            return\n",
    "        self.u = self.warp(self.y)\n",
    "        self.set_train_data(self.X, self.u)\n",
    "        self.old_params = self.parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "243fa679",
   "metadata": {},
   "outputs": [],
   "source": [
    "#in 1d:\n",
    "## Daten von x=0..1\n",
    "#f:=x->2+exp(exp(3-100*x))-1/5*exp(-1/2*(x-0.1)^2/(1/5)^2)+0.05*x;\n",
    "#\n",
    "#Noise:\n",
    "#heteroscedatic!\n",
    "#the noise is Gaussian in log-space with a standard deviation of 5%\n",
    "#[x->exp(0.95*log(f(x))),f,x->exp(1.05*log(f(x)))]\n",
    "#\n",
    "## baue 2. Input \"a\" ein. a=0..1. Hat weniger einfluss\n",
    "#g:=x->f(x-0.1*sin(5*a/2))*(1+0.1*arctan(5*a-2.5))+0.1*cos(10*a/3);\n",
    "\n",
    "\n",
    "# Requires limits of a in [0, 4], b in [0, 1]\n",
    "data_f = lambda x: 2 + torch.exp(torch.exp(3 - 100 * x)) - 1/5 * torch.exp(-1/2 * (x - 0.1)**2 / (1/5)**2) + 0.05 * x\n",
    "# Requires limits of a in [0, 2], b in [0, 1]\n",
    "data_noise = lambda x: torch.exp(torch.log(data_f(x))*(1 + 0.05 * torch.randn_like(x)))  # heteroscedastic noise\n",
    "# Requires limits of a in [0, 1], b in [0, 1]\n",
    "# Original\n",
    "#data_g = lambda x, a: data_f(x - 0.1 * torch.sin(5 * a / 2)) * (1 + 0.1 * torch.arctan(5 * a - 2.5)) + 0.1 * torch.cos(10 * a / 3)\n",
    "data_g = lambda x, a: data_f(x - 0.01 * torch.sin(5 * a / 2) ) * (1 + 0.1 * torch.arctan(5 * a - 2.5)) + 0.1 * torch.cos(10 * a / 3)\n",
    "data_g2 = lambda x: data_g(x[:, 0], x[:, 1])\n",
    "\n",
    "\n",
    "#lin = torch.stack([torch.linspace(0.000, 0.19, 5), torch.linspace(0.19, 1, 5)], dim=-1).flatten()\n",
    "lin = torch.linspace(0.000, 0.1, 1)\n",
    "lin2 = torch.linspace(0.0, 1.0, 10)\n",
    "\n",
    "# TODO log(y) OHNE warping\n",
    "\n",
    "xx, yy = torch.meshgrid(lin, lin2, indexing=\"xy\")\n",
    "coords_old = torch.stack((xx.reshape(-1), yy.reshape(-1)), dim=1)  # (121, 2)\n",
    "\n",
    "\n",
    "soboleng = torch.quasirandom.SobolEngine(dimension=2)\n",
    "#coords = soboleng.draw(100)\n",
    "#coords = torch.cat([coords, coords_old])\n",
    "#X = torch.tensor([[0.1, 0], [0.5, 0.5], [0.7, 1.0]])\n",
    "\n",
    "X = torch.cat([torch.linspace(0, 0.001, 10), torch.linspace(0.01, 1, 10)])\n",
    "#X = torch.linspace(0, 1.0, 100)\n",
    "\n",
    "y = data_f(X) \n",
    "#y = data_g2(coords) \n",
    "y = y - y.mean()  # center the data\n",
    "y = y / y.std()  # normalize the data\n",
    "\n",
    "#X = torch.stack([X, a], dim=-1)  # shape (100, 2)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "44902cb4",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "d1 = lambda x: 2 + torch.exp(torch.exp(3 - 100 * x)) \n",
    "d2 = lambda x: - 1/5 * torch.exp(-1/2 * (x - 0.1)**2 / (1/5)**2) \n",
    "d3 = lambda x: + 0.05 * x\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "X = torch.linspace(0.00, 0.001, 100)\n",
    "plt.plot(X, d1(X), label='d1')\n",
    "plt.plot(X, torch.zeros_like(X))\n",
    "#plt.plot(X, d2(X), label='d2')\n",
    "#plt.plot(X, d3(X), label='d3')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e954a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "raw",
   "id": "86a706d7",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# Dataset 2 (linear-ish increase until it flattens)\n",
    "# Potentially a good _negative_ example for not using warping\n",
    "\n",
    "\n",
    "# Requires limits of a in [0, 0.5], b in [0, 1]\n",
    "data_f = lambda x: torch.tanh(torch.exp(10 * x - 4)) \n",
    "#data_f = lambda x: torch.tanh(5 * x) \n",
    "#data_noise = lambda x: data_f(x) + 0.010 * torch.cos(x)\n",
    "data_noise = lambda x: torch.exp(torch.log(data_f(x))*(1 + 0.10 * torch.randn_like(x)))  # heteroscedastic noise\n",
    "\n",
    "\n",
    "import torch  # or numpy as np\n",
    "\n",
    "def plateau_linear(x, m, c, x0=0.5, w=0.2):\n",
    "    y_lin = c + m * x\n",
    "    t = ((x - x0) / w).clamp(0.0, 1.0)\n",
    "    S = 3*t*t - 2*t*t*t\n",
    "    y0 = c + m * x0\n",
    "    return (1 - S) * y_lin + S * y0\n",
    "\n",
    "X = torch.linspace(0.000, 2.0, 50)\n",
    "\n",
    "y = data_noise(X) # , m=1, c=0.0\n",
    "y = y - y.mean()  # center the data\n",
    "y = y / y.std()  # normalize the data\n",
    "\n",
    "#X = torch.stack([X, a], dim=-1)  # shape (100, 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee922cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "warp = TanhWarp(M=1)\n",
    "u = warp(y)\n",
    "model = Model(X, u, likelihood, warp=warp, y=y)\n",
    "if \"coords\" in locals():\n",
    "    X = coords\n",
    "#model = MIModel(X, u, likelihood, warp=warp, y=y)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.1)\n",
    "model.train(); likelihood.train()\n",
    "optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf0ac631",
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"coords\" in locals() and coords.ndim > 1:\n",
    "    z_vals = model.u.detach()\n",
    "    fig = plt.figure(figsize=(8, 6))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    ax.scatter(coords[:, 0], coords[:, 1], model.u.detach().numpy(),\n",
    "                c=z_vals.numpy(), cmap='viridis', alpha=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7be890e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not \"coords\" in locals():\n",
    "    plt.plot(model.X.numpy(), model.y.numpy(), label='Original Data', linestyle='dashed')\n",
    "    plt.plot(model.X.numpy(), model.u.detach().numpy(), label='Warped Data', linestyle='dotted')\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b4997f6f",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "for _ in range(10000):\n",
    "    optimizer.zero_grad()\n",
    "    model.apply_warp()\n",
    "    output = model(model.X)  # MultivariateNormal over f(x) (latent function in u-space)\n",
    "    base_term = mll(output, model.u)\n",
    "\n",
    "    jac_term = model.warp.log_abs_det_jacobian(model.y).sum()  # + Σ log g'(y_n)\n",
    "    loss = -(base_term + jac_term)\n",
    "    #loss = -base_term\n",
    "    if _ % 50 == 0:\n",
    "        print(loss.item())\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "model.apply_warp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f9e3ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change GP data to be new u\n",
    "#model.set_train_data(X, model.warp(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cec2bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers.training_functions import granso_optimization\n",
    "from helpers.util_functions import log_normalized_prior, get_full_kernels_in_kernel_expression, randomize_model_hyperparameters\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "kernel_parameter_priors = {\n",
    "    (\"RBFKernel\", \"lengthscale\"): {\"mean\": 0.0, \"std\": 10.0}, \n",
    "    (\"MaternKernel\", \"lengthscale\"): {\"mean\": 0.0, \"std\": 10.0},\n",
    "    (\"LinearKernel\", \"variance\"): {\"mean\": 0.0, \"std\": 10.0},\n",
    "    (\"AffineKernel\", \"variance\"): {\"mean\": 0.0, \"std\": 10.0},\n",
    "    (\"RQKernel\", \"lengthscale\"): {\"mean\": 0.0, \"std\": 10.0},\n",
    "    (\"RQKernel\", \"alpha\"): {\"mean\": 0.0, \"std\": 10.0},\n",
    "    (\"CosineKernel\", \"period_length\"): {\"mean\": 0.0, \"std\": 10.0},\n",
    "    (\"PeriodicKernel\", \"lengthscale\"): {\"mean\": 0.0, \"std\": 10.0},\n",
    "    (\"PeriodicKernel\", \"period_length\"): {\"mean\": 0.0, \"std\": 10.0},\n",
    "    (\"ScaleKernel\", \"outputscale\"): {\"mean\": 0.0, \"std\": 10.0},\n",
    "    (\"LODE_Kernel\", \"signal_variance_2_0\"): {\"mean\": 0.0, \"std\": 10.0},  # full match\n",
    "    (\"LODE_Kernel\", \"lengthscale\"): {\"mean\": 0.0, \"std\": 10.0},          # base fallback\n",
    "}\n",
    "\n",
    "\n",
    "parameter_priors = {\n",
    "    \"likelihood.raw_task_noises\": {\"mean\": 0.0, \"std\": 10.0},\n",
    "    \"likelihood.raw_noise\": {\"mean\": 0.0, \"std\": 10.0},\n",
    "    \"warp.raw_a\": {\"mean\": -5.0, \"std\": 10.0},\n",
    "    \"warp.raw_b\": {\"mean\": -5.0, \"std\": 10.0},\n",
    "    \"warp.c\": {\"mean\": 0.0, \"std\": 10.0}\n",
    "}\n",
    "\n",
    "\n",
    "kernel_param_specs = {\n",
    "    (\"RBFKernel\", \"lengthscale\"): {\"bounds\": (1e-1, 5.0)}, # add ', \"type\": \"uniform\"},' # to use uniform distribution\n",
    "    (\"MaternKernel\", \"lengthscale\"): {\"bounds\": (1e-1, 1.0)},\n",
    "    (\"LinearKernel\", \"variance\"): {\"bounds\": (1e-1, 1.0)},\n",
    "    (\"AffineKernel\", \"variance\"): {\"bounds\": (1e-1, 1.0)},\n",
    "    (\"RQKernel\", \"lengthscale\"): {\"bounds\": (1e-1, 1.0)},\n",
    "    (\"RQKernel\", \"alpha\"): {\"bounds\": (1e-1, 1.0)},\n",
    "    (\"CosineKernel\", \"period_length\"): {\"bounds\": (1e-1, 10.0), \"type\": \"uniform\"},\n",
    "    (\"PeriodicKernel\", \"lengthscale\"): {\"bounds\": (1e-1, 5.0)},\n",
    "    (\"PeriodicKernel\", \"period_length\"): {\"bounds\": (1e-1, 10.0), \"type\": \"uniform\"},\n",
    "    (\"ScaleKernel\", \"outputscale\"): {\"bounds\": (1e-1, 10.0)},\n",
    "    #(\"LODE_Kernel\", \"signal_variance_2_0\"): {\"bounds\": (0.05, 0.5)},  # full match\n",
    "    (\"LODE_Kernel\", \"signal_variance\"): {\"bounds\": (1e-1, 10)},  # base\n",
    "    (\"LODE_Kernel\", \"lengthscale\"): {\"bounds\": (1e-1, 5.0)},           \n",
    "}\n",
    "\n",
    "\n",
    "param_specs = {\n",
    "    \"likelihood.raw_task_noises\": {\"bounds\": (1e-1, 1e-0)},\n",
    "    \"likelihood.raw_noise\": {\"bounds\": (1e-1, 1e-0)}\n",
    "}\n",
    "\n",
    "\n",
    "# Define the objective function\n",
    "def objective_function(model):\n",
    "    mll = gpytorch.mlls.ExactMarginalLogLikelihood(model.likelihood, model)\n",
    "    model.apply_warp()\n",
    "    output = model.likelihood(model(model.X))\n",
    "    try:\n",
    "        # TODO PyGRANSO dying is a severe problem. as it literally exits the program instead of raising an error\n",
    "        # negative scaled MLL\n",
    "        loss = -mll(output, model.u) - model.warp.log_abs_det_jacobian(model.y).sum()\n",
    "    except Exception as E:\n",
    "        print(\"LOG ERROR: Severe PyGRANSO issue. Loss is inf+0\")\n",
    "        print(f\"LOG ERROR: {E}\")\n",
    "        loss = torch.tensor(np.finfo(np.float32).max, requires_grad=True) + torch.tensor(-10.0)\n",
    "    #print(f\"LOG: {loss}\")\n",
    "    model.apply_warp()\n",
    "    return [loss, None, None]\n",
    "\n",
    "def objective_function_MAP(model):\n",
    "    mll = gpytorch.mlls.ExactMarginalLogLikelihood(model.likelihood, model)\n",
    "    model.apply_warp()\n",
    "    output = model.likelihood(model(model.X))\n",
    "    try:\n",
    "        # TODO PyGRANSO dying is a severe problem. as it literally exits the program instead of raising an error\n",
    "        # negative scaled MLL\n",
    "        loss = -mll(output, model.u) - model.warp.log_abs_det_jacobian(model.y).sum()\n",
    "        # log_normalized_prior is in metrics.py \n",
    "        log_p = log_normalized_prior(model, param_specs=parameter_priors, kernel_param_specs=kernel_parameter_priors, prior=None)\n",
    "        # negative scaled MAP\n",
    "        loss -= log_p\n",
    "    except Exception as E:\n",
    "        print(\"LOG ERROR: Severe PyGRANSO issue. Loss is inf+0\")\n",
    "        print(f\"LOG ERROR: {E}\")\n",
    "        loss = torch.tensor(np.finfo(np.float32).max, requires_grad=True) + torch.tensor(-10.0)\n",
    "    #print(f\"LOG: {loss}\")\n",
    "    model.apply_warp()\n",
    "    return [loss, None, None]\n",
    "\n",
    "\n",
    "#var_in = {\"model\" : model, \"train_x\" : X, \"train_y\" : y, \"likelihood\" : likelihood, \"MAP\" : False,\n",
    "#          \"parameter_priors\" : parameter_priors, \"kernel_parameter_priors\" : kernel_parameter_priors, \"model_parameter_prior\" : None}\n",
    "#\n",
    "#comb_fn = lambda X_struct: objective_function(var_in)\n",
    "\n",
    "neg_scaled_mll, model_MLL, model_likelihood_MLL, training_log_MLL = granso_optimization(model, likelihood, model.X, model.u, random_restarts=5, maxit=1000, MAP=False, double_precision=False, verbose=True, objective_function=objective_function_MAP)\n",
    "\n",
    "#neg_scaled_mll -= model.warp.log_abs_det_jacobian(model.y).sum()\n",
    "\n",
    "model_parameters = [p for p in model.parameters() if p.requires_grad]\n",
    "\n",
    "\n",
    "jacobian_neg_unscaled_map = torch.autograd.grad(neg_scaled_mll, model_parameters, retain_graph=True, create_graph=True, allow_unused=True)\n",
    "hessian_neg_unscaled_map_raw = []\n",
    "# Calcuate -\\nabla\\nabla log(f(\\theta)) (i.e. Hessian of negative log posterior)\n",
    "for i in range(len(jacobian_neg_unscaled_map)):\n",
    "    hessian_neg_unscaled_map_raw.append(torch.autograd.grad(jacobian_neg_unscaled_map[i], model_parameters, retain_graph=True, allow_unused=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62797677",
   "metadata": {},
   "outputs": [],
   "source": [
    "hessian_neg_unscaled_map_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e50d163",
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_scaled_mll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3aca37f",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(model.named_parameters())\n",
    "#list(likelihood.named_parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if \"coords\" in locals() and coords.ndim >1:\n",
    "    Xtest = torch.linspace(0.0, 1, 1000)\n",
    "    atest = torch.linspace(0.0, 1, 1000)\n",
    "    Xtest = torch.stack([Xtest, atest], dim=-1)  # shape (1000, 2)\n",
    "    model.eval(); likelihood.eval()\n",
    "    with torch.no_grad():\n",
    "        mvn = likelihood(model(Xtest))            # dist over u*\n",
    "        mean_u = mvn.mean\n",
    "        var_u  = mvn.variance\n",
    "\n",
    "        # Point estimates in y-space\n",
    "        median_y = model.warp.inverse(mean_u)           # ≈ predictive median\n",
    "\n",
    "        # Monte Carlo moments\n",
    "        S = 512\n",
    "        eps = torch.randn(S, *mean_u.shape, device=mean_u.device)\n",
    "        u_samps = mean_u.unsqueeze(0) + eps * var_u.sqrt().unsqueeze(0)\n",
    "        y_samps = model.warp.inverse(u_samps)\n",
    "\n",
    "        mean_y = y_samps.mean(dim=0)\n",
    "        \n",
    "        #plot_3d_gp(model, likelihood, data=None, x_min=0.0, x_max=1.0, y_min=0.0, y_max=1.0,\n",
    "        #            resolution=50, return_figure=False, fig=None, ax=None, \n",
    "        #            display_figure=True, loss_val=None, loss_type=None, shadow=False,\n",
    "        #            title_add = \"\"):\n",
    "        plot_3d_gp(model, likelihood, data=torch.stack([X[:,0], X[:,1], u], -1), x_min=0.0, x_max=1.0, y_min=0.0, y_max=1.0)\n",
    "\n",
    "\n",
    "\n",
    "        # matplotlib plot the samples\n",
    "        #plt.figure(figsize=(10, 6))\n",
    "        #plt.plot(Xtest.cpu().numpy(), median_y.cpu().numpy(), label='Predictive Median', color='blue')\n",
    "        ##plt.fill_between(warp.inverse(mean_u -+ 2*torch.sqrt(var_u))))\n",
    "        #plt.fill_between(Xtest.cpu().numpy(),\n",
    "        #                warp.inverse(mean_u - 2 * torch.sqrt(var_u)).cpu().numpy(),\n",
    "        #                warp.inverse(mean_u + 2 * torch.sqrt(var_u)).cpu().numpy(),\n",
    "        #                color='blue', alpha=0.2, label='95% Credible Interval')\n",
    "        ##plt.fill_between(Xtest.cpu().numpy(), \n",
    "        ##                 y_samps.quantile(0.05, dim=0).cpu().numpy(), \n",
    "        ##                 y_samps.quantile(0.95, dim=0).cpu().numpy(), \n",
    "        ##                 color='blue', alpha=0.2, label='95% Credible Interval')\n",
    "        #plt.scatter(X.cpu().numpy(), y.cpu().numpy(), s=10, color='red', label='Data')\n",
    "        #plt.title('GP with Tanh Warp')\n",
    "        #plt.xlabel('X')\n",
    "        #plt.ylabel('y')\n",
    "        #plt.legend()\n",
    "        #plt.show()\n",
    "else:\n",
    "    Xtest = torch.linspace(0.0, 2, 1000)\n",
    "    model.eval(); likelihood.eval()\n",
    "    with torch.no_grad():\n",
    "        mvn = likelihood(model(Xtest))            # dist over u*\n",
    "        mean_u = mvn.mean\n",
    "        var_u  = mvn.variance\n",
    "\n",
    "        # Point estimates in y-space\n",
    "        median_y = model.warp.inverse(mean_u)           # ≈ predictive median\n",
    "\n",
    "        # Monte Carlo moments\n",
    "        S = 512\n",
    "        eps = torch.randn(S, *mean_u.shape, device=mean_u.device)\n",
    "        u_samps = mean_u.unsqueeze(0) + eps * var_u.sqrt().unsqueeze(0)\n",
    "        y_samps = model.warp.inverse(u_samps)\n",
    "\n",
    "        mean_y = y_samps.mean(dim=0)\n",
    "        # matplotlib plot the samples\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(Xtest.cpu().numpy(), median_y.cpu().numpy(), label='Predictive Median', color='blue')\n",
    "        #plt.fill_between(warp.inverse(mean_u -+ 2*torch.sqrt(var_u))))\n",
    "        plt.fill_between(Xtest.cpu().numpy(),\n",
    "                        warp.inverse(mean_u - 2 * torch.sqrt(var_u)).cpu().numpy(),\n",
    "                        warp.inverse(mean_u + 2 * torch.sqrt(var_u)).cpu().numpy(),\n",
    "                        color='blue', alpha=0.2, label='95% Credible Interval')\n",
    "\n",
    "        #plt.fill_between(Xtest.cpu().numpy(), \n",
    "        #                 y_samps.quantile(0.05, dim=0).cpu().numpy(), \n",
    "        #                 y_samps.quantile(0.95, dim=0).cpu().numpy(), \n",
    "        #                 color='blue', alpha=0.2, label='95% Credible Interval')\n",
    "        plt.scatter(model.X.cpu().numpy(), model.y.cpu().numpy(), s=10, color='red', label='Data')\n",
    "        plt.title('GP with Tanh Warp. Real space')\n",
    "        plt.xlabel('X')\n",
    "        plt.ylabel('y')\n",
    "        plt.legend()\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb0c9099",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.y, model.u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c273b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    mvn = likelihood(model(Xtest))            # dist over u*\n",
    "    mean_u = mvn.mean\n",
    "    var_u  = mvn.variance\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(Xtest.cpu().numpy(), mean_u.cpu().numpy(), label='Predictive Median', color='blue')\n",
    "    #plt.fill_between(warp.inverse(mean_u -+ 2*torch.sqrt(var_u))))\n",
    "    plt.fill_between(Xtest.cpu().numpy(),\n",
    "                     mean_u - 2 * torch.sqrt(var_u),\n",
    "                     mean_u + 2 * torch.sqrt(var_u),\n",
    "                     color='blue', alpha=0.2, label='95% Credible Interval')\n",
    "\n",
    "    plt.scatter(model.X.cpu().numpy(), model.u.cpu().numpy(), s=10, color='red', label='Data')\n",
    "    plt.title('GP with Tanh Warp. Latent space')\n",
    "    plt.xlabel('X')\n",
    "    plt.ylabel('warp(y)')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a48f13a5",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "Warping\n",
    "u=warp(y); y ~ GP(mu,K)\n",
    "Manifolding\n",
    "y~GP(mu(xi(x)),k(xi(x)),xi(x')))\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6e2697e2",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c1a9bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the original and warped data with x label \"y\" and y label \"warp(y)\" \n",
    "plt.scatter(model.y, model.u.detach().numpy(), label='Warped Data')\n",
    "#plt.plot(X, y, label='Original Data')\n",
    "plt.xlabel(\"y\")\n",
    "plt.ylabel(\"warp(y)\")\n",
    "plt.legend()\n",
    "#list(model.warp.parameters())\n",
    "\n",
    "\n",
    "\n",
    "#plot_3d_data(X[:, 0], X[:, 1], model.warp(y).detach())\n",
    "#plot_3d_data(X[:, 0], X[:, 1], y.detach())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "laplace",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
